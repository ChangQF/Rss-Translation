<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 04 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Funny Valen-Tine：通过定义解决方案分布来解决视觉抽象推理问题</title>
      <link>https://arxiv.org/abs/2407.02688</link>
      <description><![CDATA[arXiv:2407.02688v1 公告类型：新
摘要：视觉抽象推理问题在图像处理领域具有重要意义。Bongard-Logo 和 Raven 的渐进矩阵 (RPM) 都属于此领域，其中 Bongard-Logo 归类为图像聚类推理，而 RPM 涉及图像渐进模式推理。本文介绍了 Valen，这是概率突出显示模型下的一种新型基线模型。Valen 在解决 RPM 和 Bongard-Logo 问题方面表现出色，提供了一种多功能解决方案。我们的研究深入研究了概率突出显示求解器的潜在机制，发现它们将推理问题实例的解近似为由主要和辅助样本划定的分布。我们提出学习目标不是正确解的分布，而是由主要和辅助样本定义的分布。为了弥合差异，我们引入了 Tine 方法，这是一种基于对抗学习的方法，可帮助 Valen 估计更接近正确解分布，尽管存在训练不稳定等问题。反思 Tine，我们提出将推理问题的样本分布建模为高斯分布的混合，从而产生了 Funny 方法。这有效地使 Valen 能够捕捉正确解分布的真实形式。此外，我们设计了 SBR 方法来类似地建模渐进模式表示的分布。总体而言，Funny、Tine 和 SBR 方法显著提高了 Valen 的性能，为研究视觉抽象推理问题提供了新的思路和方法。]]></description>
      <guid>https://arxiv.org/abs/2407.02688</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:03 GMT</pubDate>
    </item>
    <item>
      <title>通过渐进式知识提炼推进压缩视频动作识别</title>
      <link>https://arxiv.org/abs/2407.02713</link>
      <description><![CDATA[arXiv:2407.02713v1 公告类型：新
摘要：压缩视频动作识别通过利用压缩视频中的不同模态（即运动矢量、残差和帧内）对视频样本进行分类。为此，部署了三个神经网络，每个神经网络专用于处理一种模态。我们的观察表明，处理帧内的网络倾向于收敛到比处理残差的网络更平坦的最小值，而处理残差的网络又收敛到比运动矢量网络更平坦的最小值。这种收敛层次激发了我们在模态之间进行知识转移的策略，以实现更平坦的最小值，这通常与更好的泛化相关。基于这一见解，我们提出了渐进式知识蒸馏 (PKD)，这是一种在模态之间逐步转移知识的技术。该方法涉及将早期出口（内部分类器 - IC）附加到三个网络。 PKD 从运动矢量网络开始提取知识，然后是残差，最后是帧内网络，依次提高 IC 的准确性。此外，我们提出了带缩放集成的加权推理 (WISE)，它使用学习到的权重将 IC 的输出组合起来，从而提高推理过程中的准确性。我们的实验证明了使用 PKD 训练 IC 的有效性，与基于交叉熵的标准训练相比，IC 的准确性分别在 UCF-101 和 HMDB-51 数据集上分别提高了 5.87% 和 11.42%。此外，WISE 分别在 UCF-101 和 HMDB-51 上将准确性提高了 4.28% 和 9.30%。]]></description>
      <guid>https://arxiv.org/abs/2407.02713</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:03 GMT</pubDate>
    </item>
    <item>
      <title>打开全景分割</title>
      <link>https://arxiv.org/abs/2407.02685</link>
      <description><![CDATA[arXiv:2407.02685v1 公告类型：新
摘要：全景图像捕捉 360{\deg} 视场 (FoV)，包含对场景理解至关重要的全向空间信息。然而，在封闭词汇环境中训练模型时，获取足够训练的密集注释全景图不仅成本高昂，而且应用受到限制。为了解决这个问题，在这项工作中，我们定义了一个称为开放全景分割 (OPS) 的新任务，其中模型在开放词汇环境中使用源域中 FoV 限制的针孔图像进行训练，同时使用目标域中的 FoV 开放全景图像进行评估，从而实现模型的零镜头开放全景语义分割能力。此外，我们提出了一种带有可变形适配器网络 (DAN) 的模型，名为 OOOPS，可显着提高零镜头全景语义分割性能。为了进一步增强针孔源域的失真感知建模能力，我们提出了一种称为随机等矩形投影 (RERP) 的新型数据增强方法，该方法专门用于提前解决对象变形问题。超越其他最先进的开放词汇语义分割方法，在三个全景数据集 WildPASS、Stanford2D3D 和 Matterport3D 上的性能显著提升，证明了我们提出的带有 RERP 的 OOOPS 模型在 OPS 任务上的有效性，尤其是在室外 WildPASS 上提高了 2.2%，在室内 Stanford2D3D 上提高了 2.4% 的 mIoU。代码将在 https://junweizheng93.github.io/publications/OPS/OPS.html 上提供。]]></description>
      <guid>https://arxiv.org/abs/2407.02685</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:02 GMT</pubDate>
    </item>
    <item>
      <title>利用对抗放大技术通过超分辨率欺骗 Deepfake 检测</title>
      <link>https://arxiv.org/abs/2407.02670</link>
      <description><![CDATA[arXiv:2407.02670v1 公告类型：新 
摘要：Deepfake 技术正在迅速发展，对检测被操纵的媒体内容构成了重大挑战。与此同时，一些对抗性攻击技术已经被开发出来以欺骗 Deepfake 检测器并使 Deepfake 更难被检测到。本文探讨了超分辨率技术作为一种可能的对抗性攻击在 Deepfake 检测中的应用。通过我们的实验，我们证明这些方法对图像外观所做的最小改变会对 Deepfake 检测系统的性能产生深远的影响。我们提出了一种新颖的攻击，使用超分辨率作为一种快速、黑盒和有效的方法来伪装假图像和/或在原始图像上产生误报。我们的结果表明，使用超分辨率会严重损害 Deepfake 检测器的准确性，从而凸显此类系统对对抗性攻击的脆弱性。重现我们实验的代码可以在以下位置找到：https：//github.com/davide-coccomini/Adversarial-Magnification-to-Deceive-Deepfake-Detection-through-Super-Resolution]]></description>
      <guid>https://arxiv.org/abs/2407.02670</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>通用事件相机</title>
      <link>https://arxiv.org/abs/2407.02683</link>
      <description><![CDATA[arXiv:2407.02683v1 公告类型：新
摘要：事件相机以高时间分辨率和最小带宽要求捕捉世界。然而，事件流仅对亮度变化进行编码，不包含足够的场景信息来支持各种下游任务。在这项工作中，我们设计了通用事件相机，它以带宽高效的方式固有地保留场景强度。我们根据事件的生成时间和传输的信息来概括事件相机。为了实现我们的设计，我们转向提供对单个光子检测的数字访问的单光子传感器；这种方式使我们能够灵活地实现丰富的通用事件相机空间。我们的单光子事件相机能够以低读出率进行高速、高保真成像。因此，这些事件相机可以支持即插即用的下游推理，而无需捕获新的事件数据集或设计专门的事件视觉模型。从实际意义来看，我们的设计涉及轻量级和近传感器兼容的计算，提供了一种使用单光子传感器的方法，而无需过高的带宽成本。]]></description>
      <guid>https://arxiv.org/abs/2407.02683</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>SMILe：利用子模互信息实现稳健的小样本目标检测</title>
      <link>https://arxiv.org/abs/2407.02665</link>
      <description><![CDATA[arXiv:2407.02665v1 公告类型：新
摘要：对象类别的混淆和遗忘一直是少样本对象检测 (FSOD) 的主要挑战。为了克服基于度量学习的 FSOD 技术中的这些缺陷，我们引入了一种新颖的子模块互信息学习 (SMILe) 框架，该框架采用组合互信息函数来强制在 FSOD 中创建更紧密和更具判别性的特征簇。我们提出的方法推广到 FSOD 中的几种现有方法，与主干架构无关，从而提高了性能。SMILe 中从基于实例的目标函数到组合目标的范式转变自然地保留了对象类内的多样性，从而减少了在接受少量训练示例时遗忘的情况。此外，在已经学习的（基础）和新添加的（新）对象之间应用互信息可确保基础类和新类之间的充分分离，从而最大限度地减少类混淆的影响。在流行的 FSOD 基准、PASCAL-VOC 和 MS-COCO 上进行的实验表明，我们的方法可以推广到最先进 (SoTA) 方法，在 VOC（分割 3）的 10 次设置和 COCO 数据集的 30 次设置上分别将其新类性能提高了 5.7%（3.3 mAP 点）和 5.4%（2.6 mAP 点）。我们的实验还表明，与现有方法相比，与底层架构无关，基类性能保留得更好，收敛速度提高了 2 倍。]]></description>
      <guid>https://arxiv.org/abs/2407.02665</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>MomentsNeRF：利用正交矩进行小样本神经渲染</title>
      <link>https://arxiv.org/abs/2407.02668</link>
      <description><![CDATA[arXiv:2407.02668v1 公告类型：新
摘要：我们提出了 MomentsNeRF，这是一种用于一次性和少量神经渲染的新型框架，它使用正交矩预测 3D 场景的神经表示。我们的架构提供了一种新的迁移学习方法，可以在多场景上进行训练，并在测试时使用一张或几张图像进行每个场景的优化。我们的方法是第一个成功利用从 Gabor 和 Zernike 矩中提取的特征的方法，将它们无缝集成到 NeRF 架构中。我们表明，与最近的一次性和少量神经渲染框架相比，MomentsNeRF 在合成具有复杂纹理和形状的图像方面表现更好，实现了显着的降噪、伪影消除和完成缺失部分。在 DTU 和 Shapenet 数据集上进行的大量实验表明，MomentsNeRF 比最佳方法提高了 {3.39\;dB\;PSNR}、11.1% SSIM、17.9% LPIPS 和 8.3% DISTS 指标。此外，它在新颖视图合成和单图像 3D 视图重建方面的表现均优于最佳方法。源代码可在以下网址访问：https://amughrabi.github.io/momentsnerf/。]]></description>
      <guid>https://arxiv.org/abs/2407.02668</guid>
      <pubDate>Thu, 04 Jul 2024 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>用于道路提取的整体嵌套结构感知图神经网络</title>
      <link>https://arxiv.org/abs/2407.02639</link>
      <description><![CDATA[arXiv:2407.02639v1 公告类型：新
摘要：卷积神经网络 (CNN) 在从卫星图像中检测道路方面取得了重大进展。然而，现有的 CNN 方法通常是重新利用的语义分割架构，并且对长而弯曲的区域的描绘不佳。缺乏整体道路拓扑和结构信息进一步降低了它们在具有挑战性的遥感图像上的性能。本文提出了一种新颖的多任务图神经网络 (GNN)，可同时检测道路区域和道路边界；这两个任务之间的相互作用从两个角度释放了卓越的性能：(1) 分层检测的道路边界使网络能够捕获和编码整体道路结构以增强道路连通性 (2) 识别语义土地覆盖区域的内在相关性减轻了识别外观相似的区域所造成的道路混乱的难度。在具有挑战性的数据集上的实验表明，与现有方法相比，所提出的架构可以提高道路边界描绘和道路提取的准确性。]]></description>
      <guid>https://arxiv.org/abs/2407.02639</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>用于高光谱图像分类的光谱图推理网络</title>
      <link>https://arxiv.org/abs/2407.02647</link>
      <description><![CDATA[arXiv:2407.02647v1 公告类型：新
摘要：卷积神经网络 (CNN) 在过去几年中在高光谱图像 (HSI) 分类中取得了显著的表现。尽管取得了进展，但现有方法在光谱域中采用的卷积核的感受野大小有限，在很大程度上未充分利用 HSI 的丰富而翔实的光谱信息。为了解决这个问题，我们提出了一个光谱图推理网络 (SGR) 学习框架，该框架包含两个关键模块：1) 光谱解耦模块，它将多个光谱嵌入解包并转换为统一的图，其节点对应于嵌入空间中的各个光谱特征通道；该图执行可解释的推理以聚合和对齐光谱信息，以指导在多个上下文级别学习特定于光谱的图嵌入 2) 光谱集成模块通过一种新颖的递归图传播机制探索跨图嵌入层次结构的相互作用和相互依赖性。在两个 HSI 数据集上的实验表明，与现有方法相比，所提出的架构可以显著提高分类准确率。]]></description>
      <guid>https://arxiv.org/abs/2407.02647</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>元 3D 生成</title>
      <link>https://arxiv.org/abs/2407.02599</link>
      <description><![CDATA[arXiv:2407.02599v1 公告类型：新
摘要：我们推出了 Meta 3D Gen (3DGen)，这是一种用于文本到 3D 资产生成的新型先进快速管道。3DGen 可在不到一分钟的时间内提供具有高即时保真度和高质量 3D 形状和纹理的 3D 资产创建。它支持基于物理的渲染 (PBR)，这对于在实际应用中重新照明 3D 资产是必不可少的。此外，3DGen 还支持使用用户提供的额外文本输入对先前生成的（或艺术家创建的）3D 形状进行生成性重新纹理化。3DGen 集成了我们分别为文本到 3D 和文本到纹理生成开发的关键技术组件 Meta 3D AssetGen 和 Meta 3D TextureGen。通过结合它们的优势，3DGen 可以同时以三种方式表示 3D 对象：在视图空间、体积空间和 UV（或纹理）空间中。这两种技术的结合相对于单阶段模型实现了 68% 的胜率。我们将 3DGen 与众多行业基准进行了比较，结果表明它在复杂文本提示的提示保真度和视觉质量方面均优于它们，而且速度明显更快。]]></description>
      <guid>https://arxiv.org/abs/2407.02599</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>HOIMotion：使用以自我为中心的 3D 物体边界框预测人与物体交互过程中的人体运动</title>
      <link>https://arxiv.org/abs/2407.02633</link>
      <description><![CDATA[arXiv:2407.02633v1 公告类型：新
摘要：我们提出了 HOIMotion——一种在人与物体交互过程中预测人体运动的新方法，它整合了有关过去身体姿势和自我中心 3D 物体边界框的信息。人体运动预测在许多增强现实应用中都很重要，但大多数现有方法仅使用过去的身体姿势来预测未来的运动。HOIMotion 首先使用编码器残差图卷积网络 (GCN) 和多层感知器分别从身体姿势和自我中心 3D 物体边界框中提取特征。然后，我们的方法将姿势和物体特征融合成一个新的姿势-物体图，并使用残差解码器 GCN 来预测未来的身体运动。我们在 Aria 数字孪生 (ADT) 和 MoGaze 数据集上对我们的方法进行了广泛的评估，结果表明 HOIMotion 在平均关节位置误差方面始终优于最先进的方法，在 ADT 上最高可达 8.7%，在 MoGaze 上最高可达 7.2%。作为这些评估的补充，我们报告了一项人体研究 (N=20)，该研究显示，我们的方法所取得的改进使预测的姿势比现有方法更精确、更真实。总之，这些结果揭示了以自我为中心的 3D 物体边界框中可用于人体运动预测的重要信息内容，以及我们的方法在利用这些信息方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.02633</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>使用通道修剪的 YOLOv5s 模型通过强大的手势识别系统实现新颖的人机界面</title>
      <link>https://arxiv.org/abs/2407.02585</link>
      <description><![CDATA[arXiv:2407.02585v1 公告类型：新
摘要：手势识别 (HGR) 是增强人机交互体验的重要组成部分，特别是在多媒体应用中，例如虚拟现实、游戏、智能家居自动化系统等。用户可以通过准确检测和识别手势来无缝控制和浏览这些应用程序。然而，在实时场景中，手势识别系统的性能有时会受到复杂背景、低光照、遮挡问题等的影响。另一个问题是在实时场景中构建快速且强大的手势控制人机界面 (HCI)。本文的总体目标是使用通道修剪的 YOLOv5-small 模型开发有效的手势检测和分类模型，并利用该模型构建具有快速响应时间（以 ms 为单位）和更高检测速度（以 fps 为单位）的手势控制 HCI。首先，选择 YOLOv5s 模型进行手势检测任务。接下来，使用通道剪枝算法简化模型。之后，进一步微调剪枝后的模型以确保检测效率。我们将我们提出的方案与其他最先进的方法进行了比较，观察到我们的模型在 mAP（平均精度）、精度（\%）、召回率（\%）和 F1 分数（\%）、快速推理时间（以毫秒为单位）和检测速度（以 fps 为单位）方面表现出色。我们提出的方法为部署剪枝后的 YOLOv5s 模型以用于基于实时手势命令的 HCI 铺平了道路，以使用正确分类的手势命令在实时场景中控制某些应用程序，例如 VLC 媒体播放器、Spotify 播放器等。我们提出的系统的平均检测速度实时达到每秒 60 帧（fps）以上，满足实时应用控制的完美要求。]]></description>
      <guid>https://arxiv.org/abs/2407.02585</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态大型语言模型改善视觉叙事</title>
      <link>https://arxiv.org/abs/2407.02586</link>
      <description><![CDATA[arXiv:2407.02586v1 公告类型：新
摘要：视觉叙事是一个新兴领域，它将图像和叙事结合起来，创造出引人入胜且内容丰富的故事。尽管它具有潜力，但由于视觉和文本信息的对齐复杂性，生成连贯且引起情感共鸣的视觉故事仍然具有挑战性。本文提出了一种新方法，利用大型语言模型 (LLM) 和大型视觉语言模型 (LVLM) 结合指令调整来应对这些挑战。我们引入了一个包含各种视觉故事的新数据集，并附有详细的标题和多模态元素。我们的方法采用监督学习和强化学习相结合的方式对模型进行微调，增强其叙事生成能力。使用 GPT-4 的定量评估和定性的人工评估表明，我们的方法明显优于现有模型，在叙事连贯性、相关性、情感深度和整体质量方面取得了更高的分数。结果强调了指令调整的有效性以及 LLM/LVLM 在推进视觉叙事方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.02586</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>AutoSplat：用于自动驾驶场景重建的约束高斯分层</title>
      <link>https://arxiv.org/abs/2407.02598</link>
      <description><![CDATA[arXiv:2407.02598v1 公告类型：新
摘要：逼真的场景重建和视图合成对于通过模拟安全关键场景来推进自动驾驶系统至关重要。3D Gaussian Splatting 在实时渲染和静态场景重建方面表现出色，但由于背景复杂、动态物体和稀疏视图，难以对驾驶场景进行建模。我们提出了 AutoSplat，这是一个采用高斯分布来实现自动驾驶场景高度逼真重建的框架。通过对代表道路和天空区域的高斯施加几何约束，我们的方法可以实现对包括车道变换在内的具有挑战性的场景的多视图一致模拟。利用 3D 模板，我们引入了反射高斯一致性约束来监督前景物体的可见面和不可见面。此外，为了模拟前景物体的动态外观，我们估计每个前景高斯的残差球谐函数。在 Pandaset 和 KITTI 上进行的大量实验表明，AutoSplat 在各种驾驶场景中的场景重建和新颖的视图合成方面优于最先进的方法。访问我们的 $\href{https://autosplat.github.io/}/text{项目页面}}$。]]></description>
      <guid>https://arxiv.org/abs/2407.02598</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>稳健的 ADAS：增强基于机器学习的高级驾驶辅助系统在恶劣天气下的稳健性</title>
      <link>https://arxiv.org/abs/2407.02581</link>
      <description><![CDATA[arXiv:2407.02581v1 公告类型：新
摘要：在将基于机器学习的高级驾驶辅助系统 (ML-ADAS) 部署到现实场景中时，恶劣的天气条件带来了重大挑战。在晴朗天气数据上训练的传统 ML 模型在面对极端雾或大雨等场景时会失效，可能会导致事故和安全隐患。本文通过提出一种新方法解决了这个问题：采用去噪深度神经网络作为预处理步骤，将恶劣天气图像转换为晴朗天气图像，从而增强 ML-ADAS 系统的鲁棒性。所提出的方法消除了在 ML-ADAS 管道中重新训练所有后续 Depp 神经网络 (DNN) 的需要，从而节省了计算资源和时间。此外，它还改善了驾驶员的可视化，这对于在恶劣天气条件下安全导航至关重要。通过利用在增强型 KITTI 数据集上训练的 UNet 架构（其中包含合成的恶劣天气图像），我们开发了天气 UNet (WUNet) DNN 来消除天气伪影。我们的研究表明，在恶劣天气条件下，使用 WUNet 预处理可以显著提高物体检测的性能。值得注意的是，在极端雾霾场景中，我们提出的解决方案将 YOLOv8n 的平均准确率 (mAP) 得分从 4% 提高到 70%。]]></description>
      <guid>https://arxiv.org/abs/2407.02581</guid>
      <pubDate>Thu, 04 Jul 2024 06:20:56 GMT</pubDate>
    </item>
    </channel>
</rss>