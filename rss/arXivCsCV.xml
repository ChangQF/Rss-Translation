<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 15 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于审美感知音乐推荐的顺序复杂度审美评估模型</title>
      <link>https://arxiv.org/abs/2402.08300</link>
      <description><![CDATA[arXiv:2402.08300v1 公告类型：新
摘要：计算美学评估对视觉艺术作品做出了显着的贡献，但其在音乐中的应用仍然很少。目前，主观评价仍然是评价艺术作品最有效的形式。然而，对艺术作品的主观评价会消耗大量的人力和物力。时下流行的人工智能生成内容（AIGC）任务已经充斥各个行业，音乐也不例外。但与人类创作的音乐相比，人工智能创作的音乐听起来仍然机械、单调，缺乏审美吸引力。由于缺乏带有评分标注的音乐数据集，我们不得不选择传统的审美方程来客观地衡量音乐的美感。为了提高AI音乐生成的质量，进一步指导计算机音乐制作、合成、推荐等任务，我们利用伯克霍夫的审美测度设计审美模型，客观地衡量音乐的审美美感，并根据音乐的美感。实验表明，我们的客观审美模型和推荐方法是有效的。]]></description>
      <guid>https://arxiv.org/abs/2402.08300</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>通过辅助损失优化编码器来改进机器的图像编码</title>
      <link>https://arxiv.org/abs/2402.08267</link>
      <description><![CDATA[arXiv:2402.08267v1 公告类型：新
摘要：机器图像编码（ICM）旨在使用识别模型而不是人类视觉来压缩图像以进行机器分析。因此，在 ICM 中，编码器识别并压缩机器识别任务所需的信息非常重要。学习 ICM 有两种主要方法；基于任务损失和感兴趣区域（ROI）的比特分配优化压缩模型。这些方法为编码器提供了识别能力。然而，当识别模型很深时，任务丢失的优化变得很困难，并且基于 ROI 的方法在评估过程中通常会涉及额外的开销。在这项研究中，我们提出了一种新的学习 ICM 模型训练方法，该方法将辅助损失应用于编码器，以提高其识别能力和率失真性能。与传统训练方法相比，我们的方法在目标检测和语义分割任务中实现了 27.7% 和 20.3% 的 Bjontegaard Delta 率提高。]]></description>
      <guid>https://arxiv.org/abs/2402.08267</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>从嘈杂的排名注释中学习胎儿超声的语义图像质量</title>
      <link>https://arxiv.org/abs/2402.08294</link>
      <description><![CDATA[arXiv:2402.08294v1 公告类型：新
摘要：我们为图像质量依赖于语义要求的应用引入了语义图像质量的概念。在胎儿超声检查中，排序具有挑战性并且注释充满噪音，我们设计了一个强大的从粗到细的模型，该模型根据图像的语义图像质量对图像进行排序，并为我们的预测排名赋予不确定性估计。为了注释训练数据的排名，我们设计了一种基于合并排序算法的有效排名注释方案。最后，我们将我们的排名算法与许多最先进的排名算法在具有挑战性的胎儿超声质量评估任务上进行比较，显示我们的方法在大多数排名相关指标上的优越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.08294</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>使用无人机深度学习进行热图像中的物体检测</title>
      <link>https://arxiv.org/abs/2402.08251</link>
      <description><![CDATA[arXiv:2402.08251v1 公告类型：新
摘要：这项工作提出了一种能够识别无人机收集的热图像中的小型和微小物体的神经网络模型。我们的模型由三部分组成：主干、颈部和预测头部。主干是基于YOLOv5的结构开发的，并在末端结合使用了Transformer编码器。颈部包括一个 BI-FPN 块，结合使用滑动窗口和变压器来增加输入预测头的信息。预测头通过使用 Sigmoid 函数评估特征图来进行检测。使用具有注意力和滑动窗口的变压器可以提高识别精度，同时使模型保持合理的参数数量和嵌入式系统的计算要求。在公共数据集 VEDAI 和我们收集的数据集上进行的实验表明，我们的模型比 ResNet、Faster RCNN、ComNet、ViT、YOLOv5、SMPNet 和 DPNetV3 等最先进的方法具有更高的准确性。在嵌入式计算机Jetson AGX上的实验表明，我们的模型实现了实时计算速度，稳定率超过90%。]]></description>
      <guid>https://arxiv.org/abs/2402.08251</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>关于将文本到图像扩散与偏好对齐的密集奖励视图</title>
      <link>https://arxiv.org/abs/2402.08265</link>
      <description><![CDATA[arXiv:2402.08265v1 公告类型：新
摘要：将文本到图像扩散模型（T2I）与偏好对齐已经受到越来越多的研究关注。虽然先前的工作是通过偏好数据直接优化 T2I，但这些方法是在整个扩散反向链上潜在奖励的强盗假设下开发的，同时忽略了生成过程的顺序性质。从文献来看，这可能会损害对齐的功效和效率。在本文中，我们采用更精细密集的奖励视角，并得出一个易于处理的对齐目标，强调 T2I 反向链的初始步骤。特别是，我们将时间贴现引入到 DPO 风格的显式无奖励损失中，以打破其中的时间对称性并适应 T2I 生成层次结构。在单个和多个提示生成的实验中，我们的方法在定量和定性方面都与强大的相关基线具有竞争力。进行了进一步的研究来说明我们方法的洞察力。]]></description>
      <guid>https://arxiv.org/abs/2402.08265</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>将图像转换为道路网络：一种非自回归序列到序列方法</title>
      <link>https://arxiv.org/abs/2402.08207</link>
      <description><![CDATA[arXiv:2402.08207v1 公告类型：新
摘要： 道路网络的提取对于生成高清地图至关重要，因为它能够精确定位道路地标及其互连。然而，由于欧几里德（例如，道路地标位置）和非欧几里德（例如，道路拓扑连通性）结构的潜在组合相互冲突，生成道路网络提出了重大挑战。现有的方法很难有效地合并两种类型的数据域，但很少能正确解决这个问题。相反，我们的工作通过将欧几里德和非欧几里德数据投影到称为 RoadNet 序列的整数序列中，建立了两种类型数据域的统一表示。除了对自回归序列到序列 Transformer 模型进行建模以理解 RoadNet 序列之外，我们还将 RoadNet 序列的依赖性解耦为自回归和非自回归依赖性的混合。在此基础上，我们提出的非自回归序列到序列方法利用非自回归依赖性，同时修复与自回归依赖性的差距，从而在效率和准确性上取得成功。 nuScenes 数据集上的大量实验证明了 RoadNet 序列表示和非自回归方法与现有最先进替代方法相比的优越性。该代码在 https://github.com/fudan-zvg/RoadNetworkTRansformer 上开源。]]></description>
      <guid>https://arxiv.org/abs/2402.08207</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>SepRep-Net：通过模型分离和重新参数化进行多源自由域适应</title>
      <link>https://arxiv.org/abs/2402.08249</link>
      <description><![CDATA[arXiv:2402.08249v1 公告类型：新
摘要：我们考虑多源自由域适应，即在不访问源数据的情况下将多个现有模型适应新域的问题。在现有方法中，基于模型集成的方法在源域和目标域中均有效，但会导致计算成本显着增加。针对这一困境，在这项工作中，我们提出了一种名为 SepRep-Net 的新颖框架，该框架通过模型分离和重新参数化来解决多源自由域适应问题。具体来说，SepRep-Net 将多个现有模型重新组装成一个统一的网络，同时保持单独的路径（分离）。在训练期间，单独的路径与通过附加特征合并单元定期执行的信息交换并行优化。通过我们的具体设计，这些路径可以进一步重新参数化为单个路径，以方便推理（重新参数化）。 SepRep-Net 的特点是 1) 有效性：在目标域上的竞争性能，2) 效率：低计算成本，3) 通用性：比现有解决方案维护更多的源知识。作为一种通用方法，SepRep-Net 可以无缝插入各种方法中。大量实验验证了 SepRep-Net 在主流基准上的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.08249</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>变压器跟踪的优化信息流</title>
      <link>https://arxiv.org/abs/2402.08195</link>
      <description><![CDATA[arXiv:2402.08195v1 公告类型：新
摘要：单流 Transformer 跟踪器在过去三年中在具有挑战性的基准数据集中表现出了出色的性能，因为它们能够实现目标模板和搜索区域标记之间的交互，以在相互指导下提取面向目标的特征。以前的方法允许模板和搜索标记之间的自由双向信息流，而无需研究它们对跟踪器辨别能力的影响。在本研究中，我们对代币的信息流进行了详细的研究，并根据研究结果提出了一种新颖的优化信息流跟踪（OIFTrack）框架来增强跟踪器的判别能力。所提出的 OIFTrack 阻止了早期编码器层中所有搜索标记与目标模板标记的交互，因为搜索区域中的大量非目标标记降低了目标特定特征的重要性。在所提出的跟踪器的较深层编码器层中，搜索令牌被划分为目标搜索令牌和非目标搜索令牌，允许从目标搜索令牌到模板令牌的双向流以捕获目标的外观变化。此外，由于所提出的跟踪器结合了动态背景线索，因此可以通过捕获目标的周围信息成功地避免干扰对象。 OIFTrack 在具有挑战性的基准测试中表现出了出色的性能，特别是在一次性跟踪基准 GOT-10k 中表现出色，实现了 74.6% 的平均重叠。这项工作的代码、模型和结果可在 \url{https://github.com/JananiKugaa/OIFTrack} 获取]]></description>
      <guid>https://arxiv.org/abs/2402.08195</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>微调文本到图像的扩散模型以生成按类别的杂散特征</title>
      <link>https://arxiv.org/abs/2402.08200</link>
      <description><![CDATA[arXiv:2402.08200v1 公告类型：新
摘要：我们提出了一种利用大规模文本到图像扩散模型生成虚假特征的方法。尽管之前的工作检测了像 ImageNet 这样的大规模数据集中的虚假特征并引入了 Spurious ImageNet，但我们发现并非所有虚假图像在不同的分类器中都是虚假的。尽管虚假图像有助于衡量分类器的可靠性，但过滤来自互联网的许多图像以查找更多虚假特征非常耗时。为此，我们利用现有的方法，通过可用的发现的虚假图像来个性化大规模文本到图像扩散模型，并提出一种基于对抗鲁棒模型的神经特征的新的虚假特征相似性损失。准确地说，我们使用来自 Spurious ImageNet 的几张参考图像来微调稳定扩散，并修改了目标，并结合了所提出的虚假特征相似性损失。实验结果表明，我们的方法可以生成在不同分类器中始终是虚假的虚假图像。此外，生成的虚假图像在视觉上与来自 Spurious ImageNet 的参考图像相似。]]></description>
      <guid>https://arxiv.org/abs/2402.08200</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>揭露蜂蜜掺假：通过尖端的热图像卷积神经网络分析在质量保证方面取得突破</title>
      <link>https://arxiv.org/abs/2402.08122</link>
      <description><![CDATA[arXiv:2402.08122v1 公告类型：新
【摘要】：蜂蜜是一种由有机来源产生的天然产品,因其备受推崇的声誉而受到广泛认可。然而，蜂蜜很容易掺假，这种情况对普通民众的福祉和国家的财政福祉都会产生重大影响。检测蜂蜜掺假的传统方法通常需要大量时间且灵敏度有限。本文提出了一种解决上述问题的新方法，即采用卷积神经网络（CNN）根据热图像对蜂蜜样本进行分类。热成像技术的使用在检测掺假物方面具有显着优势，因为它可以揭示由于糖成分、水分含量和其他掺假物质的变化而引起的蜂蜜样品中的温度差异。为了建立一种细致的蜂蜜分类方法，收集了包含真实和受污染蜂蜜样品热图像的完整数据集。使用收集的数据集训练和优化了几种最先进的卷积神经网络 (CNN) 模型。在这组模型中，存在 InceptionV3、Xception、VGG19 和 ResNet 等预训练模型，它们表现出了出色的性能，实现了 88% 到 98% 的分类准确率。此外，我们还实现了更精简、更简单的卷积神经网络 (CNN) 模型，其准确率高达 99%，优于同类模型。这种简化不仅提供了该模型的唯一优势，而且还同时提供了在资源和时间方面更有效的解决方案。这种方法为在蜂蜜行业实施质量控制措施提供了一种可行的方法，从而保证了这种有价值的有机商品的真实性和安全性。]]></description>
      <guid>https://arxiv.org/abs/2402.08122</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>H2O-SDF：使用物体表面场进行 3D 室内重建的两阶段学习</title>
      <link>https://arxiv.org/abs/2402.08138</link>
      <description><![CDATA[arXiv:2402.08138v1 公告类型：新
摘要：使用神经辐射场 (NeRF)、符号距离场 (SDF) 和占用场的先进技术最近已成为 3D 室内场景重建的解决方案。我们引入了一种新颖的两阶段学习方法 H2O-SDF，它可以区分室内环境中的物体和非物体区域。这种方法实现了微妙的平衡，仔细保留了房间布局的几何完整性，同时还捕捉了特定物体复杂的表面细节。我们的两阶段学习框架的基石是引入物体表面场（OSF），这是一个新颖的概念，旨在减轻持续消失的梯度问题，该问题先前阻碍了其他方法中高频细节的捕获。我们提出的方法通过包括消融研究在内的多项实验得到验证。]]></description>
      <guid>https://arxiv.org/abs/2402.08138</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>用于鲁棒多模态半监督学习的多个随机掩蔽自动编码器集成</title>
      <link>https://arxiv.org/abs/2402.08035</link>
      <description><![CDATA[arXiv:2402.08035v1 公告类型：新
摘要：计算机视觉和机器学习中越来越多的现实问题需要考虑世界的多个解释层（模式或视图）并了解它们之间的关系。例如，在通过卫星数据进行地球观测的情况下，能够从其他层（例如水蒸气、积雪、温度等）预测一个观测层（例如植被指数）非常重要，以便最好地了解如何地球系统功能，并且当数据丢失时（例如由于测量失败或错误），还能够可靠地预测一层的信息。]]></description>
      <guid>https://arxiv.org/abs/2402.08035</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>多属性视觉变压器是高效且强大的学习器</title>
      <link>https://arxiv.org/abs/2402.08070</link>
      <description><![CDATA[arXiv:2402.08070v1 公告类型：新
摘要：自诞生以来，视觉变换器 (ViT) 已成为跨多种任务的卷积神经网络 (CNN) 的引人注目的替代品。 ViT 表现出显着的特征，包括全球关注、抗遮挡能力以及对分布变化的适应性。 ViT 的一个未被充分开发的方面是它们的多属性学习潜力，指的是它们同时掌握多个属性相关任务的能力。在本文中，我们深入研究了 ViT 的多属性学习能力，提出了一种简单而有效的策略，通过单个 ViT 网络将各种属性训练为不同的任务。我们评估多属性 ViT 抵御对抗性攻击的弹性，并将其性能与针对单属性设计的 ViT 进行比较。此外，我们还进一步评估了多属性 ViT 对抗最近基于 Transformer 的名为 Patch-Fool 的攻击的鲁棒性。我们对 CelebA 数据集的实证研究结果验证了我们的主张。]]></description>
      <guid>https://arxiv.org/abs/2402.08070</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>Lumos：通过场景文本识别为多模式法学硕士提供支持</title>
      <link>https://arxiv.org/abs/2402.08017</link>
      <description><![CDATA[arXiv:2402.08017v1 公告类型：新
摘要：我们介绍 Lumos，第一个具有文本理解功能的端到端多模态问答系统。 Lumos 的核心是场景文本识别 (STR) 组件，它从第一人称视角图像中提取文本，其输出用于增强多模态大语言模型 (MM-LLM) 的输入。在构建 Lumos 时，我们遇到了与 STR 质量、整体延迟和模型推理相关的众多挑战。在本文中，我们深入研究了这些挑战，并讨论了用于克服这些障碍的系统架构、设计选择和建模技术。我们还对每个组件进行全面评估，展示高质量和高效率。]]></description>
      <guid>https://arxiv.org/abs/2402.08017</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>超越泥浆：越野赛车中计算机视觉的数据集和基准</title>
      <link>https://arxiv.org/abs/2402.08025</link>
      <description><![CDATA[arXiv:2402.08025v1 公告类型：新
摘要：尽管光学字符识别（OCR）和计算机视觉系统取得了重大进展，但在不受约束的环境中拍摄的图像中稳健地识别文本和识别人物仍然是一个持续的挑战。然而，在视觉系统的实际应用中必须克服这些障碍，例如在越野比赛期间拍摄的照片中识别赛车手。为此，我们引入了两个新的具有挑战性的现实世界数据集——越野摩托车赛车手编号数据集（RND）和泥泞赛车手重新识别数据集（MUDD）——以突出当前方法的缺点并推动 OCR 和极端条件下的人员重新识别（ReID）。这两个数据集包含越野比赛期间拍摄的 6,300 多张图像，这些图像表现出各种甚至会破坏现代视觉系统的因素，即泥浆、复杂姿势和运动模糊。我们使用最先进的模型在两个数据集上建立基准性能。现成模型的传输性能很差，在文本识别方面仅达到 15% 的端到端 (E2E) F1 分数，在 ReID 方面仅达到 33% 的 1 级准确率。微调带来了重大改进，使模型在 E2E 文本识别方面的 F1 分数达到 53%，在 ReID 方面达到 79% 的 1 级准确率，但仍达不到良好的性能。我们的分析揭示了现实世界 OCR 和 ReID 中的开放问题，这些问题需要针对领域的技术。通过这些数据集和对模型局限性的分析，我们的目标是促进处理泥浆和复杂姿势等现实世界条件的创新，以推动稳健的计算机视觉的进步。所有数据均来自 PerformancePhoto.co，这是一个专业赛车摄影师、赛车手和车迷使用的网站。该平台上部署了性能最佳的文本识别和 ReID 模型，以支持实时比赛照片搜索。]]></description>
      <guid>https://arxiv.org/abs/2402.08025</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    </channel>
</rss>