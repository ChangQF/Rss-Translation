<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有微分干涉对比 3D 乳腺癌球体选择性堆叠的模糊一致性分割框架</title>
      <link>https://arxiv.org/abs/2406.05349</link>
      <description><![CDATA[arXiv:2406.05349v1 公告类型：新
摘要：三维 (3D) 球体建模在研究乳腺癌细胞侵袭性行为方面的能力引起了越来越多的关注。基于深度学习的图像处理框架在加速细胞形态分析过程方面非常有效。然而，在多个 z 切片下捕捉 3D 细胞时拍摄的失焦照片可能会对深度学习模型产生负面影响。在这项工作中，我们创建了一种新算法来处理模糊图像，同时保持堆叠图像的质量。此外，我们提出了一种独特的训练架构，利用一致性训练来帮助减少应用密集切片堆叠时模型的偏差。此外，通过利用自训练方法，模型在稀疏切片堆叠效应下的稳定性得到了提高。新的模糊堆叠技术和训练流程与建议的架构和自训练机制相结合，提供了一个创新且易于使用的框架。我们的方法在定量和定性方面都产生了值得注意的实验结果。]]></description>
      <guid>https://arxiv.org/abs/2406.05349</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>2024 年野外像素级视频理解 (CVPR'24 PVUW) 挑战赛视频全景分割和视频语义分割最佳长视频一致性第一名</title>
      <link>https://arxiv.org/abs/2406.05352</link>
      <description><![CDATA[arXiv:2406.05352v1 公告类型：新
摘要：第三届像素级野外视频理解 (PVUW CVPR 2024) 挑战赛旨在通过对大规模野外视频全景分割 (VIPSeg) 测试集和大规模野外视频场景解析 (VSPW) 测试集中引入的具有挑战性的视频和场景对视频全景分割 (VPS) 和视频语义分割 (VSS) 进行基准测试，提高视频理解的最新水平。本文详细介绍了我们在 PVUW&#39;24 VPS 挑战赛中获得第一名的研究工作，在所有指标中建立了最先进的结果，包括视频全景质量 (VPQ) 和分割和跟踪质量 (STQ)。经过细微调整，我们的方法还在 PVUW&#39;24 VSS 挑战赛中取得了 mIoU（平均交并比）指标排名第三的成绩，以及 VC16（16 帧视频一致性）指标排名第一的成绩。我们的获胜解决方案建立在巨大的基础视觉转换模型 (DINOv2 ViT-g) 和经过验证的多阶段解耦视频实例分割 (DVIS) 视频理解框架的基础之上。]]></description>
      <guid>https://arxiv.org/abs/2406.05352</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>度量卷积：自适应卷积的统一理论</title>
      <link>https://arxiv.org/abs/2406.05400</link>
      <description><![CDATA[arXiv:2406.05400v1 公告类型：新
摘要：标准卷积在图像处理和深度学习中很普遍，但它们的固定核设计限制了适应性。已经提出了几种参考核网格的变形策略。然而，它们缺乏统一的理论框架。通过返回图像的度量视角，现在将其视为配备局部和测地线距离概念的二维流形，无论是对称的（黎曼度量）还是不对称的（芬斯勒度量），我们提供了一个统一的原则：内核位置是隐式度量单位球的样本。凭借这种新视角，我们还提出了度量卷积，这是一种从显式信号相关度量中采样单位球的新方法，为可解释的运算符提供了几何正则化。该框架与基于梯度的优化兼容，可以直接替换应用于输入图像或神经网络深度特征的现有卷积。度量卷积通常需要更少的参数并提供更好的泛化。我们的方法在标准去噪和分类任务中表现出色。]]></description>
      <guid>https://arxiv.org/abs/2406.05400</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>整合文本和图像预训练实现多模态算法推理</title>
      <link>https://arxiv.org/abs/2406.05318</link>
      <description><![CDATA[arXiv:2406.05318v1 公告类型：新
摘要：本文介绍了 CVPR 多模态算法推理任务 2024 的 SMART-101 挑战赛的解决方案。与传统的视觉问答任务不同，此挑战赛评估神经网络在解决专门为 6-8 岁儿童设计的视觉语言谜题时的抽象、推理和泛化能力。我们的模型基于两个预训练模型，分别用于从文本和图像中提取特征。为了整合来自不同模态的特征，我们采用了一个带有注意机制的融合层。我们探索了不同的文本和图像预训练模型，并在 SMART-101 数据集上对集成分类器进行了微调。实验结果表明，在拼图分割的数据分割方式下，我们提出的集成分类器取得了优异的性能，验证了多模态预训练表示的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.05318</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>MotionClone：无需训练的运动克隆，实现可控视频生成</title>
      <link>https://arxiv.org/abs/2406.05338</link>
      <description><![CDATA[arXiv:2406.05338v1 公告类型：新
摘要：基于运动的可控文本到视频生成涉及控制视频生成的运动。以前的方法通常需要训练模型来编码运动提示或微调视频扩散模型。然而，当应用于训练域之外时，这些方法通常会导致次优的运动生成。在这项工作中，我们提出了 MotionClone，这是一个无需训练的框架，可以从参考视频中克隆运动以控制文本到视频的生成。我们在视频反转中使用时间注意力来表示参考视频中的运动，并引入主要的时间注意力指导来减轻注意力权重内嘈杂或非常细微的运动的影响。此外，为了帮助生成模型合成合理的空间关系并增强其提示跟随能力，我们提出了一种位置感知语义指导机制，该机制利用参考视频中前景的粗略位置和原始的无分类器指导特征来指导视频生成。大量实验表明，MotionClone 在全局相机运动和局部物体运动方面都表现出色，在运动保真度、文本对齐和时间一致性方面具有显著的优势。]]></description>
      <guid>https://arxiv.org/abs/2406.05338</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>YouTube SFV+HDR 质量数据集</title>
      <link>https://arxiv.org/abs/2406.05305</link>
      <description><![CDATA[arXiv:2406.05305v1 公告类型：新
摘要：在过去几年中，短视频（SFV）的受欢迎程度急剧上升，已成为拥有数十亿观众的现象级视频类别。同时，高动态范围（HDR）作为一项高级功能也在视频共享平台上越来越受欢迎。作为一个影响巨大的热门话题，SFV 和 HDR 为视频质量研究带来了新的问题：1）SFV+HDR 质量评估与传统的用户生成内容（UGC）质量评估有显著不同吗？2）为传统 UGC 设计的客观质量指标是否仍然适用于 SFV+HDR？为了回答上述问题，我们创建了第一个具有可靠主观质量分数的大规模 SFV+HDR 数据集，涵盖了 10 个热门内容类别。此外，我们还引入了一个通用采样框架，以最大限度地提高数据集的代表性。我们对短视频 SDR 和 HDR 的主观质量分数进行了全面分析，并讨论了最先进的 UGC 质量指标的可靠性和潜在的改进。]]></description>
      <guid>https://arxiv.org/abs/2406.05305</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>弱监督集合一致性学习改善单细胞图像的形态学分析</title>
      <link>https://arxiv.org/abs/2406.05308</link>
      <description><![CDATA[arXiv:2406.05308v1 公告类型：新
摘要：光学池筛选 (OPS) 是一种将高内涵显微镜与基因工程相结合以研究疾病基因功能的强大工具。高内涵图像的表征仍然是一个活跃的研究领域，目前正在通过应用自监督学习和视觉转换器进行快速创新。在本研究中，我们提出了一种集合级一致性学习算法 Set-DINO，该算法将自监督学习与弱监督相结合，以改进单细胞图像中扰动效应的学习表示。我们的方法利用 OPS 实验的重复结构（即在批次内和批次间经历相同遗传扰动的细胞）作为弱监督的一种形式。我们对具有超过 5000 个遗传扰动的大规模 OPS 数据集进行了广泛的实验，并证明 Set-DINO 有助于减轻混杂因素的影响并编码更多具有生物学意义的信息。特别是，与常用的形态学分析方法相比，Set-DINO 能够以更高的准确度回忆已知的生物学关系，这表明它可以从利用 OPS 的药物靶点发现活动中获得更可靠的见解。]]></description>
      <guid>https://arxiv.org/abs/2406.05308</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>VISTA3D：用于 3D 计算机断层扫描的多功能成像分割和注释模型</title>
      <link>https://arxiv.org/abs/2406.05285</link>
      <description><![CDATA[arXiv:2406.05285v1 公告类型：新
摘要：分割基础模型引起了人们的极大兴趣，然而，它们都不足以满足 3D 计算机断层扫描 (CT) 图像的使用情况。现有的工作使用在自然图像上训练的 2D 基础模型对医学图像进行微调，但交互式分割（尤其是 2D 分割）对于 3D 扫描来说太耗时，对于大型队列分析来说用处不大。能够执行开箱即用的自动分割的模型更受欢迎。然而，以这种方式训练的模型缺乏对新肿瘤等看不见的物体进行分割的能力。因此，对于 3D 医学图像分析，理想的分割解决方案可能具有两个特点：覆盖主要器官类别的精确开箱即用性能，以及对新结构的有效适应或零样本能力。本文讨论了 3D CT 分割基础模型应具备哪些特征，并介绍了 VISTA3D，即多功能影像分割和注释模型。该模型在 11454 个体积上进行系统训练，涵盖 127 种人体解剖结构和各种病变，并提供准确的开箱即用分割。该模型的设计还实现了最先进的 3D 零样本交互式分割。新颖的模型设计和训练方法代表着朝着开发多功能医学图像基础模型迈出了有希望的一步。代码和模型权重将很快发布。可以在 https://build.nvidia.com/nvidia/vista-3d 上试用在线演示的早期版本。]]></description>
      <guid>https://arxiv.org/abs/2406.05285</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>最佳眼科医生：在初始化时通过稀疏生成器寻找图像先验</title>
      <link>https://arxiv.org/abs/2406.05288</link>
      <description><![CDATA[arXiv:2406.05288v1 公告类型：新
摘要：我们引入了最佳眼科手术 (OES)，这是一个用于修剪和训练深度图像生成器网络的框架。通常，未经训练的深度卷积网络（包括图像采样操作）可作为有效的图像先验 (Ulyanov 等人，2018)。然而，由于过度参数化，它们往往会在图像恢复任务中过度拟合噪声。OES 通过在随机初始化时自适应地将网络修剪到参数不足的水平来解决这个问题。这个过程只需通过掩蔽就可以有效地捕获低频图像成分，即使没有训练。当训练这些修剪后的子网络以适应嘈杂的图像时，我们称之为稀疏 DIP，可以抵抗对噪声的过度拟合。这种好处来自于参数不足和掩蔽的正则化效应，将它们限制在图像先验的流形中。我们证明，通过 OES 修剪的子网络优于其他领先的修剪方法，例如彩票假说，众所周知，彩票假说对于图像恢复任务而言并非最佳选择 (Wu et al., 2023)。我们进行了广泛的实验，证明了 OES 掩码的可迁移性和稀疏子网络在图像生成中的特性。代码可在 https://github.com/Avra98/Optimal-Eye-Surgeon.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.05288</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>CPLIP：具有全面视觉-语言对齐的组织病理学零样本学习</title>
      <link>https://arxiv.org/abs/2406.05205</link>
      <description><![CDATA[arXiv:2406.05205​​v1 公告类型：新
摘要：本文提出了综合病理语言图像预训练 (CPLIP)，这是一种新的无监督技术，旨在增强组织病理学中图像和文本的对齐，以完成分类和分割等任务。该方法通过利用大量数据而不需要基本事实注释来丰富视觉语言模型。CPLIP 涉及构建病理特定词典、使用语言模型为图像生成文本描述以及通过预训练模型检索每个文本片段的相关图像。然后使用多对多对比学习方法对模型进行微调，以在两种模态中对齐复杂的相互关联概念。在多个组织病理学任务中进行评估后，CPLIP 在零样本学习场景中显示出显着的改进，在可解释性和稳健性方面均优于现有方法，并为视觉语言模型在该领域的应用设定了更高的基准。为了鼓励进一步研究和复制，CPLIP 的代码可在 GitHub 上获取，网址为 https://cplip.github.io/]]></description>
      <guid>https://arxiv.org/abs/2406.05205</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>拆分与拟合：通过结构感知 Voronoi 分区学习 B-Reps</title>
      <link>https://arxiv.org/abs/2406.05261</link>
      <description><![CDATA[arXiv:2406.05261v1 公告类型：新
摘要：我们介绍了一种获取 3D CAD 模型边界表示 (B-Reps) 的新方法，该方法涉及两个步骤：首先应用空间分区（称为“split”），然后执行“fit”操作以在每个分区内导出单个基元。具体来说，我们的分区旨在生成一组地面实况 (GT) B-Rep 基元的经典 Voronoi 图。与之前自下而上（通过直接基元拟合或点聚类）的 B-Rep 构造相比，我们的 Split-and-Fit 方法是自上而下且结构感知的，因为 Voronoi 分区明确揭示了基元的数量和基元之间的连接。我们设计了一个神经网络，通过二元分类从输入点云或距离场预测 Voronoi 图。我们展示了我们的网络（即神经 Voronoi 图的 NVD-Net），它可以有效地从训练数据中学习 CAD 模型的 Voronoi 分区，并表现出卓越的泛化能力。大量实验和评估表明，由此产生的由参数曲面、曲线和顶点组成的 B-Reps 比现有替代方案获得的 B-Reps 更合理，重建质量显著提高。代码将在 https://github.com/yilinliu77/NVDNet 上发布。]]></description>
      <guid>https://arxiv.org/abs/2406.05261</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>用途：用于开放词汇图像分割的通用片段嵌入</title>
      <link>https://arxiv.org/abs/2406.05271</link>
      <description><![CDATA[arXiv:2406.05271v1 公告类型：新
摘要：开放词汇图像分割任务涉及将图像划分为语义上有意义的片段，并使用灵活的文本定义类别对其进行分类。最近的基于视觉的基础模型，例如 Segment Anything Model (SAM)，在生成与类别无关的图像片段方面表现出色。开放词汇图像分割的主要挑战现在在于准确地将这些片段分类为文本定义的类别。在本文中，我们引入了通用片段嵌入 (USE) 框架来应对这一挑战。该框架由两个关键组件组成：1) 一个数据管道，旨在有效地管理大量不同粒度的片段文本对，2) 一个通用片段嵌入模型，能够将片段精确分类到大量文本定义的类别中。USE 模型不仅可以帮助开放词汇图像分割，还可以促进其他下游任务（例如查询和排名）。通过对语义分割和部分分割基准的全面实验研究，我们证明 USE 框架优于最先进的开放词汇分割方法。]]></description>
      <guid>https://arxiv.org/abs/2406.05271</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>合成训练图像的未实现承诺：使用检索到的真实图像效果更佳</title>
      <link>https://arxiv.org/abs/2406.05184</link>
      <description><![CDATA[arXiv:2406.05184v1 公告类型：新
摘要：生成式文本到图像模型使我们能够以可控的方式合成无限量的图像，这激发了最近许多使用合成数据训练视觉模型的努力。然而，每个合成图像最终都源自用于训练生成器的上游数据。与直接对上游数据的相关部分进行训练相比，中间生成器提供了什么附加价值？将这个问题置于图像分类的环境中，我们将由稳定扩散（在 LAION-2B 数据集上训练的生成模型）生成的与任务相关的目标合成数据的微调与直接从 LAION-2B 检索的目标真实图像的微调进行比较。我们表明，虽然合成数据可以使一些下游任务受益，但从我们的简单检索基线来看，它普遍与真实数据相匹配或优于真实数据。我们的分析表明，这种表现不佳部分是由于生成器伪影和合成图像中与任务相关的视觉细节不准确。总体而言，我们认为检索是使用合成数据进行训练时需要考虑的关键基准——当前方法尚未超越这一基准。我们在 https://github.com/scottgeng00/unmet-promise 上发布了代码、数据和模型。]]></description>
      <guid>https://arxiv.org/abs/2406.05184</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>DiffusionPID：通过部分信息分解解释扩散</title>
      <link>https://arxiv.org/abs/2406.05191</link>
      <description><![CDATA[arXiv:2406.05191v1 公告类型：新
摘要：文本到图像的扩散模型在从文本输入生成自然图像方面取得了重大进展，并展示了学习和表示复杂视觉语义关系的能力。虽然这些扩散模型取得了显著的成功，但推动其性能的根本机制尚未完全解释，围绕它们学习什么、它们如何表示视觉语义关系以及为什么它们有时无法概括，仍有许多未解问题。我们的工作提出了扩散部分信息分解 (DiffusionPID)，这是一种新技术，它应用信息论原理将输入文本提示分解为其基本组成部分，从而能够详细检查各个标记及其相互作用如何塑造生成的图像。我们引入了一种正式的方法来分析唯一性、冗余性和协同效应，方法是将 PID 应用于图像和像素级别的去噪模型。这种方法使我们能够描述各个标记及其相互作用如何影响模型输出。我们首先对模型用来唯一定位特定概念的特征进行细粒度分析，然后将我们的方法应用于偏见分析，并表明它可以恢复性别和种族偏见。最后，我们使用我们的方法从模型的角度直观地描述词语歧义和相似性，并说明我们的方法在及时干预方面的有效性。我们的结果表明，PID 是评估和诊断文本到图像扩散模型的有力工具。]]></description>
      <guid>https://arxiv.org/abs/2406.05191</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>电影精彩片段生成系统的打斗场景检测</title>
      <link>https://arxiv.org/abs/2406.05152</link>
      <description><![CDATA[arXiv:2406.05152v1 公告类型：新 
摘要：在这篇基于研究的项目论文中，我们使用双向长短期记忆 (BiLSTM) 网络，提供了一种新颖的打斗场景检测 (FSD) 模型，该模型可用于基于深度学习和神经网络的电影精彩片段生成系统 (MHGS)。电影通常有打斗场面来让观众惊叹不已。对于预告片生成或任何其他精彩片段生成的应用，首先手动识别所有此类场景，然后编译它们以生成用于目的的精彩片段是非常繁琐的。我们提出的 FSD 系统利用电影场景的时间特征，因此能够自动识别打斗场景。从而有助于有效制作引人入胜的电影精彩片段。我们观察到，所提出的解决方案的准确率为 93.5%，高于 2D CNN 和霍夫森林，后者的准确率为 92%，并且明显高于 3D CNN，后者的准确率为 65%。]]></description>
      <guid>https://arxiv.org/abs/2406.05152</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    </channel>
</rss>