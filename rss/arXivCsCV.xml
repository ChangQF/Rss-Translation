<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 28 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于鸟瞰人体检测的合成数据中的人体姿势多样化</title>
      <link>https://arxiv.org/abs/2405.15939</link>
      <description><![CDATA[arXiv:2405.15939v1 公告类型：新 
摘要：我们提出了一个框架，用于在鸟瞰人体检测的合成数据集中使人体姿势多样化。我们的方法首先使用姿势生成器构建一组新颖的姿势，然后更改现有合成数据集中的图像以呈现新颖的姿势，同时使用图像转换器保持原始风格。由于与新姿势相对应的图像在训练中不可用，因此图像翻译器被训练为仅当输入和目标姿势相似时才适用，因此训练不需要新姿势及其对应的图像。接下来，我们从新颖姿势集中选择一系列目标新颖姿势，使用 Dijkstra 算法确保彼此更接近的姿势在序列中相邻定位。最后，我们按顺序重复将图像转换器应用到每个目标姿势，以生成一组新颖的姿势图像，表示来自源姿势的各种不同的有限身体运动。实验表明，无论合成数据如何用于训练或数据大小，在训练中利用姿势多样化的合成数据集通常比在三个鸟瞰人体检测基准（VisDrone、 Okutama-Action 和 ICG）在少镜头状态下。]]></description>
      <guid>https://arxiv.org/abs/2405.15939</guid>
      <pubDate>Tue, 28 May 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>激活器：GLU 激活是视觉转换器的核心功能</title>
      <link>https://arxiv.org/abs/2405.15953</link>
      <description><![CDATA[arXiv:2405.15953v1 公告类型：新 
摘要：目前，Transformer 架构是深度学习解决的各种任务取得许多成功的主要驱动力，特别是自然语言处理 (NLP) 领域的最新进展，最终导致大型语言模型 (LLM)。此外，Transformer 架构引起了计算机视觉 (CV) 研究人员和从业者的广泛兴趣，它使得视觉相关任务取得了许多进步，并为共享相同架构的多任务和多模式深度学习架构打开了大门。工作原理。这些架构的一个缺点是它们依赖于带有 softmax 激活函数的缩放点积注意力机制，这种机制的计算成本很高，并且需要大量的计算能力来进行训练和推理。本文研究了用在多层感知器 (MLP) 结构中结合门控线性单元 (GLU) 激活的架构以及传统 Transformer 设计中包含的默认 MLP 来替代 Transformer 架构通常采用的注意机制。本文向前迈出的另一步是消除第二个非门控 MLP，以进一步降低计算成本。本研究进行的实验评估表明，所提出的修改和减少都提供了与基线架构相关的竞争性能，支持这项工作的目标，即建立一种更有效且更有能力的传统注意力机制替代方案，作为设计变压器的核心组件架构。]]></description>
      <guid>https://arxiv.org/abs/2405.15953</guid>
      <pubDate>Tue, 28 May 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>将通用预训练视觉变换器重塑为以对象为中心的场景编码器，用于操作策略</title>
      <link>https://arxiv.org/abs/2405.15916</link>
      <description><![CDATA[arXiv:2405.15916v1 公告类型：新
摘要：通用可重复使用的预训练图像表示编码器已成为许多计算机视觉任务方法的标准组件。然而，作为机器人的视觉表示，它们的实用性有限，导致最近出现了一波努力来预训练机器人专用的图像编码器，这些编码器比通用编码器更适合机器人任务。我们提出了 Scene Objects From Transformers，简称 SOFT，它是预训练视觉变换器 (PVT) 模型的包装器，无需进一步训练即可弥补这一差距。SOFT 不是仅从最后一层激活构建表示，而是从 PVT 注意力中个体化和定位类似对象的实体，并用 PVT 激活描述它们，从而产生以对象为中心的嵌入。在通用预训练视觉变换器 PVT 的标准选择中，我们在每种情况下都证明了在 SOFT(PVT) 上训练的策略远远超过了模拟和真实环境中操作任务的标准 PVT 表示，接近最先进的机器人感知表示。代码、附录和视频：https://sites.google.com/view/robot-soft/]]></description>
      <guid>https://arxiv.org/abs/2405.15916</guid>
      <pubDate>Tue, 28 May 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>可控变压器</title>
      <link>https://arxiv.org/abs/2405.15932</link>
      <description><![CDATA[arXiv:2405.15932v1 公告类型：新 
摘要：在这项工作中，我们引入了 Steerable Transformers，它是 Vision Transformer 机制的扩展，可保持与特殊欧几里得群 $\mathrm{SE}(d)$ 的等变性。我们提出了一种等变注意力机制，该机制对可操纵卷积提取的特征进行操作。我们的网络在傅里叶空间中运行，利用傅里叶空间非线性。我们在二维和三维方面的实验表明，向可操纵卷积网络添加可操纵变换器编码器层可以提高性能。]]></description>
      <guid>https://arxiv.org/abs/2405.15932</guid>
      <pubDate>Tue, 28 May 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>CNN 中偏差校正的神经符号框架</title>
      <link>https://arxiv.org/abs/2405.15886</link>
      <description><![CDATA[arXiv:2405.15886v1 公告类型：新 
摘要：最近解释卷积神经网络 (CNN) 的工作重点是将 CNN 过滤器的激活转化为分层答案集编程 (ASP) 规则集。众所周知，CNN 过滤器可以捕获高级图像概念，因此规则集中的谓词被映射到其相应过滤器表示的概念。因此，规则集根据 CNN 为任何图像分类任务学习的概念有效地例证了 CNN 的决策过程。这些规则集有助于揭示和理解 CNN 中的偏差，尽管有效纠正偏差仍然是一个挑战。我们引入了一个名为 NeSyBiCor 的神经符号框架，用于在经过训练的 CNN 中进行偏差校正。给定 CNN 偏向的符号概念（表示为 ASP 约束），我们将不需要和需要的概念转换为其相应的向量表示。然后，使用我们新颖的语义相似性损失对 CNN 进行重新训练，该损失将过滤器推离不需要的概念的表示，同时使它们更接近需要的概念。再训练后得到的最终ASP规则集高度满足约束条件，体现了CNN对图像分类任务知识的修正。我们证明，我们的 NeSyBiCor 框架通过大大减小最终偏差校正规则集的大小，成功地纠正了使用 Places 数据集中的类子集训练的 CNN 的偏差，同时牺牲了最小的准确性并提高了可解释性。初始规则集。]]></description>
      <guid>https://arxiv.org/abs/2405.15886</guid>
      <pubDate>Tue, 28 May 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>通过重新参数化的 DDIM 进行分数蒸馏</title>
      <link>https://arxiv.org/abs/2405.15891</link>
      <description><![CDATA[arXiv:2405.15891v1 公告类型：新 
摘要：虽然 2D 扩散模型生成逼真、高细节的图像，但基于这些 2D 扩散模型构建的分数蒸馏采样 (SDS) 等 3D 形状生成方法会生成卡通般的过度平滑形状。为了帮助解释这种差异，我们表明分数蒸馏中使用的图像引导可以理解为二维去噪生成过程的速度场，直到噪声项的选择。特别是，在变量更改后，SDS 类似于具有不同采样噪声项的去噪扩散隐式模型 (DDIM) 的高方差版本：SDS 引入了噪声 i.i.d.。每一步都是随机的，而 DDIM 是从之前的噪声预测中推断出来的。这种过度的方差可能会导致过度平滑和不切实际的输出。我们证明，通过在每个 SDS 更新步骤中反转 DDIM 可以恢复更好的噪声近似。这种修改使得 SDS 的 2D 图像生成过程几乎与 DDIM 相同。在 3D 中，它消除了过度平滑，保留了高频细节，并使生成质量更接近 2D 采样器。实验上，与其他最先进的分数蒸馏方法相比，我们的方法实现了更好或相似的 3D 生成质量，所有这些都无需训练额外的神经网络或多视图监督，并且提供了有关 2D 和 3D 资产生成之间关系的有用见解扩散模型。]]></description>
      <guid>https://arxiv.org/abs/2405.15891</guid>
      <pubDate>Tue, 28 May 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>ExactDreamer：通过精确分数匹配创建高保真文本到 3D 内容</title>
      <link>https://arxiv.org/abs/2405.15914</link>
      <description><![CDATA[arXiv:2405.15914v1 公告类型：新 
摘要：文本转 3D 内容创建是一个快速发展的研究领域。鉴于 3D 数据的稀缺，当前的方法通常采用预先训练的 2D 扩散模型来进行 3D 合成。在这些方法中，分数蒸馏采样（SDS）已被广泛采用。然而，过度平滑的问题对 3D 模型的高保真生成造成了重大限制。为了应对这一挑战，LucidDreamer将SDS中的去噪扩散概率模型（DDPM）替换为去噪扩散隐式模型（DDIM）来构建区间得分匹配（ISM）。然而，ISM不可避免地继承了DDIM的不一致性，导致DDIM反演过程中出现重构错误。这会导致 3D 对象的详细生成性能不佳并导致内容丢失。为了缓解这些问题，我们提出了一种名为精确分数匹配（ESM）的新方法。具体来说，ESM 利用辅助变量在数学上保证 DDIM 逆向过程中的精确恢复。此外，为了有效捕获原始变量和辅助变量的动态变化，预训练扩散模型的 LoRA 实现了这些精确路径。大量实验证明了 ESM 在文本到 3D 生成中的有效性，特别突出了其在细节生成方面的优越性。]]></description>
      <guid>https://arxiv.org/abs/2405.15914</guid>
      <pubDate>Tue, 28 May 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>在多标签图像分类中混合多个部分标记的样本可带来免费的性能增益</title>
      <link>https://arxiv.org/abs/2405.15860</link>
      <description><![CDATA[arXiv:2405.15860v1 公告类型：新 
摘要：多标签图像分类数据集通常是部分标记的，其中许多标签丢失，这对训练准确的深度分类器构成了重大挑战。然而，强大的Mixup样本混合数据增强不能很好地利用来解决这一挑战，因为它无法对未知标签进行线性插值来构建增强样本。在本文中，我们提出了 LogicMix，这是一种专为此类部分标记数据集设计的 Mixup 变体。 LogicMix 通过逻辑 OR 混合样本标签，以便利用 OR 的逻辑等价性（包括支配律和恒等律）正确混合未知标签。与仅混合两个样本的 Mixup 不同，LogicMix 可以混合多个 ($\geq2$) 部分标记的样本，构建视觉上更混乱的增强样本来规范训练。在各种部分标记数据集场景的实验中，LogicMix 比其他比较的 Mixup 变体更通用、更有效。此外，它是即插即用的，只需要很少的计算，因此它可以很容易地插入现有框架中，与其他方法协作来提高模型性能，而对训练时间的影响可以忽略不计，正如大量实验所证明的那样。特别是，通过 LogicMix、RandAugment、课程标签和类别微调的协作，我们在 MS-COCO、VG-200 和 Pascal VOC 2007 基准数据集上获得了最先进的性能。显着的通用性、有效性、协作性和简单性表明 LogicMix 有望成为一种流行且重要的数据增强方法。]]></description>
      <guid>https://arxiv.org/abs/2405.15860</guid>
      <pubDate>Tue, 28 May 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>使用双向 SSM 扩展 Diffusion Mamba，以实现高效的图像和视频生成</title>
      <link>https://arxiv.org/abs/2405.15881</link>
      <description><![CDATA[arXiv:2405.15881v1 公告类型：新 
摘要：在最近的发展中，以其选择性状态空间方法而闻名的 Mamba 架构在长序列的高效建模方面显示出了潜力。然而，它在图像生成中的应用仍未得到充分探索。传统的扩散变换器（DiT）利用自注意力块，是有效的，但它们的计算复杂度与输入长度呈二次方缩放，限制了它们在高分辨率图像中的使用。为了应对这一挑战，我们引入了一种新颖的扩散架构，Diffusion Mamba (DiM)，它放弃了传统的注意力机制，转而采用可扩展的替代方案。通过利用 Mamba 架构的固有效率，DiM 实现了快速推理时间并减少了计算负载，同时保持了序列长度的线性复杂性。我们的架构不仅可以有效地扩展，而且在图像和视频生成任务中都优于现有的扩散变压器。结果证实了 DiM 的可扩展性和效率，为图像和视频生成技术建立了新的基准。这项工作推动了生成模型领域的发展，并为可扩展架构的进一步应用铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2405.15881</guid>
      <pubDate>Tue, 28 May 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>用于 LiDAR 点云场景分割的 3D 可学习 Supertoken Transformer</title>
      <link>https://arxiv.org/abs/2405.15826</link>
      <description><![CDATA[arXiv:2405.15826v1 公告类型：新 
摘要：3D Transformers 在点云理解和表示方面取得了巨大成功。然而，用于大规模激光雷达点云场景分割的有效且高效的 Transformers 仍有很大的进一步发展空间。本文提出了一种新颖的 3D Transformer 框架，称为 3D Learnable Supertoken Transformer (3DLST)。主要贡献总结如下。首先，我们引入了第一个动态超级令牌优化（DSO）块，用于高效令牌聚类和聚合，其中可学习的超级令牌定义避免了传统超级点生成的耗时预处理。由于可学习的超级令牌可以在网络学习过程中通过多级深度特征动态优化，因此它们是针对语义同质感知令牌聚类而定制的。其次，提出了一种高效的交叉注意力引导上采样（CAU）块，用于从优化的超级令牌重建令牌。第三，3DLST配备了新颖的W-net架构，而不是常见的U-net设计，更适合基于Transformer的特征学习。在三个具有挑战性的 LiDAR 数据集（机载多光谱 LiDAR (MS-LiDAR)（平均 F1 分数的 89.3%）、DALES（mIoU 的 80.2%）和 Toronto-3D 数据集（mIoU 的 80.4%））上的 SOTA 性能证明了其优越性3DLST的特点及其对各种LiDAR点云数据（机载MS-LiDAR、航空LiDAR和车载LiDAR数据）的强大适应性。此外，3DLST 在算法效率方面也取得了令人满意的结果，比之前性能最佳的方法快了 5 倍。]]></description>
      <guid>https://arxiv.org/abs/2405.15826</guid>
      <pubDate>Tue, 28 May 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>用于点云处理的具有动态令牌聚合的高效点转换器</title>
      <link>https://arxiv.org/abs/2405.15827</link>
      <description><![CDATA[arXiv:2405.15827v1 公告类型：新 
摘要：近年来，由于 3D Transformer 的发展，点云处理和分析取得了长足的进步。然而，现有的 3D Transformer 方法由于其庞大且冗余的注意力图，通常计算成本昂贵且效率低下。由于需要耗时的点云采样和分组过程，它们也往往很慢。为了解决这些问题，我们提出了一种具有动态令牌聚合功能的高效点 TransFormer（DTA-Former），用于点云表示和处理。首先，我们提出了一种高效的可学习令牌稀疏化（LTS）块，它考虑局部和全局语义信息来自适应选择关键令牌。其次，为了实现稀疏令牌的特征聚合，我们在 3D Transformer 范例中提出了第一个动态令牌聚合（DTA）块，为我们的模型提供了强大的聚合特征，同时防止信息丢失。之后，使用基于双注意力 Transformer 的全局特征增强（GFE）块来提高模型的表示能力。 DTA-Former 配备了 LTS、DTA 和 GFE 模块，通过分层特征学习实现了出色的分类结果。最后，引入了一种新颖的迭代令牌重建（ITR）块进行密集预测，从而在迭代重建过程中逐渐优化令牌的语义特征及其语义关系。基于ITR，我们提出了一种新的W-net架构，它比常见的U-net设计更适合基于Transformer的特征学习。大量的实验证明了我们方法的优越性。它在 ModelNet40、ShapeNet 和机载多光谱 LiDAR (MS-LiDAR) 数据集上实现了 SOTA 性能，比之前的点 Transformer 快 30 倍。]]></description>
      <guid>https://arxiv.org/abs/2405.15827</guid>
      <pubDate>Tue, 28 May 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>SpotNet：一种以图像为中心、激光雷达锚定的远距离感知方法</title>
      <link>https://arxiv.org/abs/2405.15843</link>
      <description><![CDATA[arXiv:2405.15843v1 公告类型：新
摘要：在本文中，我们提出了 SpotNet：一种快速、单阶段、以图像为中心但以 LiDAR 为锚定的长距离 3D 物体检测方法。我们证明了我们的 LiDAR/图像传感器融合方法，结合 2D 和 3D 检测任务的联合学习，可以在非常稀疏的 LiDAR 支持下实现准确的 3D 物体检测。与最近的鸟瞰图 (BEV) 传感器融合方法不同，后者随范围 $r$ 缩放为 $O(r^2)$，而 SpotNet 随范围缩放为 $O(1)$。我们认为这种架构非常适合利用每个传感器的优势，即从图像中进行语义理解和从 LiDAR 数据中进行精确的测距。最后，我们表明，在 LiDAR 点上进行锚定检测消除了回归距离的需要，因此该架构能够将 2MP 分辨率图像转换为 8MP 分辨率图像而无需重新训练。]]></description>
      <guid>https://arxiv.org/abs/2405.15843</guid>
      <pubDate>Tue, 28 May 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>多模态人类行为识别中从 CNN 到 Transformers：一项调查</title>
      <link>https://arxiv.org/abs/2405.15813</link>
      <description><![CDATA[arXiv:2405.15813v1 公告类型：新 
摘要：由于其广泛的应用，人体动作识别是计算机视觉中研究最广泛的研究问题之一。最近的研究表明，与依赖单一数据模态相比，使用多模态数据解决该问题可以带来更优越的性能。在过去十年采用深度学习进行视觉建模的过程中，动作识别方法主要依赖于卷积神经网络（CNN）。然而，最近变形金刚在视觉建模领域的兴起也导致了动作识别任务的范式转变。这项调查捕捉了这一转变，同时重点关注多模式人类行为识别 (MHAR)。多模态计算模型归纳的独特之处在于“融合”各个数据模态的特征的过程。因此，我们特别关注 MHAR 方法的融合设计方面。我们分析了这方面的经典和新兴技术，同时还强调了 CNN 和 Transformer 构建块适应整体问题的流行趋势。我们特别强调最近的设计选择，这些选择导致了更高效的 MHAR 模型。与从广泛的角度讨论人类行为识别的现有评论不同，本次调查专门旨在通过确定有前途的架构和融合设计选择来训练实用模型，从而突破 MHAR 研究的界限。我们还从规模和评估的角度对多模式数据集进行了展望。最后，在回顾的文献的基础上，我们讨论了 MHAR 面临的挑战和未来的途径。]]></description>
      <guid>https://arxiv.org/abs/2405.15813</guid>
      <pubDate>Tue, 28 May 2024 06:19:39 GMT</pubDate>
    </item>
    <item>
      <title>重新思考单图像去雾的初等函数融合</title>
      <link>https://arxiv.org/abs/2405.15817</link>
      <description><![CDATA[arXiv:2405.15817v1 公告类型：新 
摘要：本文通过提出一种创新的去雾网络（CL2S）来解决当前图像去雾领域物理模型的局限性。它以 DM2F 模型为基础，识别了消融实验中的问题，并用三角（正弦）模型取代了原始的对数函数模型。这种替代旨在更好地适应雾霾复杂多变的分布。该方法还集成了大气散射模型和其他基本函数来增强去雾性能。实验结果表明，CL2S 在多个去雾数据集上取得了出色的性能，特别是在保持图像细节和颜色真实性方面。此外，补充 DM2F 的系统消融实验验证了对 DM2F 的担忧，并证实了所提出的 CL2S 模型中功能组件的必要性和有效性。我们的代码可在 \url{https://github.com/YesianRohn/CL2S} 获取，也可以访问相应的预训练模型。]]></description>
      <guid>https://arxiv.org/abs/2405.15817</guid>
      <pubDate>Tue, 28 May 2024 06:19:39 GMT</pubDate>
    </item>
    <item>
      <title>前沿科学图像视觉转换器中的序列长度缩放</title>
      <link>https://arxiv.org/abs/2405.15780</link>
      <description><![CDATA[arXiv:2405.15780v1 公告类型：新 
摘要：视觉变换器（ViT）由于其处理大序列长度的能力，对于科学图像（包括地球科学应用）的基础模型至关重要。虽然文本转换器启发了 ViT 中序列长度的缩放，但针对 ViT 进行调整会带来独特的挑战。我们为 ViT 开发分布式序列并行性，使它们能够处理多达 1M 个令牌。我们的方法利用 DeepSpeed-Ulysses 和长序列分割与模型分片，是第一个在 ViT 训练中应用序列并行性的方法，在 2,048 个 AMD-MI250X GPU 上实现了 94% 的批量扩展效率。评估 ViT 中的序列并行性，特别是在高达 10B 参数的模型中，突显了重大瓶颈。我们通过混合序列、管道、张量并行和闪存注意力策略来应对这些问题，以超越单个 GPU 内存限制。我们的方法将温度预测的气候建模精度显着提高了 20%，这标志着 Transformer 模型首次在超过 188K 序列长度的全注意力矩阵上进行训练。]]></description>
      <guid>https://arxiv.org/abs/2405.15780</guid>
      <pubDate>Tue, 28 May 2024 06:19:38 GMT</pubDate>
    </item>
    </channel>
</rss>