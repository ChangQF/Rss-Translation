<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 14 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>IndicSTR12：印度场景文本识别数据集</title>
      <link>https://arxiv.org/abs/2403.08007</link>
      <description><![CDATA[arXiv:2403.08007v1 公告类型：新
摘要：在当今日益数字化的世界中，场景文本识别（STR）的重要性怎么强调都不为过。鉴于 STR 的重要性，自动学习特征映射的数据密集型深度学习方法主要推动了 STR 解决方案的开发。拉丁语言可以使用多个基准数据集和深度学习模型的大量工作来满足这一需求。对于 13 亿人使用和阅读的更复杂的语法和语义印度语言，可用的工作和数据集较少。本文旨在通过提出最大、最全面的真实数据集 IndicSTR12 并在 12 种主要印度语言上对 STR 性能进行基准测试，解决印度空间缺乏综合数据集的问题。一些著作已经解决了同样的问题，但据我们所知，它们集中于少数印度语言。所提出的数据集的大小和复杂性与现有的拉丁语同时代数据集相当，而其多语言性将促进稳健的文本检测和识别模型的发展。它是专门为一组具有不同文字的相关语言创建的。该数据集包含从各种自然场景收集的超过 27000 个文字图像，每种语言有超过 1000 个文字图像。与以前的数据集不同，这些图像涵盖了更广泛的现实条件，包括模糊、照明变化、遮挡、非标志性文本、低分辨率、透视文本等。除了新数据集之外，我们还为三个模型提供了高性能基线- PARSeq、CRNN 和 STARNet。]]></description>
      <guid>https://arxiv.org/abs/2403.08007</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>使用可解释的人工智能进行高光谱图像分析的红队模型</title>
      <link>https://arxiv.org/abs/2403.08017</link>
      <description><![CDATA[arXiv:2403.08017v1 公告类型：新
摘要：太空领域的遥感（RS）应用需要可靠、稳健且有质量保证的机器学习（ML）模型，这使得红队成为识别和揭露潜在缺陷和偏见的重要方法。由于这两个领域各自独立发展，因此在将红队策略融入RS方面存在显着差距。本文介绍了一种在 HYPERVIEW 挑战赛中检查在高光谱图像上运行的机器学习模型的方法，重点关注土壤参数的估计。我们使用可解释人工智能 (XAI) 领域的事后解释方法来严格评估赢得 HYPERVIEW 挑战的最佳性能模型，并为 INTUITION-1 高光谱任务上部署的模型提供灵感。我们的方法通过查明和验证关键缺陷，构建一个模型，仅使用 1% 的输入特征即可实现可比较的性能，并且性能损失最多可达 5%，从而有效地对模型进行了红队。此外，我们提出了一种可视化解释的新方法，该方法集成了有关高光谱波段（波长）和数据转换的特定领域信息，以更好地适合高光谱图像分析的解释模型。]]></description>
      <guid>https://arxiv.org/abs/2403.08017</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>WorldGPT：受 Sora 启发的视频 AI 代理，作为来自文本和图像输入的丰富世界模型</title>
      <link>https://arxiv.org/abs/2403.07944</link>
      <description><![CDATA[arXiv:2403.07944v1 公告类型：新
摘要：几种文本到视频的扩散模型在合成高质量视频内容方面表现出了值得称赞的能力。然而，保持时间一致性并确保整个生成序列的动作流畅性仍然是一个艰巨的挑战。在本文中，我们提出了一种创新的视频生成人工智能代理，它利用 Sora 启发的多模式学习的力量，基于文本提示和随附图像构建熟练的世界模型框架。该框架包括两部分：提示增强器和完整视频翻译。第一部分利用ChatGPT的能力，为后续的每一步精心提炼并主动构建精确的提示，从而保证后续模型操作的及时沟通和准确执行的最大准确性。第二部分采用与现有先进扩散技术兼容的方式在视频结束时广泛生成和细化关键帧。然后，我们可以熟练地利用前导和尾随关键帧的力量来制作具有增强的时间一致性和动作平滑度的视频。实验结果证实，与其他方法相比，我们的方法在从文本和图像输入构建世界模型方面具有很强的有效性和新颖性。]]></description>
      <guid>https://arxiv.org/abs/2403.07944</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>AesopAgent：代理驱动的故事到视频制作的进化系统</title>
      <link>https://arxiv.org/abs/2403.07952</link>
      <description><![CDATA[arXiv:2403.07952v1 公告类型：新
摘要：Agent 和 AIGC（人工智能生成内容）技术最近取得了重大进展。我们提出了 AesopAgent，一种代理驱动的故事到视频制作的进化系统。 AesopAgent 是多模式内容生成代理技术的实际应用。该系统将多种生成能力集成在一个统一的框架内，以便个人用户可以轻松地利用这些模块。这个创新系统将用户故事提案转换为脚本、图像和音频，然后将这些多模式内容集成到视频中。此外，动画单元（例如 Gen-2 和 Sora）可以使视频更具感染力。 AesopAgent系统可以编排视频生成的任务工作流程，确保生成的视频内容丰富且连贯。该系统主要包含两层，即水平层和实用层。在水平层中，我们引入了一种新颖的基于 RAG 的进化系统，该系统优化了整个视频生成工作流程以及工作流程中的步骤。它通过积累专家经验和专业知识，不断发展和迭代优化工作流程，包括优化LLM提示和实用程序使用。实用程序层提供多种实用程序，从而生成一致的图像，在构图、字符和风格方面在视觉上保持一致。同时，它提供音频和特效，将它们集成到富有表现力和逻辑排列的视频中。总体而言，与之前的许多视觉叙事作品相比，我们的 AesopAgent 实现了最先进的性能。我们的 AesopAgent 专为个人用户提供便捷服务而设计，可在以下页面获取：https://aesopai.github.io/。]]></description>
      <guid>https://arxiv.org/abs/2403.07952</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>使用点跟踪和分割任何内容在视频中进行实时手术器械分割</title>
      <link>https://arxiv.org/abs/2403.08003</link>
      <description><![CDATA[arXiv:2403.08003v1 公告类型：新
摘要：分段任意模型（SAM）是一个强大的视觉基础模型，正在彻底改变传统的分段范式。尽管如此，对提示每一帧的依赖和大量的计算成本限制了其在机器人辅助手术中的使用。增强现实指导等应用程序只需很少的用户干预以及有效的推理即可在临床上使用。在本研究中，我们通过采用轻量级 SAM 变体来满足速度要求并采用微调技术来增强其在手术场景中的泛化，从而解决了这些限制。跟踪任意点 (TAP) 的最新进展在准确性和效率方面都显示出有希望的结果，特别是当点被遮挡或离开视野时。受这一进展的启发，我们提出了一种新颖的框架，该框架将在线点跟踪器与针对手术器械分割进行微调的轻量级 SAM 模型相结合。跟踪感兴趣区域内的稀疏点并用于在整个视频序列中提示 SAM，从而提供时间一致性。定量结果超越了 EndoVis 2015 数据集上最先进的半监督视频对象分割方法，在单个 GeForce RTX 4060 GPU 上运行的推理速度超过 25 FPS。]]></description>
      <guid>https://arxiv.org/abs/2403.08003</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>ACTrack：为视觉对象跟踪添加时空条件</title>
      <link>https://arxiv.org/abs/2403.07914</link>
      <description><![CDATA[arXiv:2403.07914v1 公告类型：新
摘要：有效地建模对象的时空关系是视觉对象跟踪（VOT）中的一个关键挑战。现有方法通过基于外观的相似性或长期关系建模进行跟踪，导致连续帧之间丰富的时间上下文很容易被忽视。此外，从头开始训练跟踪器或微调大型预训练模型需要更多的时间和内存消耗。在本文中，我们提出了 ACTrack，一种具有附加时空条件的新跟踪框架。它通过冻结参数来保留预训练 Transformer 主干的质量和功能，并制作可训练的轻量级附加网络来模拟跟踪中的时空关系。我们设计了一个附加的暹罗卷积网络来确保空间特征的完整性并执行时间序列建模以简化跟踪管道。多个基准测试的实验结果证明，ACTrack 可以平衡训练效率和跟踪性能。]]></description>
      <guid>https://arxiv.org/abs/2403.07914</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>用于整个幻灯片图像分类的动态策略驱动的自适应多实例学习</title>
      <link>https://arxiv.org/abs/2403.07939</link>
      <description><![CDATA[arXiv:2403.07939v1 公告类型：新
摘要：多实例学习（MIL）在使用袋或伪袋进行组织病理学全幻灯片图像（WSI）分析方面表现出了令人印象深刻的性能。它涉及实例采样、特征表示和决策。然而，现有的基于MIL的技术至少存在以下一个或多个问题：1）需要大量的存储和密集的预处理（采样）； 2）用有限的知识来预测袋子标签（特征表示）可能会出现过度拟合； 3）伪袋计数和先验偏差影响模型的稳健性和泛化性（决策）。受临床诊断的启发，使用过去的采样实例可以促进最终的 WSI 分析，但在现有技术中几乎没有探索过。为了打破这些限制，我们将动态实例采样和强化学习集成到一个统一的框架中，以改进实例选择和特征聚合，形成一种新颖的动态策略实例选择（DPIS）方案，以实现更好、更可信的决策。具体来说，采用特征距离和奖励函数的测量来促进连续实例采样。为了缓解过度拟合，我们探索实例之间的潜在全局关系，以获得更稳健和有区别的特征表示，同时建立奖励和惩罚机制，以使用对比学习来纠正伪袋中的偏差。这些策略形成了用于 WSI 任务的最终动态策略驱动自适应多实例学习 (PAMIL) 方法。大量实验表明，我们的 PAMIL 方法在 CAMELYON16 上比最先进的方法高出 3.8%，在 TCGA 肺癌数据集上比最先进的方法高出 4.4%。]]></description>
      <guid>https://arxiv.org/abs/2403.07939</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>基于压缩算法的数字视频篡改检测技术</title>
      <link>https://arxiv.org/abs/2403.07891</link>
      <description><![CDATA[arXiv:2403.07891v1 公告类型：新
摘要：数字图像和视频在日常生活中发挥着非常重要的作用。如今，人们可以使用价格实惠的移动设备，这些设备配备了先进的集成摄像头和强大的图像处理应用程序。技术发展不仅促进了多媒体内容的生成，而且还促进了出于娱乐或恶意目的对其进行故意修改。这就是检测图像和视频操纵的取证技术变得至关重要的地方。本文通过分析 H.264 编码所使用的压缩算法，提出了一种取证技术。再压缩的存在使用宏块信息、H.264-MPEG4标准的特征和运动向量。矢量支持机用于创建模型，可以准确检测视频是否已被重新压缩。]]></description>
      <guid>https://arxiv.org/abs/2403.07891</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>MIP：基于 PEFT 梯度的基于 CLIP 的图像重建</title>
      <link>https://arxiv.org/abs/2403.07901</link>
      <description><![CDATA[arXiv:2403.07901v1 公告类型：新
摘要：对比语言-图像预训练（CLIP）模型作为一种有效的预训练多模态神经网络，已广泛应用于分布式机器学习任务，特别是联邦学习（FL）中。通常，基于 CLIP 的 FL 采用参数高效微调（PEFT）进行模型训练，仅微调适配器参数或软提示，而不是完整参数。尽管PEFT与传统的训练模式不同，但在本文中，我们从理论上分析了适配器或软提示的梯度仍然可以用于执行图像重建攻击。基于我们的理论分析，我们提出了 Multm-In-Parvo (MIP)，这是一种针对基于 CLIP 的分布式机器学习架构的专有重建攻击方法。具体来说，MIP可以根据软提示或适配器的梯度重建CLIP训练图像。此外，MIP 还包括加速收敛的标签预测策略和避免文本编码器上梯度消失问题的逆梯度估计机制。实验结果表明，MIP可以根据软提示或CLIP模型适配器的梯度有效地重建训练图像。]]></description>
      <guid>https://arxiv.org/abs/2403.07901</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>HandGCAT：从单目图像重建遮挡稳健的 3D 手部网格</title>
      <link>https://arxiv.org/abs/2403.07912</link>
      <description><![CDATA[arXiv:2403.07912v1 公告类型：新
摘要：我们提出了一种稳健且准确的方法，用于从单目图像重建 3D 手部网格。这是一个非常具有挑战性的问题，因为手经常被物体严重遮挡。以前的工作经常忽略 2D 手部姿势信息，其中包含与遮挡区域密切相关的手部先验知识。因此，在这项工作中，我们提出了一种新颖的 3D 手部网格重建网络 HandGCAT，它可以充分利用手部先验作为补偿信息来增强遮挡区域特征。具体来说，我们设计了知识引导图卷积（KGC）模块和交叉注意力变换器（CAT）模块。 KGC 通过图卷积从 2D 手部姿势中提取手部先验信息。考虑到遮挡区域的高相关性，CAT 将手优先融合到遮挡区域中。对具有挑战性的手部物体遮挡的流行数据集（例如 HO3D v2、HO3D v3 和 DexYCB）进行的大量实验表明，我们的 HandGCAT 达到了最先进的性能。该代码可在 https://github.com/heartStrive/HandGCAT 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.07912</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>MOD-CL：具有约束损失的多标签目标检测</title>
      <link>https://arxiv.org/abs/2403.07885</link>
      <description><![CDATA[arXiv:2403.07885v1 公告类型：新
摘要：我们介绍了 MOD-CL，这是一种多标签目标检测框架，它利用训练过程中的约束损失来产生更好地满足给定要求的输出。在本文中，我们使用 $\mathrm{MOD_{YOLO}}$，这是一种基于近年来发布的最先进的对象检测模型 YOLOv8 构建的多标签对象检测模型。在任务 1 中，我们引入了 Corrector Model 和 Blender Model，这是对象检测过程之后的两个新模型，旨在生成更受约束的输出。对于任务 2，约束损失已使用 Product T-Norm 合并到 $\mathrm{MOD_{YOLO}}$ 架构中。结果表明，这些实现有助于提高任务 1 和任务 2 的分数。]]></description>
      <guid>https://arxiv.org/abs/2403.07885</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>神经槽解释器：在紧急槽表示中奠定对象语义基础</title>
      <link>https://arxiv.org/abs/2403.07887</link>
      <description><![CDATA[arXiv:2403.07887v1 公告类型：新
摘要：以对象为中心的方法在将原始感知无监督分解为丰富的类对象抽象方面取得了重大进展。然而，将现实世界的对象语义融入学习抽象的能力有限，阻碍了它们在下游理解应用中的采用。我们提出了神经槽解释器（NSI），它通过槽表示来学习基础和生成对象语义。 NSI 的核心是一种类似 XML 的编程语言，它使用简单的语法规则将场景的对象语义组织成以对象为中心的程序原语。然后，对齐模型通过共享嵌入空间上的双层对比学习目标学习将程序原语放入槽中。最后，我们制定了 NSI 程序生成器模型，以使用从对齐模型推断出的密集关联从槽生成以对象为中心的程序。双模态检索任务的实验证明了学习对齐的有效性，大大超过了基于集合匹配的预测器。此外，从基础关联中学习程序生成器可以增强老虎机的预测能力。 NSI 生成的程序证明了以对象为中心的学习器在属性预测和对象检测方面的性能得到了提高，并且可以根据现实世界场景的复杂性进行扩展。]]></description>
      <guid>https://arxiv.org/abs/2403.07887</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>跨模态去偏差：使用语言减轻成像中的亚群变化</title>
      <link>https://arxiv.org/abs/2403.07888</link>
      <description><![CDATA[arXiv:2403.07888v1 公告类型：新
摘要：子群体转移是一种特定类型的领域转移，它突出了训练和测试之间特定子组或群体内数据分布的变化。子群体的变化是算法偏差的一个重要来源，需要分布的稳健性。最近的研究发现多模态基础模型（例如视觉语言模型 CLIP）具有固有的分布稳健性，但这种稳健性很容易通过参数微调而受到影响。在本文中，我们建议利用不同模态之间稳健性的联系，并重塑一种模态与另一种模态的分布稳健性。具体来说，在 CLIP 的分布稳健性的背景下，我们建议利用自然语言输入来消除图像特征表示的偏差，以提高子群体的最坏情况性能。我们广泛的实证研究表明，通过自然语言消除偏差的图像表示可以在子群体变化下实现显着的性能改进并减少性能不稳定性。]]></description>
      <guid>https://arxiv.org/abs/2403.07888</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>通过文本相关图像块选择进行高效的视觉和语言预训练</title>
      <link>https://arxiv.org/abs/2403.07883</link>
      <description><![CDATA[arXiv:2403.07883v1 公告类型：新
摘要：视觉变换器（ViT）在大规模视觉和语言预训练（VLP）模型中变得越来越流行。尽管之前的 VLP 研究已经证明了 ViT 的功效，但这些努力仍然面临着冗长的视觉序列导致的计算效率低下的问题。为了应对这一挑战，我们引入了一种名为 TRIPS 的高效 VLP 方法，它代表文本相关图像块选择。 TRIPS 在视觉主干中使用文本引导的补丁选择层逐步减少视觉序列，从而加速训练和推理过程。该补丁选择层动态计算依赖于文本的视觉注意力，使其能够通过文本引导识别注意力集中的图像标记，并以端到端的方式融合注意力不集中的图像标记。重要的是，TRIPS 没有添加任何额外的参数，并且可以推广到大多数基于 ViT 的 VLP 模型。我们将 TRIPS 纳入涵盖单流、双流和生成范式的三个代表性 VLP 模型中，并在五个广泛使用的多模态基准数据集上进行了广泛的实验。我们的实验结果表明，TRIPS 可实现 40% 的加速，同时在下游任务上保持有竞争力或卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.07883</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>Seg-metrics：用于计算分段指标的 Python 包</title>
      <link>https://arxiv.org/abs/2403.07884</link>
      <description><![CDATA[arXiv:2403.07884v1 公告类型：新
摘要：为了应对医学图像分割（MIS）研究中选择性强调指标的趋势，我们引入了 \texttt{seg-metrics}，这是一个用于标准化 MIS 模型评估的开源 Python 包。与现有的软件包不同，\texttt{seg-metrics} 为各种基于重叠和基于距离的指标提供了用户友好的界面，从而提供了全面的解决方案。 \texttt{seg-metrics} 支持多种文件格式，并且可以通过 Python 包索引 (PyPI) 轻松安装。 \texttt{seg-metrics} 注重速度和便利性，是高效 MIS 模型评估的宝贵工具。]]></description>
      <guid>https://arxiv.org/abs/2403.07884</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:53 GMT</pubDate>
    </item>
    </channel>
</rss>