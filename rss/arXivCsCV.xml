<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 24 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有子组分布对齐调整的公平文本到医学图像扩散模型</title>
      <link>https://arxiv.org/abs/2406.14847</link>
      <description><![CDATA[arXiv:2406.14847v1 公告类型：新
摘要：具有潜在扩散模型的文本到医学图像 (T2MedI) 具有巨大潜力，可以缓解医学影像数据的稀缺性并探索特定患者状态描述中病变的潜在外观分布。然而，作为文本到自然图像模型，我们表明 T2MedI 模型也会偏向某些子组而忽略训练集中的少数子组。在这项工作中，我们首先基于预训练的 Imagen 模型构建一个 T2MedI 模型，该模型具有固定的对比语言图像预训练 (CLIP) 文本编码器，而其解码器已在来自放射学对象上下文 (ROCO) 数据集的医学图像上进行了微调。对其性别偏见进行了定性和定量分析。针对这个问题，我们建议对 T2MedI 进行微调以使其敏感子组分布概率对齐。具体来说，微调的对齐损失由现成的敏感度子组分类器引导，以匹配生成的图像和预期目标数据集之间的分类概率。此外，图像质量由遵循知识蒸馏方案的 CLIP 一致性正则化项保持。为了进行评估，我们将要增强的目标数据集设置为 BraST18 数据集，并从中训练基于脑磁共振 (MR) 切片的性别分类器。使用我们的方法，生成的 MR 图像可以显著减少与 BraTS18 数据集中性别比例的不一致。]]></description>
      <guid>https://arxiv.org/abs/2406.14847</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>SAM-EG：具有 Egde Guidance 框架的“任何分割模型”，可实现高效的息肉分割</title>
      <link>https://arxiv.org/abs/2406.14819</link>
      <description><![CDATA[arXiv:2406.14819v1 公告类型：新
摘要：息肉分割是医学成像中的一个关键问题，它促使人们提出了许多旨在提高分割掩模质量的方法。虽然目前最先进的技术产生了令人印象深刻的结果，但这些模型的大小和计算成本对实际的行业应用构成了挑战。最近，Segment Anything 模型 (SAM) 已被提出作为一种强大的基础模型，有望适应医学图像分割。受此概念的启发，我们提出了 SAM-EG，这是一个指导息肉分割的小型分割模型解决计算成本挑战的框架。此外，在本研究中，我们引入了边缘引导模块，该模块将边缘信息集成到图像特征中，以帮助分割模型解决此任务中当前分割模型的边界问题。通过大量实验，我们的小模型展示了它们的有效性，与最先进的方法取得了有竞争力的结果，为开发用于息肉分割和更广泛的医学成像领域的高精度紧凑模型提供了一种有前途的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.14819</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-Decoder：使用多模态 CLIP 对齐表示的 ZeroShot 多标签分类</title>
      <link>https://arxiv.org/abs/2406.14830</link>
      <description><![CDATA[arXiv:2406.14830v1 公告类型：新
摘要：多标签分类是各种实际应用中必不可少的任务。多标签零样本学习是一种将图像分类为多个未见类别的方法，这些类别没有可用的训练数据，而在一般的零样本情况下，测试集可能包括观察到的类别。CLIP-Decoder 是一种基于最先进的 ML-Decoder 注意力机制的新方法。我们在 CLIP-Decoder 中引入了多模态表示学习，利用文本编码器提取文本特征，利用图像编码器提取图像特征。此外，我们通过在同一维度上对齐图像和单词嵌入并使用组合损失（包括分类损失和 CLIP 损失）比较它们各自的表示来最小化语义不匹配。该策略优于其他方法，我们使用 CLIP-Decoder 在零样本多标签分类任务上取得了前沿成果。与现有的零样本学习多标签分类任务方法相比，我们的方法在性能上实现了 3.9% 的绝对提升。此外，在广义零样本学习多标签分类任务中，我们的方法也实现了近 2.3% 的惊人提升。]]></description>
      <guid>https://arxiv.org/abs/2406.14830</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>通过在神经辐射场中插入物体来重新照亮场景</title>
      <link>https://arxiv.org/abs/2406.14806</link>
      <description><![CDATA[arXiv:2406.14806v1 公告类型：新
摘要：将对象插入场景并重新照明是增强现实 (AR) 中常用的应用。以前的方法侧重于使用 CAD 模型或单视图图像中的真实对象插入虚拟对象，导致 AR 应用场景受到极大限制。我们提出了一种基于 NeRF 的新型管道，用于将对象 NeRF 插入场景 NeRF，实现新颖的视图合成和逼真的重新照明，支持从描绘对象和场景的两组图像中投射阴影等物理交互。照明环境是球面谐波和球面高斯的混合表示，可以很好地表示高频和低频照明成分，并支持非朗伯表面。具体而言，我们利用体积渲染的优势，并通过比较相机视图和光源视图之间的深度图并生成生动的软阴影，引入了一种高效阴影渲染的创新方法。所提出的方法在广泛的实验评估中实现了逼真的重新照明效果。]]></description>
      <guid>https://arxiv.org/abs/2406.14806</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>基于相的地质模型参数化和数据同化的潜在扩散模型</title>
      <link>https://arxiv.org/abs/2406.14815</link>
      <description><![CDATA[arXiv:2406.14815v1 公告类型：新
摘要：地质参数化需要使用一小组潜在变量来表示地质模型，并将这些变量映射到网格块属性（例如孔隙度和渗透率）。参数化对于数据同化（历史匹配）很有用，因为它可以保持地质真实性，同时减少要确定的变量数量。扩散模型是一类新的生成深度学习程序，已被证明在图像生成任务中优于以前的方法，例如生成对抗网络。扩散模型经过训练可以“去噪”，这使它们能够从以随机噪声为特征的输入场中生成新的地质实现。潜在扩散模型是本研究中考虑的特定变体，它通过使用低维潜在变量来降低维度。本研究开发的模型包括用于降维的变分自动编码器和用于去噪过程的 U-net。我们的应用涉及条件​​二维三相（水道-堤坝-泥浆）系统。结果表明，潜在扩散模型提供的实现与地质建模软件的样本在视觉上一致。评估了涉及空间和流量响应统计的定量指标，并观察到扩散生成的模型和参考实现之间的总体一致性。执行稳定性测试以评估参数化方法的平滑度。然后使用潜在扩散模型进行基于集合的数据同化。考虑了两个合成的“真实”模型。在这两种情况下，都实现了显著的不确定性减少、后验 P$_{10}$-P$_{90}$ 预测（通常包含观测数据）和一致的后验地质模型。]]></description>
      <guid>https://arxiv.org/abs/2406.14815</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>正则分布匹配蒸馏用于一步非配对图像到图像转换</title>
      <link>https://arxiv.org/abs/2406.14762</link>
      <description><![CDATA[arXiv:2406.14762v1 公告类型：新
摘要：扩散蒸馏方法旨在将扩散模型压缩为高效的一步生成器，同时尽量保持质量。其中，分布匹配蒸馏 (DMD) 提供了一个适合训练通用形式的一步生成器的框架，适用于无条件生成之外。在这项工作中，我们介绍了它的修改，称为正则化分布匹配蒸馏，适用于非配对图像到图像 (I2I) 问题。我们展示了它在多个翻译任务中的实证性能，包括 2D 示例和不同图像数据集之间的 I2I，其中它的表现与多步扩散基线相当或更好。]]></description>
      <guid>https://arxiv.org/abs/2406.14762</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>用于单摄像头训练行人重新识别的摄像头不变元学习网络</title>
      <link>https://arxiv.org/abs/2406.14797</link>
      <description><![CDATA[arXiv:2406.14797v1 公告类型：新 
摘要：单摄像头训练行人重新识别 (SCT re-ID) 旨在使用 SCT 数据集训练重新识别模型，其中每个人只出现在一个摄像头中。SCT re-ID 的主要挑战是在没有跨摄像头同一人 (CCSP) 数据作为监督的情况下学习相机不变的特征表示。以前的方法通过假设最相似的人应该在另一个相机中找到来解决这个问题。然而，这种假设并不能保证是正确的。在本文中，我们提出了一种用于 SCT re-ID 的相机不变元学习网络 (CIMN)。CIMN 假设相机不变特征表示应该对相机变化具有鲁棒性。为此，我们根据摄像机 ID 将训练数据分为元训练集和元测试集，并通过元学习策略进行跨摄像头模拟，旨在强制从元训练集中学习到的表示对元测试集具有鲁棒性。通过跨相机模拟，即使没有 CCSP 数据，CIMN 也可以学习相机不变和身份判别的表示。然而，这种模拟也会导致元训练集和元测试集分离，从而忽略它们之间的一些有益关系。因此，我们引入了三种损失：元三元组损失、元分类损失和元相机对齐损失，以利用被忽略的关系。实验结果表明，我们的方法在有和没有 CCSP 数据的情况下都实现了相当的性能，并且在 SCT re-ID 基准上优于最先进的方法。此外，它还能有效提高模型的领域泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2406.14797</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>交错文本和图像生成的整体评估</title>
      <link>https://arxiv.org/abs/2406.14643</link>
      <description><![CDATA[arXiv:2406.14643v1 公告类型：新
摘要：交错文本和图像生成一直是一个有趣的研究方向，其中模型需要以任意顺序生成图像和文本片段。尽管交错生成取得了一些新进展，但其评估进展仍然明显落后。现有的评估基准不支持输入和输出的任意交错图像和文本，并且它们仅涵盖有限数量的领域和用例。此外，当前的工作主要使用基于相似性的指标，这在评估开放式场景的质量方面存在不足。为此，我们推出了 InterleavedBench，这是第一个精心策划的用于评估交错文本和图像生成的基准。InterleavedBench 具有丰富的任务阵列，可涵盖各种实际用例。此外，我们提出了 InterleavedEval，这是一种由 GPT-4o 提供支持的强大的无参考指标，可提供准确且可解释的评估。我们仔细定义了 InterleavedEval 的五个基本评估方面，包括文本质量、感知质量、图像连贯性、文本-图像连贯性和有用性，以确保全面而细致的评估。通过大量实验和严格的人工评估，我们表明我们的基准和指标可以有效地评估现有模型，并且与人类判断具有很强的相关性，超越了以前基于参考的指标。我们还提供了实质性的发现和见解，以促进未来对交错生成及其评估的研究。]]></description>
      <guid>https://arxiv.org/abs/2406.14643</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>这看起来比那更好：使用 ProtoPNeXt 构建更好的可解释模型</title>
      <link>https://arxiv.org/abs/2406.14675</link>
      <description><![CDATA[arXiv:2406.14675v1 公告类型：新
摘要：原型部分模型是计算机视觉黑盒深度学习模型的一种流行的可解释替代方案。然而，它们很难训练，对超参数调整高度敏感，阻碍了它们在新数据集中的应用，也阻碍了我们对哪些方法真正提高了它们的性能的理解。为了便于对原型部分网络 (ProtoPNets) 进行仔细研究，我们创建了一个用于集成原型部分模型组件的新框架——ProtoPNeXt。使用 ProtoPNeXt，我们表明，将贝叶斯超参数调整和角度原型相似性度量应用于原始 ProtoPNet 足以在多个主干上的 CUB-200 上为原型部分模型产生新的最先进的准确性。我们进一步部署该框架以联合优化准确性和原型可解释性，以 ProtoPNeXt 中包含的指标来衡量。使用相同的资源，这将产生具有明显优越语义的模型，准确率变化在 +1.3% 和 -1.5% 之间。代码和经过训练的模型将在发布后公开。]]></description>
      <guid>https://arxiv.org/abs/2406.14675</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>ICAL：通过将轨迹转化为可操作的见解，实现多模式代理的持续学习</title>
      <link>https://arxiv.org/abs/2406.14596</link>
      <description><![CDATA[arXiv:2406.14596v1 公告类型：新
摘要：大规模生成语言和视觉语言模型 (LLM 和 VLM) 在决策和指令遵循的少样本上下文学习方面表现出色。但是，它们需要在其上下文窗口中包含高质量的示例演示。在这项工作中，我们提出以下问题：LLM 和 VLM 能否从通用的次优演示中生成自己的提示示例？我们提出了上下文抽象学习 (ICAL)，这是一种从次优演示和人工反馈中构建多模态经验洞察记忆的方法。给定一个新领域中的嘈杂演示，VLM 通过修复低效操作和注释认知抽象将轨迹抽象为一般程序：任务关系、对象状态变化、时间子目标和任务构造。当代理尝试在类似环境中执行轨迹时，这些抽象通过人工反馈以交互方式进行改进和调整。当将生成的抽象用作提示中的范例时，可以显著改善检索增强型 LLM 和 VLM 代理的决策能力。我们的 ICAL 代理在 TEACh 中的对话式教学跟踪、VisualWebArena 中的多模式网络代理和 Ego4D 中的动作预期方面都超越了最先进的技术。在 TEACh 中，我们的目标条件成功率提高了 12.6%。在 VisualWebArena 中，我们的任务成功率从 SOTA 的 14.3% 提高到 22.7%。在 Ego4D 动作预测中，我们比少样本 GPT-4V 有所改进，并且与监督模型保持竞争力。我们展示了对检索增强型上下文代理进行微调可以带来额外的改进。我们的方法大大减少了对专家制作的示例的依赖，并且始终优于缺乏此类见解的行动计划中的上下文学习。]]></description>
      <guid>https://arxiv.org/abs/2406.14596</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>Stylebreeder：通过文本到图像模型探索和普及艺术风格</title>
      <link>https://arxiv.org/abs/2406.14599</link>
      <description><![CDATA[arXiv:2406.14599v1 公告类型：新
摘要：文本到图像模型越来越受欢迎，通过实现高度详细和富有创意的视觉内容生成，彻底改变了数字艺术创作的格局。这些模型已广泛应用于各个领域，特别是在艺术创作领域，它们促进了广泛的创意表达并使艺术创作的获取变得民主化。在本文中，我们介绍了 \texttt{STYLEBREEDER}，这是一个全面的数据集，包含 680 万张图像和 180 万条提示，由 Artbreeder 上的 95K 名用户生成，该平台已成为拥有超过 1300 万用户的创意探索重要中心。我们利用该数据集引入了一系列任务，旨在识别不同的艺术风格、生成个性化内容并根据用户兴趣推荐风格。通过记录超越“赛博朋克”或“毕加索”等传统类别的独特用户生成风格，我们探索了独特的众包风格的潜力，这些风格可以深入了解全球用户的集体创作心理。我们还评估了不同的个性化方法来增强艺术表达并引入了风格图集，使这些模型以 LoRA 格式可供公众使用。我们的研究证明了文本到图像传播模型的潜力，可以发现和推广独特的艺术表达，进一步使艺术领域的人工智能民主化，并培育一个更加多元化和包容性的艺术社区。数据集、代码和模型可在 https://stylebreeder.github.io 上根据公共领域 (CC0) 许可证获得。]]></description>
      <guid>https://arxiv.org/abs/2406.14599</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>使用通道改组的深度学习实现更快的金属表面缺陷检测</title>
      <link>https://arxiv.org/abs/2406.14582</link>
      <description><![CDATA[arXiv:2406.14582v1 公告类型：新
摘要：近年来，深度学习不断进步，大量研究人员致力于缺陷检测算法的研究。小而复杂的目标的检测和识别仍然是一个需要解决的问题。这项研究的作者想提出一种改进的缺陷检测模型，用于检测钢表面的小而复杂的缺陷目标。在钢带生产过程中，机械力和环境因素会导致钢带表面出现缺陷。因此，检测此类缺陷是生产高质量产品的关键。此外，钢带表面缺陷给高科技产业造成了巨大的经济损失。到目前为止，很少有研究探索识别缺陷的方法，而且目前可用的大多数算法都不够有效。因此，本研究提出了一种改进的基于 You Only Look Once (YOLOv5) 的实时金属表面缺陷检测模型，专为小型网络设计。对于目标的较小特征，常规部分被深度卷积和通道混洗机制取代。然后为特征金字塔网络（FPN）输出特征分配权重并融合它们，以增加特征传播和网络表征能力。实验结果表明，改进的模型在准确率和检测时间方面优于其他同类模型。@mAP 在东北大学数据集 NEU-DET 上实现的模型精度为 77.5%，在 GC10-DET 数据集上实现的精度为 70.18%]]></description>
      <guid>https://arxiv.org/abs/2406.14582</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络在钢铁表面缺陷分类中的建模与性能评估</title>
      <link>https://arxiv.org/abs/2406.14583</link>
      <description><![CDATA[arXiv:2406.14583v1 公告类型：新
摘要：最近，卷积神经网络 (CNN) 在图像分类任务中取得了出色的识别率。要使用此类技能，需要对使用 RGB 相机捕获的金属表面缺陷的著名图像数据集进行选择性 CNN 训练。出于生产方面的考虑，必须尽早发现缺陷，以便及时采取纠正措施。到目前为止，对于图像分类，已经采用了一种基于模型的方法，该方法表明了与无瑕疵表面相比，表面缺陷的预测反射特性。由于钢铁在汽车、家庭、建筑等终端产品领域的广泛应用，检测钢铁表面缺陷的问题变得越来越重要。检测的手动过程耗时、耗力且成本高昂。已经使用了不同的策略来自动化手动过程，但事实证明，CNN 模型是最有效的，而不是图像处理和机器学习技术。通过使用经过微调的不同 CNN 模型，可以轻松比较它们的性能并为相同类型的任务选择性能最佳的模型。然而，重要的是，使用不同的 CNN 模型或进行微调在计算上都很昂贵且耗时。因此，我们的研究有助于未来的研究人员在选择 CNN 时不考虑模型复杂性、性能和计算资源的问题。在本文中，评估了使用迁移学习技术的各种 CNN 模型的性能。这些模型的选择基于它们在计算机视觉研究领域的受欢迎程度和影响力，以及它们在基准数据集上的性能。根据结果，DenseNet201 优于其他 CNN 模型，并且在 NEU 数据集上的检测率最高，达到 98.37%。]]></description>
      <guid>https://arxiv.org/abs/2406.14583</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>LM-IGTD：用于低维和混合类型表格数据的二维图像生成器，以发挥卷积神经网络的潜力</title>
      <link>https://arxiv.org/abs/2406.14566</link>
      <description><![CDATA[arXiv:2406.14566v1 公告类型：新
摘要：表格数据已广泛应用于不同的知识领域。卷积神经网络 (CNN) 已成功用于许多应用中，其中有关数据的重要信息嵌入在特征（图像）的顺序中，其预测结果优于传统模型。最近，一些研究人员提出将表格数据转换为图像，以利用 CNN 的潜力并在分类和回归等预测任务中获得高结果。在本文中，我们提出了一种将表格数据转换为图像的新颖有效的方法，解决了与低维和混合类型数据集相关的固有限制。我们的方法称为表格数据的低混合图像生成器 (LM-IGTD)，它集成了随机特征生成过程和 IGTD 的修改版本。我们引入了一种自动且可解释的端到端管道，可以从表格数据创建图像。建立原始特征与生成图像之间的映射，并采用事后可解释性方法来识别这些图像的关键区域，从而增强预测任务的可解释性。对 12 个低维和混合类型数据集（包括二分类和多分类场景）上提出的表格到图像生成方法进行了广泛的评估。特别是，在使用 LM-IGTD 和 CNN 生成的图像时，我们的方法在十二个数据集中的五个数据集中优于所有在表格数据上训练的传统 ML 模型。在其余数据集中，LM-IGTD 图像和 CNN 始终超越了四个传统 ML 模型中的三个，取得了与第四个模型相似的结果。]]></description>
      <guid>https://arxiv.org/abs/2406.14566</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习对 RGB-D 室内数据进行 3D 实例分割</title>
      <link>https://arxiv.org/abs/2406.14581</link>
      <description><![CDATA[arXiv:2406.14581v1 公告类型：新
摘要：3D 物体识别是工业和家庭室内环境中智能和机器人系统面临的一项挑战性任务。对于此类系统来说，识别和分割它们经常遇到的 3D 物体实例至关重要。计算机视觉、图形和机器学习领域都对此给予了极大的关注。传统上，3D 分割是使用手工制作的特征和设计方法完成的，这些方法没有达到可接受的性能，也无法推广到大规模数据。深度学习方法最近因其在 2D 计算机视觉中的巨大成功而成为 3D 分割挑战的首选方法。然而，实例分割任务目前探索较少。在本文中，我们提出了一种基于深度学习使用红绿蓝和深度 (RGB-D) 数据进行高效 3D 实例分割的新方法。基于 2D 区域的卷积神经网络 (Mask R-CNN) 深度学习模型与基于点的渲染模块相结合，可与深度信息相结合，以识别和分割物体的 3D 实例。为了生成 3D 点云坐标 (x, y, z)，RGB 图像中识别的物体区域的分割 2D 像素 (u, v) 被合并到深度图像的 (u, v) 点中。此外，我们进行了实验和分析，从不同角度和距离比较了我们提出的方法。实验表明，所提出的 3D 物体识别和实例分割足以支持机器人和智能系统中的物体处理。]]></description>
      <guid>https://arxiv.org/abs/2406.14581</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    </channel>
</rss>