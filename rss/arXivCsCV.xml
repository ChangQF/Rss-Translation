<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Fri, 22 Dec 2023 03:14:28 GMT</lastBuildDate>
    <item>
      <title>面向主题的视频字幕。 （arXiv：2312.13330v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13330</link>
      <description><![CDATA[根据用户的需求描述视频内容是一个长期目标。
尽管现有的视频字幕方法已经取得了显着的进步，但
生成的标题可能不会集中在用户特别关注的实体上
为了解决这个问题，我们提出了一个新的视频字幕任务，
面向主题的视频字幕，允许用户指定描述内容
通过边界框确定目标。为了支持这项任务，我们构建了两个
基于两个广泛使用的视频的面向主题的视频字幕数据集
字幕数据集：MSVD 和 MSRVTT，通过注释每个视频中的主题
每个标题。这些数据集为未来的技术开发铺平了道路。作为
第一次尝试，我们评估了四种最先进的通用视频字幕
模型，并观察到性能大幅下降。然后我们探索几个
使他们能够描述所需目标的策略。实验结果
进步明显，但仍有较大提升空间
该领域的探索。
]]></description>
      <guid>http://arxiv.org/abs/2312.13330</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>ShowRoom3D：使用 3D 先验生成文本到高质量 3D 房间。 （arXiv：2312.13324v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13324</link>
      <description><![CDATA[我们介绍 ShowRoom3D，这是一种生成高质量的三阶段方法
来自文本的 3D 房间规模场景。先前使用 2D 扩散先验的方法
优化神经辐射场以生成房间规模的场景已经表明
质量不满意。这主要是由于2D的局限性
先验缺乏 3D 意识和训练方法的限制。在
在本文中，我们利用 3D 扩散先验 MVDiffusion 来优化 3D
房间规模的场景。我们的贡献有两个方面。首先，我们提出一个
渐进式视图选择过程以优化 NeRF。这涉及到划分
训练过程分为三个阶段，逐步扩大相机采样
范围。其次，我们提出了第二阶段的姿态变换方法。
它将确保 MVDiffusion 提供准确的视图引导。因此，
ShowRoom3D 能够生成结构完整性得到改善的房间，
增强任何视图的清晰度，减少内容重复，并提高
不同观点的一致性。大量实验证明
我们的方法明显优于最先进的方法
在用户研究方面有很大的优势。
]]></description>
      <guid>http://arxiv.org/abs/2312.13324</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>NeLF-Pro：神经光场探头。 （arXiv：2312.13328v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13328</link>
      <description><![CDATA[我们推出 NeLF-Pro，一种用于建模和重建的新颖表示形式
不同自然场景中光场的范围和空间各不相同
粒度。与之前的快速重建方法相比
在全局 3D 场景中，我们将场景的光场建模为一组局部光场
光场特征探头，通过位置和多通道 2D 进行参数化
特征图。我们的中心思想是将场景的光场烘焙成
空间变化的可学习表示并通过以下方式查询点特征
靠近相机的探针的加权混合 - 允许 mipmap
表示和渲染。我们引入一种新颖的向量-矩阵-矩阵（VMM）
有效表示光场特征的分解技术
探针作为局部特征共享的核心因素（即VM）的产物
探针和基本因子（即 M） - 有效编码内部
场景中的关系和模式。通过实验，我们证明了
NeLF-Pro显着提升基于特征网格的性能
表示，并以更好的渲染质量实现快速重建
同时保持紧凑的造型。
]]></description>
      <guid>http://arxiv.org/abs/2312.13328</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>深度混合相机去模糊。 （arXiv：2312.13317v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13317</link>
      <description><![CDATA[移动相机尽管取得了显着的进步，但仍然面临低光照的问题
紧凑型传感器和镜头带来的挑战，导致更长的曝光时间和
运动模糊。传统的解决方案，如盲反卷积和基于学习的
方法通常不足以处理去模糊问题的不适定性。
为了解决这个问题，我们提出了一种新颖的多摄像头去模糊框架
智能手机，利用混合成像技术。我们同时捕获一个
来自智能手机的长曝光广角图像和超广角连拍图像，
并使用锐利突发来估计模糊内核以对广角进行去模糊
图像。为了学习和评估我们的网络，我们引入了 HCBlur
数据集，其中包括一对模糊的广角和锐利的超广角连拍
图像及其锐利的广角图像。我们广泛评估我们的
方法，结果显示了最先进的质量。
]]></description>
      <guid>http://arxiv.org/abs/2312.13317</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>In2SET：用于双摄像头压缩高光谱成像的帧内相似性利用变压器。 (arXiv:2312.13319v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.13319</link>
      <description><![CDATA[双摄像头压缩高光谱成像 (DCCHI) 能够提供
通过融合压缩和全色重建 3D 高光谱图像 (HSI)
（PAN）图像，在快照高光谱成像方面显示出巨大的潜力
在实践中。在本文中，我们介绍了一种新颖的 DCCHI 重建网络，
帧内-帧间相似度利用变换器（In2SET）。我们的主要见解是
充分利用PAN图像辅助重建。为此，我们
建议使用 PAN 图像内的内部相似性作为代理
近似原始 HSI 中的内部相似性，从而提供
增强内容先验以实现更准确的 HSI 重建。此外，我们
旨在将基础 HSI 的特征与 PAN 图像的特征对齐，
保持语义一致性并引入新的上下文信息
重建过程。通过将 In2SET 集成到 PAN 引导的展开中
框架，我们的方法大大提高了空间光谱保真度和
重建图像的细节，提供更全面和准确的
场景的描述。在真实和真实环境中进行了大量的实验
模拟数据集表明我们的方法始终优于
重建质量和现有最先进的方法
计算复杂度。代码将被发布。
]]></description>
      <guid>http://arxiv.org/abs/2312.13319</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>解锁用于语义图像合成的预训练图像主干。 （arXiv：2312.13314v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13314</link>
      <description><![CDATA[语义图像合成，即根据用户提供的语义生成图像
标签图是一项重要的条件图像生成任务，因为它允许
控制生成图像的内容和空间布局。
尽管扩散模型已经推动了生成图像的最先进水平
建模，其推理过程的迭代性质使他们
计算要求高。其他方法（例如 GAN）效率更高，因为
他们只需要一次前馈传递即可生成，但图像质量
往往会在大型且多样化的数据集上受到影响。在这项工作中，我们提出了一种新的
用于语义图像合成的 GAN 判别器类别，可生成高度
通过利用针对任务预先训练的特征骨干网络来获得逼真的图像
比如图像分类。我们还引入了新的生成器架构
通过更好的上下文建模并使用交叉注意力将噪声注入
潜在变量，导致生成的图像更加多样化。我们的模型，我们
dub DP-SIMS 在图像质量和
与 ADE-20K、COCO-Stuff 和 Cityscapes 上的输入标签图保持一致，
超越最近的扩散模型，同时需要减少两个数量级
计算进行推理。
]]></description>
      <guid>http://arxiv.org/abs/2312.13314</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>ECAMP：以实体为中心的上下文感知医学视觉语言预训练。 （arXiv：2312.13316v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13316</link>
      <description><![CDATA[尽管医学视觉语言预训练取得了重大进展，
现有的方法在很大程度上忽略了固有的实体特定背景
在放射学报告和复杂的跨模态背景中
文本和图像之间的关系。为了缩小这一差距，我们提出了一部小说
以实体为中心的上下文感知医学视觉语言预训练（ECAMP）
框架，旨在实现更加以实体为中心和
医疗数据的上下文相关解释。利用近期强大的
大语言模型，我们从医疗报告中提取以实体为中心的上下文，
这使得ECAMP能够从文本模态中获得更有效的监督。
通过使用精心设计的实体感知进一步预训练我们的模型，
上下文增强的掩码语言建模和上下文引导的超分辨率
任务中，ECAMP 显着改善了文本和图像之间的相互作用
模式，从而增强提取以实体为中心的能力
上下文特征。此外，我们提出的多尺度上下文融合设计
还提高了粗略和精细图像的语义集成
表示，促进多尺度下游更好的性能
应用程序。组合这些组件会带来显着的性能飞跃
超越当前最先进的方法，并建立了新的标准
医学影像中的跨模态学习，其有效性已得到证明
通过我们对各种任务（包括分类）的广泛实验，
跨多个公共数据集的分割和检测。代码和型号是
可在 https://github.com/ToniChopp/ECAMP 获取。
]]></description>
      <guid>http://arxiv.org/abs/2312.13316</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>使用统一编码模型的计算光谱成像：比较研究及其他。 (arXiv:2312.13310v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.13310</link>
      <description><![CDATA[计算光谱成像由于
快照优势，幅度、相位和波长编码系统
三种具有代表性的实现方式。相当比较和
了解这些系统的性能至关重要，但具有挑战性
由于编码设计的异质性。为了克服这个限制，我们
提出覆盖所有物理系统的统一编码模型（UEM）
三种编码类型。具体来说，UEM包括物理幅度，
可以组合的物理相位和物理波长编码模型
在联合编码器-解码器优化框架中使用数字解码模型
在统一的实验设置下公平地比较三个系统。
此外，我们将 UEM 扩展到理想版本，即理想振幅，
理想相位和理想波长编码模型，不受物理影响
约束，探索三种类型计算的全部潜力
光谱成像系统。最后，我们进行整体比较
三种类型的计算光谱成像系统，并提供有价值的
未来设计和利用这些系统的见解。
]]></description>
      <guid>http://arxiv.org/abs/2312.13310</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>ParamISP：使用相机参数学习正向和反向 ISP。 (arXiv:2312.13313v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.13313</link>
      <description><![CDATA[RAW 图像很少被共享，主要是因为其数据量与
到相机 ISP 获得的 sRGB 对应项。学习前进和
相机 ISP 的逆过程最近已被证明，使得
对输入 sRGB 图像进行具有物理意义的 RAW 级图像处理。然而，
现有的基于学习的 ISP 方法无法处理
ISP 处理有关相机参数的信息，例如 ISO 和曝光时间，
并且在用于各种应用时存在局限性。在本文中，我们
提出ParamISP，一种基于学习的正向和反向转换方法
sRGB 和 RAW 图像之间，采用新颖的神经网络模块
利用相机参数，称为 ParamNet。给定相机
EXIF数据中提供的参数，ParamNet将它们转换为特征
矢量来控制 ISP 网络。大量的实验表明
与相比，ParamISP 可实现卓越的 RAW 和 sRGB 重建结果
以前的方法，它可以有效地用于各种应用
例如去模糊数据集合成、原始去模糊、HDR 重建等
相机到相机的传输。
]]></description>
      <guid>http://arxiv.org/abs/2312.13313</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>并非所有步骤都是平等的：渐进扩散模型的高效生成。 （arXiv：2312.13307v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.13307</link>
      <description><![CDATA[扩散模型在各种生成中都表现出了显着的功效
具有去噪模型预测能力的任务。目前，这些型号
在所有时间步长上采用统一的去噪方法。然而，固有的
每个时间步的噪声潜伏的变化会导致训练期间的冲突，
限制扩散模型的潜力。为了应对这一挑战，我们
提出了一种新颖的两阶段训练策略，称为步进自适应训练。在
在初始阶段，训练基本去噪模型以涵盖所有
时间步长。随后，我们将时间步分为不同的组，
对每组内的模型进行微调以实现专门的去噪
能力。认识到预测噪声的困难
不同的时间步长有所不同，我们引入了多样化的模型尺寸要求。我们
通过估计任务动态调整每个时间步的模型大小
基于微调前的信噪比的难度。这
基于代理的结构重要性评估促进了调整
机制，实现对基础去噪模型的精确高效剪枝。
我们的实验验证了所提出的训练策略的有效性，
证明 CIFAR10 的 FID 分数提高了 0.3 以上，同时
仅利用 80% 的计算资源。这种创新方法
不仅增强了模型性能，还显着降低了
计算成本，为开发和应用开辟新途径
扩散模型。
]]></description>
      <guid>http://arxiv.org/abs/2312.13307</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>SWAGS：自适应采样窗口以实现动态 3D 高斯分布。 （arXiv：2312.13308v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13308</link>
      <description><![CDATA[新颖的视图合成最近取得了快速进展，其方法能够
产生更加逼真的结果。 3D 高斯泼溅有
成为一种特别有前途的方法，可以生成高质量的渲染图
静态场景并支持实时帧速率的交互式观看。
但目前仅限于静态场景。在这项工作中，我们扩展了
3D 高斯泼溅重建动态场景。我们对一个动力学进行建模
使用可调谐 MLP 来构建场景，该 MLP 从规范中学习变形场
每帧一组 3D 高斯空间。解开静态与动态
在场景的某些部分，我们为每个高斯学习一个可调参数，
权衡各个 MLP 参数以将注意力集中在动态部分。
这提高了模型捕捉场景动态的能力
静态区域与动态区域的不平衡。处理任意长度的场景
在保持高渲染质量的同时，我们引入了自适应窗口
根据样本数量将序列划分为窗口的采样策略
序列中的运动。我们训练一个单独的动态高斯泼溅模型
对于每个窗口，允许改变规范表示，从而使
具有显着几何或拓扑变化的场景的重建。
使用具有自我监督的微调步骤来强制执行时间一致性
随机采样的新颖观点的一致性损失。结果，我们的方法
生成具有竞争力的一般动态场景的高质量渲染
定量表现，可以通过我们的动态实时查看
交互式查看器。
]]></description>
      <guid>http://arxiv.org/abs/2312.13308</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>融合品类共性与个性化风格生成电商产品背景。 （arXiv：2312.13309v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13309</link>
      <description><![CDATA[最先进的电子商务产品背景生成方法
在扩大规模时，因设计产品提示而效率低下
生产，以及描述细粒度的无效性
为某些特定品牌定制个性化背景时的样式。到
针对这些障碍，我们将品类共性性和个性化融为一体
风格转化为扩散模型。具体来说，我们提出了一个类别生成器
首次实现大规模背景生成。一个特别的
提示中的标识符被分配给每个类别，其关注点是
通过掩模引导的交叉注意层位于背景上以学习
类别明智的风格。此外，对于具有特定和细粒度的产品
根据布局、元素等方面的要求，设计了个性化生成器
直接从参考图像中学习这种个性化风格来解决
文本歧义，并以自我监督的方式进行训练，以获取更多信息
有效的训练数据使用。为了推进这一领域的研究，首先
大规模电商产品后台生成数据集BG60k为
构建，涵盖超过 2000 个类别的 60k 多个产品图像。
实验证明我们的方法可以生成高质量的背景
针对不同的类别，保持个性化的背景风格
参考图像。 BG60k 和代码的链接即将提供。
]]></description>
      <guid>http://arxiv.org/abs/2312.13309</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>DVIS++：改进的通用视频分割解耦框架。 （arXiv：2312.13305v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13305</link>
      <description><![CDATA[我们提出\textbf{D}耦合\textbf{VI}deo \textbf{S}分段（DVIS）
框架，一种解决通用视频挑战性任务的新颖方法
分割，包括视频实例分割（VIS）、视频语义
分割（VSS）和视频全景分割（VPS）。与之前不同
以端到端方式对视频分割进行建模的方法，我们的方法
将视频分割解耦为三个级联子任务：分割、
跟踪、细化。这种解耦设计允许更简单、更多
对象时空表示的有效建模，
尤其是在复杂场景和长视频中。据此，我们介绍两种
新颖的组件：引用跟踪器和时间细化器。这些
组件逐帧跟踪对象并建立时空模型
基于预对齐特征的表示。改善追踪
基于 DVIS 的能力，我们提出了一种去噪训练策略并引入
对比学习，产生了一个更强大的框架，名为 DVIS++。
此外，我们在各种设置下评估 DVIS++，包括开放词汇表
并使用冷冻的预训练骨干。通过将 CLIP 与 DVIS++ 集成，我们
推出 OV-DVIS++，第一个开放词汇通用视频分割
框架。我们对六个主流基准进行了广泛的实验，
包括 VIS、VSS 和 VPS 数据集。使用统一架构，DVIS++
在这些方面明显优于最先进的专门方法
封闭和开放词汇环境中的基准。
代码：~\url{https://github.com/zhang-tao-whu/DVIS_Plus}。
]]></description>
      <guid>http://arxiv.org/abs/2312.13305</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:22 GMT</pubDate>
    </item>
    <item>
      <title>通过自组织高斯网格的紧凑 3D 场景表示。 （arXiv：2312.13299v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.13299</link>
      <description><![CDATA[3D 高斯溅射最近成为一种非常有前途的技术
用于静态 3D 场景建模。与神经辐射场相比，它
利用高效的光栅化，可以非常快速地渲染
高质量。然而，存储大小明显更高，这阻碍了
实际部署，例如在资源受限的设备上。在本文中，我们
引入组织 3D 参数的紧凑场景表示
高斯分布 (3DGS) 到具有局部同质性的 2D 网格中，确保
在不影响视觉质量的情况下大幅减少存储需求
渲染期间。我们的想法的核心是明确利用
自然场景中存在的感知冗余。从本质上讲，固有的
场景的性质允许高斯参数的多种排列
等价地表示它。为此，我们提出了一种新颖的高度并行
将高维高斯参数规则排列成的算法
二维网格，同时保留其邻域结构。在训练过程中，我们
进一步加强网格中排序参数之间的局部平滑度。这
未压缩的高斯使用与 3DGS 相同的结构，确保无缝
与已建立的渲染器集成。我们的方法实现了减少因子
对于复杂场景，大小为 8 倍到 26 倍，且不增加训练时间，
标志着3D场景分发领域的实质性飞跃
消耗。更多信息可以在我们的项目页面上找到：
https://fraunhoferhhi.github.io/Self-Organizing-Gaussians/
]]></description>
      <guid>http://arxiv.org/abs/2312.13299</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAW 图像进行端到端雨纹去除。 (arXiv:2312.13304v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.13304</link>
      <description><![CDATA[在这项工作中，我们解决了使用 RAW 图像去除雨纹的问题。
一般的做法是先将RAW数据处理成RGB图像，然后
使用 RGB 图像消除雨条纹。其实雨的原始信息
RAW 图像中的图像受图像信号处理 (ISP) 管道的影响，包括
非线性算法、意外噪声、伪影等。则收获更多
有利于在处理成 RGB 之前直接去除 RAW 数据中的雨水
格式。为了解决这个问题，我们提出了除雨和除雨的联合解决方案
RAW处理，从雨天RAW图像中获得干净的彩色图像。成为
具体来说，我们通过将彩色雨条纹转换为 RAW 来生成雨天 RAW 数据
空间并设计简单但高效的RAW处理算法来合成
雨天和干净的彩色图像。雨天彩色图像作为参考
帮助色彩校正。不同的主干表明我们的方法进行了
与其他几种最先进的除雨方法相比，效果更好
专注于彩色图像。此外，所提出的网络可以很好地推广到
我们选择的 RAW 数据集之外的其他相机。最后我们给出结果
对不同 ISP 管道处理的图像进行测试以显示
与方法相比，我们模型的泛化性能更好
彩色图像。
]]></description>
      <guid>http://arxiv.org/abs/2312.13304</guid>
      <pubDate>Fri, 22 Dec 2023 03:14:21 GMT</pubDate>
    </item>
    </channel>
</rss>