<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 21 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>重新审视对抗性提示调整的鲁棒泛化</title>
      <link>https://arxiv.org/abs/2405.11154</link>
      <description><![CDATA[arXiv:2405.11154v1 公告类型：新
摘要：了解 CLIP 等大规模预训练视觉语言模型在对抗性攻击下的脆弱性是确保在各种下游任务上具有零样本泛化能力的关键。最先进的防御机制通常采用即时学习策略进行对抗性微调，以提高预训练模型的对抗性鲁棒性，同时保持适应下游任务的效率。这样的设置会导致过度拟合的问题，阻碍模型在干净和对抗性示例上的泛化能力的进一步提高。在这项工作中，我们提出了一种自适应一致性引导的对抗性即时调整 (即 CAPT) 框架，该框架利用多模态即时学习来增强对抗性示例的图像和文本特征的对齐，并利用预训练 CLIP 的强大泛化能力来指导模型 - 增强其在对抗性示例上的鲁棒泛化能力，同时保持其在干净示例上的准确性。我们还设计了一种新颖的自适应一致性目标函数，以平衡微调模型和预训练模型之间对抗性输入和干净输入的一致性。我们在 14 个数据集和 4 种数据稀疏方案（从 1-shot 到完整训练数据设置）上进行了广泛的实验，以展示 CAPT 优于其他最先进的自适应方法。CAPT 在分布内性能和输入分布偏移和跨数据集下的泛化方面表现出色。]]></description>
      <guid>https://arxiv.org/abs/2405.11154</guid>
      <pubDate>Tue, 21 May 2024 06:20:07 GMT</pubDate>
    </item>
    <item>
      <title>检测上下文不足的多模态情况并避免毫无根据的预测</title>
      <link>https://arxiv.org/abs/2405.11145</link>
      <description><![CDATA[arXiv:2405.11145v1 公告类型：新
摘要：尽管 VQA v2、OKVQA、A-OKVQA、GQA、VCR、SWAG 和 VisualCOMET 等视觉语言理解 (VLU) 基准测试得到了广泛采用，但我们的分析揭示了一个影响其完整性的普遍问题：这些基准测试包含以下样本：答案依赖于所提供的上下文不支持的假设。基于此类数据的训练模型会助长有偏见的学习和幻觉，因为模型往往会做出类似的无根据的假设。为了解决这个问题，我们尽可能收集每个样本的上下文数据，并训练上下文选择模块以促进基于证据的模型预测。多个基准的显着改进证明了我们方法的有效性。此外，我们开发了一种通用的上下文感知放弃（CARA）检测器来识别缺乏足够上下文的样本，并通过在不存在所需上下文时放弃响应来提高模型准确性。 CARA 展示了对其未受过训练的新基准的泛化，强调了其在未来 VLU 基准检测或清理背景不足的样本方面的实用性。最后，我们策划了一个上下文歧义和充分性评估（CASE）集来对不充分的上下文检测器的性能进行基准测试。总的来说，我们的工作在确保视觉语言模型在复杂的现实场景中生成可信且基于证据的输出方面取得了重大进步。]]></description>
      <guid>https://arxiv.org/abs/2405.11145</guid>
      <pubDate>Tue, 21 May 2024 06:20:06 GMT</pubDate>
    </item>
    <item>
      <title>具有边界注意力的息肉分割多尺度信息共享和选择网络</title>
      <link>https://arxiv.org/abs/2405.11151</link>
      <description><![CDATA[arXiv:2405.11151v1 公告类型：新
摘要：结肠镜检查图像的息肉分割在临床实践中至关重要。它可以为结直肠癌的诊断和手术提供有价值的信息。虽然现有的方法已经取得了相对较好的效果，但息肉分割仍然面临以下挑战：（1）结肠镜检查中的光照条件不同，息肉的位置、大小和形态也不同。（2）息肉与周围组织之间的边界不清晰。为了应对这些挑战，我们提出了一种用于息肉分割任务的多尺度信息共享和选择网络（MISNet）。我们设计了一个选择性共享融合模块（SSFM）来加强低级和高级特征之间的信息共享和主动选择，从而增强模型捕获综合信息的能力。然后，我们设计了一个并行注意模块（PAM）来增强模型对边界的注意力，以及一个平衡权重模块（BWM）以促进自下而上过程中边界分割的持续细化。在五个息肉分割数据集上的实验表明，MISNet 成功提高了分割结果的准确性和清晰度，优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2405.11151</guid>
      <pubDate>Tue, 21 May 2024 06:20:06 GMT</pubDate>
    </item>
    <item>
      <title>与扩散模型之间的灵活运动</title>
      <link>https://arxiv.org/abs/2405.11126</link>
      <description><![CDATA[arXiv:2405.11126v1 公告类型：新
摘要：中间运动是角色动画中的一项基本任务，包括生成合理地插入用户提供的关键帧约束的运动序列。长期以来，它一直被认为是一个劳动密集型且具有挑战性的过程。我们研究了扩散模型在生成由关键帧引导的不同人体运动方面的潜力。与之前的中间方法不同，我们提出了一种简单的统一模型，能够生成精确且多样化的运动，符合灵活的用户指定的空间约束范围以及文本调节。为此，我们提出了条件运动扩散中间（CondMDI），它允许任意密集或稀疏关键帧放置和部分关键帧约束，同时生成与给定关键帧不同且一致的高质量运动。我们评估了 CondMDI 在文本条件 HumanML3D 数据集上的性能，并证明了关键帧中间扩散模型的多功能性和有效性。我们进一步探索使用指导和基于插补的方法进行推理时间关键帧，并将 CondMDI 与这些方法进行比较。]]></description>
      <guid>https://arxiv.org/abs/2405.11126</guid>
      <pubDate>Tue, 21 May 2024 06:20:05 GMT</pubDate>
    </item>
    <item>
      <title>MotionGS：通过运动滤波器的紧凑型高斯泼溅 SLAM</title>
      <link>https://arxiv.org/abs/2405.11129</link>
      <description><![CDATA[arXiv:2405.11129v1 Announce Type: new 
摘要：神经辐射场（NeRF）和3D高斯铺展（3DGS）以其高保真的场景表示能力，深深吸引了SLAM领域的关注。最近，基于NeRF的SLAM出现了激增，而基于3DGS的SLAM则比较稀疏。本文提出了一种基于3DGS的新型SLAM方法，融合了深度视觉特征、双关键帧选择和3DGS。与现有方法相比，所提出的选择性跟踪是通过对每帧进行特征提取和运动滤波来实现的。姿态和3D高斯的联合优化贯穿整个建图过程。此外，通过双关键特征选择和新颖的损失函数实现了由粗到细的姿态估计和紧凑的高斯场景表示。实验结果表明，所提出的算法不仅在跟踪和建图方面优于现有方法，而且内存使用量更少。]]></description>
      <guid>https://arxiv.org/abs/2405.11129</guid>
      <pubDate>Tue, 21 May 2024 06:20:05 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯学习驱动的类增量学习原型对比损失</title>
      <link>https://arxiv.org/abs/2405.11067</link>
      <description><![CDATA[arXiv:2405.11067v1 公告类型：新
摘要：持续学习方法的主要目标是随着时间的推移从数据流中按顺序学习任务，同时减轻灾难性遗忘的有害现象。在本文中，我们专注于学习以前的类原型和新遇到的类原型之间的最佳表示。我们提出了一个带有贝叶斯学习驱动的对比损失（BLCL）的原型网络，专为类增量学习场景量身定制。因此，我们引入了对比损失，通过减少类内距离和增加类间距离，将新类合并到潜在表示中。我们的方法通过贝叶斯学习技术动态调整交叉熵和对比损失函数之间的平衡。对用于图像分类的 CIFAR-10 数据集和用于干扰分类的基于 GNSS 数据集的图像进行的实证评估验证了我们方法的有效性，展示了其相对于现有最先进方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2405.11067</guid>
      <pubDate>Tue, 21 May 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>通过野生动物重新识别加深了解</title>
      <link>https://arxiv.org/abs/2405.11112</link>
      <description><![CDATA[arXiv:2405.11112v1 公告类型：新
摘要：我们通过使用 NumPy 从头开始​​实现 MLP、使用 Keras 实现 DCNN 以及使用 LightGBM 实现二元分类器来探索野生动物重新识别领域，以学习作业。分析多个模型在多个数据集上的性能。我们尝试复制先前在野生动物重新识别方面的度量学习研究。首先，我们发现使用 MLP 进行分类训练，然后删除输出层并使用倒数第二层作为嵌入，对于类似的学习来说并不是一个成功的策略；似乎需要为嵌入设计的损失，例如三元组损失。 DCNNS 在某些数据集上表现良好，但在其他数据集上表现不佳，这与之前文献中的发现不一致。 LightGBM 分类器过度拟合，并且在使用准确度作为指标对所有对进行训练和评估时，并没有明显优于恒定模型。根据与文档示例的比较以及某些数据集上的良好结果，所使用的技术实现似乎符合标准。然而，要充分再现过去的文学，还有很多需要探索的地方。]]></description>
      <guid>https://arxiv.org/abs/2405.11112</guid>
      <pubDate>Tue, 21 May 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>用于元少镜头图像分类的多模态 CLIP 推理</title>
      <link>https://arxiv.org/abs/2405.10954</link>
      <description><![CDATA[arXiv:2405.10954v1 公告类型：新
摘要：在最近的文献中，few-shot 分类主要由 N 路 k-shot 元学习问题来定义。为此目的而设计的模型通常经过严格的训练，以在有限的设置（不使用外部数据）的标准基准测试中表现出色。鉴于大型语言和视觉模型的最新进展，自然会出现一个问题：这些模型能否直接在元少样本学习基准上表现良好？像 CLIP 这样学习联合（图像、文本）嵌入的多模态基础模型特别令人感兴趣。事实上，多模态训练已被证明可以增强模型的鲁棒性，特别是在模糊性方面，这是在几次镜头设置中经常观察到的限制。这项研究表明，在广泛采用的基准上，结合 CLIP 文本和图像编码器的模式优于最先进的元少样本学习器，并且所有这些都不需要额外的训练。我们的结果证实了 CLIP 等多模式基础模型的潜力和稳健性，并可作为利用此类模型的现有和未来方法的基线。]]></description>
      <guid>https://arxiv.org/abs/2405.10954</guid>
      <pubDate>Tue, 21 May 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 Google Earth 图像和高斯溅射进行真实感 3D 城市场景重建和点云提取</title>
      <link>https://arxiv.org/abs/2405.11021</link>
      <description><![CDATA[arXiv:2405.11021v1 公告类型：新
摘要： 3D城市场景重建和建模是遥感的一个重要研究领域，在学术界、商业、工业和行政管理领域有着广泛的应用。视图合成模型的最新进展促进了仅从 2D 图像进行逼真的 3D 重建。利用 Google 地球图像，我们构建了以滑铁卢大学为中心的滑铁卢地区的 3D 高斯 Splatting 模型，并且能够实现远远超过之前基于神经辐射场的 3D 视图合成结果的视图合成结果（我们在基准测试中展示了这一结果） 。此外，我们使用从 3D 高斯喷射模型中提取的 3D 点云来检索场景的 3D 几何形状，该模型以场景的多视图立体密集重建为基准，从而重建大型物体的 3D 几何形状和真实感照明。 - 通过 3D 高斯泼溅缩放城市场景]]></description>
      <guid>https://arxiv.org/abs/2405.11021</guid>
      <pubDate>Tue, 21 May 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉变压器设备上训练的块选择性重编程</title>
      <link>https://arxiv.org/abs/2405.10951</link>
      <description><![CDATA[arXiv:2405.10951v1 公告类型：新
摘要：用于各种边缘应用（包括个性化学习）的视觉变换器（ViT）的普遍存在，产生了对设备上微调的需求。然而，利用边缘设备有限的内存和计算能力进行训练仍然是一个重大挑战。特别是，训练所需的内存远高于推理所需的内存，主要是因为需要存储所有层的激活，以便计算权重更新所需的梯度。之前的工作已经探索通过冻结重量训练以及以压缩格式存储激活来减少这种内存需求。然而，这些方法由于无法提供训练或推理加速而被认为效率低下。在本文中，我们首先研究了旨在减少内存和计算需求的现有设备上训练方法的局限性。然后，我们提出了块选择性重编程（BSR），其中我们仅微调预训练模型的总块的一小部分，并根据冻结层的自注意力分数选择性地删除标记。为了展示 BSR 的功效，我们使用五个不同的数据集对 ViT-B 和 DeiT-S 进行了广泛的评估。与现有替代方案相比，我们的方法可同时将训练内存减少高达 1.4 倍，将计算成本减少高达 2 倍，同时保持相似的准确性。我们还展示了混合专家 (MoE) 模型的结果，证明了我们的方法在多任务学习场景中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.10951</guid>
      <pubDate>Tue, 21 May 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>VICAN：适用于大型相机网络的非常高效的校准算法</title>
      <link>https://arxiv.org/abs/2405.10952</link>
      <description><![CDATA[arXiv:2405.10952v1 公告类型：新
摘要：大型相机网络中相机姿态的精确估计是计算机视觉和机器人技术的一个基本问题，在自主导航、监视和增强现实等领域有着广泛的应用。在本文中，我们介绍了一种新颖的方法，该方法扩展了最先进的姿势图优化（PGO）技术。与主要依赖于相机-相机边缘的传统 PGO 范式不同，我们的方法集中于引入动态元素 - 任何在场景中自由移动的刚性物体 - 其姿势可以从单个图像中可靠地推断出来。具体来说，我们考虑包含相机、动态演变的物体姿态以及每个时间步的相机-物体相对变换的二部图。这种转变不仅为直接估计相机之间的相对姿势所遇到的挑战提供了解决方案，特别是在不利的环境中，而且还利用包含大量物体姿势来改善和整合误差，从而获得准确的相机姿势估计。尽管我们的框架保留了与传统 PGO 求解器的兼容性，但其功效得益于定制的优化方案。为此，我们引入了一种迭代原对偶算法，能够处理大型图。在模拟室内环境的新数据集上进行的实证基准证实了我们方法的功效和效率。]]></description>
      <guid>https://arxiv.org/abs/2405.10952</guid>
      <pubDate>Tue, 21 May 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>Surgical-LVLM：学习适应大型视觉语言模型以实现机器人手术中的基础视觉问答</title>
      <link>https://arxiv.org/abs/2405.10948</link>
      <description><![CDATA[arXiv:2405.10948v1 公告类型：新
摘要：手术视觉问答（Surgical-VQA）和相关区域接地的最新进展为机器人和医疗应用展现了巨大的前景，满足了个性化手术指导中对自动化方法的迫切需求。然而，现有模型主要提供简单的结构化答案，并且由于其识别远程依赖性和对齐多模态信息的能力有限，因此难以应对复杂的场景。在本文中，我们介绍了 Surgical-LVLM，这是一种专为复杂手术场景量身定制的新型个性化大型视觉语言模型。利用预先训练的大型视觉语言模型和专门的视觉感知 LoRA (VP-LoRA) 模块，我们的模型擅长理解手术环境中复杂的视觉语言任务。在解决视觉基础任务时，我们提出了令牌交互（TIT）模块，该模块在将大型视觉语言模型（LVLM）的语言响应投影到潜在空间后，加强了基础模块和语言响应之间的交互。我们在多个基准上展示了 Surgical-LVLM 的有效性，包括 EndoVis-17-VQLA、EndoVis-18-VQLA 以及新引入的 EndoVis Conversations 数据集，该数据集设定了新的性能标准。我们的工作通过提供上下文感知解决方案来推动自动化手术指导领域的发展。]]></description>
      <guid>https://arxiv.org/abs/2405.10948</guid>
      <pubDate>Tue, 21 May 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>全球车牌数据集</title>
      <link>https://arxiv.org/abs/2405.10949</link>
      <description><![CDATA[arXiv:2405.10949v1 公告类型：新
摘要：为了追求道路安全、交通监控、监视和物流自动化领域的最先进水平 (SOTA)，我们引入了全球车牌数据集 (GLPD)。该数据集包含超过 500 万张图像，包括从 74 个国家捕获的各种样本，并进行了细致的注释，包括车牌字符、车牌分割掩模、车牌角顶点以及车辆制造商、颜色和型号。我们还包括更多类别的注释数据，例如行人、车辆、道路等。我们包括数据集的统计分析，并提供基线高效且准确的模型。 GLPD 旨在成为车牌识别模型开发和微调的主要基准数据集。]]></description>
      <guid>https://arxiv.org/abs/2405.10949</guid>
      <pubDate>Tue, 21 May 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>深度感知全景分割</title>
      <link>https://arxiv.org/abs/2405.10947</link>
      <description><![CDATA[arXiv:2405.10947v1 公告类型：新
摘要：全景分割统一了语义和实例分割，从而提供了语义类标签，对于所谓的事物类，还提供了每个像素的实例标签。区分具有相似外观的同一类的不同对象尤其具有挑战性，并且经常导致这些对象被错误地分配给单个实例。在目前的工作中，我们证明了有关观察场景的 3D 几何信息可以用来缓解这个问题：我们提出了一种基于 CNN 的新颖的全景分割方法，该方法处理在单独的网络分支中作为输入给出的 RGB 图像和深度图并以后期融合的方式融合所得的特征图。此外，我们提出了一种新的深度感知骰子损失项，它根据像素到相机的相关距离之间的差异来惩罚将像素分配给同一事物实例。在 Cityscapes 数据集上进行的实验表明，所提出的方法减少了错误合并到一个事物实例中的对象数量，并且在全景质量方面比用作基础的方法高了 2.2%。]]></description>
      <guid>https://arxiv.org/abs/2405.10947</guid>
      <pubDate>Tue, 21 May 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>张量化神经网络在云分类中的应用</title>
      <link>https://arxiv.org/abs/2405.10946</link>
      <description><![CDATA[arXiv:2405.10946v1 公告类型：新
摘要：卷积神经网络（CNN）由于其提取空间信息、共享参数和学习局部特征的卓越能力，在天气预报、计算机视觉、自动驾驶和医学图像分析等各个领域得到了广泛的应用。然而，CNN 在这些领域的实际实施和商业化受到模型大小、过度拟合和计算时间相关挑战的阻碍。为了解决这些限制，我们的研究提出了一种突破性的方法，包括对 CNN 中的密集层进行张量化，以减少模型大小和计算时间。此外，我们将注意力层合并到 CNN 中，并使用对比自监督学习对其进行训练，以有效地对云信息进行分类，这对于准确的天气预报至关重要。我们阐明了张量神经网络（TNN）的关键特征，包括数据压缩率、准确性和计算速度。结果表明 TNN 在批量大小设置下如何改变其属性。]]></description>
      <guid>https://arxiv.org/abs/2405.10946</guid>
      <pubDate>Tue, 21 May 2024 06:19:59 GMT</pubDate>
    </item>
    </channel>
</rss>