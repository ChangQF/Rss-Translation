<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Wed, 10 Jan 2024 06:18:29 GMT</lastBuildDate>
    <item>
      <title>StarCraftImage：用于对多智能体环境的空间推理方法进行原型设计的数据集。 （arXiv：2401.04290v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04290</link>
      <description><![CDATA[多智能体环境中的空间推理任务，例如事件预测、
代理类型识别或缺失数据插补对于
多种应用（例如，通过传感器网络进行自主监视和
强化学习（RL）的子任务）。星际争霸 II 游戏重播编码
智能（和对抗性）多代理行为，可以提供测试平台
为了这些任务；然而，提取简单且标准化的表示
因为对这些任务进行原型设计非常费力并且阻碍了可重复性。在
相比之下，MNIST 和 CIFAR10 尽管极其简单，但却能够
机器学习方法的快速原型设计和可重复性。遵循简单性
在这些数据集中，我们构建了一个基准空间推理数据集
《星际争霸 II》重播展示了复杂的多智能体行为，同时仍然
与 MNIST 和 CIFAR10 一样易于使用。具体来说，我们仔细总结一下
255 个连续游戏状态的窗口可创建 360 万个摘要图像
来自 60,000 次重播，包括所有相关元数据，例如游戏结果和
玩家比赛。我们开发了三种降低复杂性的格式： 高光谱
包含每种单位类型一个通道的图像（类似于多光谱
地理空间图像）、模仿 CIFAR10 的 RGB 图像以及
模仿 MNIST。我们展示了如何使用该数据集来构建空间原型
推理方法。所有数据集、提取代码和数据集代码
加载可以在 https://starcraftdata.davidinouye.com 找到
]]></description>
      <guid>http://arxiv.org/abs/2401.04290</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:28 GMT</pubDate>
    </item>
    <item>
      <title>视觉重新构想：人工智能驱动的 WiFi 室内成像突破。 （arXiv：2401.04317v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04317</link>
      <description><![CDATA[室内成像是机器人和物联网的一项关键任务。无线上网
作为一种无所不在的信号，它是执行被动攻击的有希望的候选者
成像并将最新信息同步到所有连接的设备。
这是第一个将 WiFi 室内成像视为
多模态图像生成任务，将测得的 WiFi 功率转换为
高分辨率室内图像。我们提出的 WiFi-GEN 网络已初具规模
重建精度是基于物理模型的重建精度的 275%
反演方法。此外，Frechet 起始距离分数已
大幅减少82%。检查模型的有效性
任务，发布第一个包含 80,000 对 WiFi 的大规模数据集
信号和成像目标。我们的模型吸收了基于模型的挑战
方法包括非线性、不适定性和不确定性
我们的生成人工智能网络的大量参数。网络也设计了
以最适合测量的 WiFi 信号和所需的成像输出。为了
再现性，我们将在接受后发布数据和代码。
]]></description>
      <guid>http://arxiv.org/abs/2401.04317</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:28 GMT</pubDate>
    </item>
    <item>
      <title>RadarCam-Depth：雷达相机融合，用于利用学习的公制尺度进行深度估计。 （arXiv：2401.04325v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04325</link>
      <description><![CDATA[我们提出了一种基于度量密集深度估计的新方法
单视图图像和稀疏、嘈杂的雷达点云的融合。直接的
异构雷达和图像数据或其编码的融合往往
产生具有明显伪影、模糊边界的密集深度图，以及
准确性次优。为了解决这个问题，我们学习增强通用性
以及鲁棒的单目深度预测，具有密集的度量尺度
稀疏且嘈杂的雷达数据。我们提出了一个雷达相机框架，用于高度
准确且详细的密集深度估计有四个阶段，包括
单目深度预测，单目深度的全局尺度对齐
稀疏雷达点，通过学习准密集尺度估计
雷达点和图像块之间的关联以及局部尺度细化
使用比例图学习器获得密集深度。我们提出的方法显着
优于最先进的雷达相机深度估计方法
深度估计的平均绝对误差 (MAE) 分别降低 25.6% 和 40.2%
在具有挑战性的 nuScenes 数据集和我们自行收集的 ZJU-4DRadarCam 上
数据集，分别。
]]></description>
      <guid>http://arxiv.org/abs/2401.04325</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:28 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定扩散的鲁棒图像水印。 （arXiv：2401.04247v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04247</link>
      <description><![CDATA[图像加水印对于跟踪图像出处和声明至关重要
所有权。随着稳定扩散等生成模型的出现，
为了创建虚假但逼真的图像，水印变得尤为重要
重要的是，例如，使生成的图像能够可靠地识别。很遗憾，
同样的稳定扩散技术可以去除使用注入的水印
现有的方法。为了解决这个问题，我们提出了 ZoDiac，它使用
预训练的稳定扩散模型，将水印注入可训练的模型中
潜在空间，从而产生可以在
潜在向量，即使受到攻击时也是如此。我们根据三个基准评估 ZoDiac，
MS-COCO、DiffusionDB 和 WikiArt，并发现 ZoDiac 对于
最先进的水印攻击，水印检测率超过98%
误报率低于 6.4%，优于最先进的技术
水印方法。我们的研究表明稳定扩散是
有前景的稳健水印方法，能够承受甚至
基于稳定扩散的攻击。
]]></description>
      <guid>http://arxiv.org/abs/2401.04247</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:27 GMT</pubDate>
    </item>
    <item>
      <title>使用隐藏融合模型检测人脸合成。 （arXiv：2401.04257v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04257</link>
      <description><![CDATA[由于以下原因，人脸图像合成在计算机安全中受到越来越多的关注
对其潜在负面影响的担忧，包括与假货相关的影响
生物识别技术。因此，建立可以检测合成人脸图像的模型
是一个需要应对的重要挑战。在本文中，我们提出了一种基于融合的
检测人脸图像合成的策略，同时为多个人提供弹性
攻击。所提出的策略使用由下式计算的输出的后期融合
几个未公开的模型依靠随机多项式系数和
指数来隐藏新的特征空间。与现有的隐藏解决方案不同，
我们的策略不需要量化，这有助于保留特征
空间。我们的实验表明我们的策略达到了最先进的水平
性能，同时提供防止中毒、扰动的保护，
后门和反向模型攻击。
]]></description>
      <guid>http://arxiv.org/abs/2401.04257</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:27 GMT</pubDate>
    </item>
    <item>
      <title>使用贝叶斯 CNN 进行与数据无关的人脸图像合成检测。 （arXiv：2401.04241v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04241</link>
      <description><![CDATA[人脸图像合成检测由于以下原因而受到相当大的关注：
此类合成数据对社会的潜在负面影响
带来。在本文中，我们提出了一种与数据无关的解决方案来检测人脸
图像合成过程。具体来说，我们的解决方案基于异常
仅需要真实数据即可学习推理的检测框架
过程。因此，它是与数据无关的，因为它不需要
合成人脸图像。该解决方案使用后验概率
参考数据来确定新样品是否是合成的。我们的
使用不同合成器的评估结果表明我们的解决方案非常
与最先进的技术竞争，这需要合成数据
训练。
]]></description>
      <guid>http://arxiv.org/abs/2401.04241</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>时空湍流缓解：转化视角。 (arXiv:2401.04244v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2401.04244</link>
      <description><![CDATA[恢复因大气湍流而扭曲的图像是一项具有挑战性的工作
由于湍流的随机性质而导致的逆问题。虽然数量众多
湍流缓解（TM）算法已经被提出，它们的效率和
对现实世界动态场景的推广仍然受到严重限制。
基于经典 TM 算法的直觉，我们提出了 Deep
大气湍流缓解网络 (DATUM)。 DATUM 旨在克服主要
从经典学习方法过渡到深度学习方法时面临的挑战。经过
仔细地将经典多框架TM方法的优点整合到一个
深层网络结构，我们证明了 DATUM 可以高效地执行
使用循环方式进行远程时间聚合，同时可变形
注意力和时间通道注意力无缝促进像素
注册和幸运成像。通过额外的监督、倾斜和模糊
可以共同减轻退化。这些归纳偏差使 DATUM 能够
显着优于现有方法，同时性能提高十倍
在处理速度上。大规模训练数据集 ATSyn 被表示为
共同发明以实现真实湍流中的泛化。我们的代码和数据集
将于
\href{https://xg416.github.io/DATUM}{\textcolor{粉色}{https://xg416.github.io/DATUM}}
]]></description>
      <guid>http://arxiv.org/abs/2401.04244</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>unnyNet-W：野外视频中有趣时刻的多模态学习。 （arXiv：2401.04210v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04210</link>
      <description><![CDATA[自动理解有趣的时刻（即让人兴奋的时刻）
笑）当观看喜剧具有挑战性时，因为它们涉及各种功能，
例如肢体语言、对话和文化。在本文中，我们建议
unnynet-w，一个依赖交叉和自注意力的视觉、音频模型
和文本数据来预测视频中的有趣时刻。与大多数依赖的方法不同
基于字幕形式的真实数据，在这项工作中我们利用
视频自然出现的模式：(a) 视频帧，因为它们包含
视觉信息对于场景理解不可或缺，(b) 音频
包含与有趣时刻相关的更高级别的线索，例如语调，
音高和停顿以及 (c) 通过语音转文本自动提取文本
模型，因为它在由大语言处理时可以提供丰富的信息
模型。为了获取训练标签，我们提出了一种无监督方法
发现并标记有趣的音频时刻。我们提供了五个数据集的实验：
情景喜剧《TBBT》、《MHD》、《MUStARD》、《老友记》和 TED 演讲《UR-Funny》。广泛的
实验和分析表明，FunnyNet-W 成功地利用了视觉、
听觉和文字提示来识别有趣的时刻，而我们的研究结果表明
unnyNet-W 能够预测野外的有趣时刻。 unnyNet-W 设置了
最先进的有趣时刻检测，所有内容均具有多模式提示
使用和不使用地面真实信息的数据集。
]]></description>
      <guid>http://arxiv.org/abs/2401.04210</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>SOAP：使用固定对象聚合伪标签进行 3D 对象检测的跨传感器域适应。 （arXiv：2401.04230v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04230</link>
      <description><![CDATA[我们在以下背景下考虑跨传感器域适应问题
基于 LiDAR 的 3D 对象检测并提出固定对象聚合
伪标签 (SOAP) 用于生成静态的高质量伪标签
对象。与当前最先进的领域内实践相比
SOAP 只聚合几个输入扫描，聚合整个点序列
输入级别的云以减少传感器域间隙。然后，通过
我们所说的准静态训练和空间一致性后处理，
SOAP 模型为静止对象生成准确的伪标签，
与少帧检测器相比，域间隙至少为 30.3%。我们的结果也
表明最先进的领域适应方法甚至可以实现
与 SOAP 结合使用，无论是在无人监督还是在无人监督的情况下，都可以获得更好的性能
半监督设置。
]]></description>
      <guid>http://arxiv.org/abs/2401.04230</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>基于对比学习的少镜头动作识别的两流联合匹配方法（arXiv：2401.04150v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04150</link>
      <description><![CDATA[尽管基于度量学习范式的少样本动作识别已经
取得了重大成功，但未能解决以下问题：（1）
行动关系建模不充分且多模式利用不足
信息; (2)处理不同视频匹配问题的挑战
长度和速度，以及视频未对准的视频匹配问题
子动作。为了解决这些问题，我们提出了两流联合匹配
基于对比学习的方法（TSJM），它由两个模块组成：
多模态对比学习模块（MCL）和联合匹配模块（JMM）。
MCL 的目标是广泛研究多式联运的相互关系
信息关系，从而彻底提取模态信息
增强动作关系的建模。 JMM 的目标是同时
解决上述视频匹配问题。的有效性
所提出的方法在两种广泛使用的少镜头动作识别上进行了评估
数据集，即 SSv2 和 Kinetics。综合消融实验是
还进行了验证我们提出的方法的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2401.04150</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>用于音频-视频分类的高效选择性音频屏蔽多模态瓶颈变压器。 （arXiv：2401.04154v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04154</link>
      <description><![CDATA[音频和视频是主流媒体中最常见的两种形式
平台，例如 YouTube。为了有效地从多模态视频中学习，在这个
在工作中，我们提出了一种新颖的音频视频识别方法，称为音频视频
Transformer，AVT，利用有效的时空表示
视频 Transformer 提高动作识别准确性。对于多式联运
fusion，简单地在跨模态 Transformer 中连接多模态 token
需要大量的计算和内存资源，相反我们减少了
通过音视频瓶颈 Transformer 来降低跨模态复杂性。到
提高多模态Transformer的学习效率，我们集成
自监督目标，即音视频对比学习、音视频
匹配和屏蔽音频和视频学习，到 AVT 训练中，映射
将不同的音频和视频表示转化为通用的多模态表示
空间。我们进一步提出了一种掩蔽音频片段损失来学习语义音频
AVT 活动。对三个公共场所进行了广泛的实验和消融研究
数据集和两个内部数据集一致证明了有效性
拟议的 AVT。具体来说，AVT 的性能优于之前的
Kinetics-Sounds 上最先进的同类产品提高了 8%。 AVT也超过了1
VGGSound 上之前最先进的视频 Transformers [25] 提高了 10%
利用音频信号。与之前最先进的之一相比
多模式方法、MBT [32]、AVT 在 FLOP 和
Epic-Kitchens-100 的准确率提高了 3.8%。
]]></description>
      <guid>http://arxiv.org/abs/2401.04154</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>具有快速和慢速思维的语言条件机器人操作。 （arXiv：2401.04181v1 [cs.RO]）</title>
      <link>http://arxiv.org/abs/2401.04181</link>
      <description><![CDATA[语言调节的机器人操作旨在传递自然
将语言指令转化为可执行动作，从简单的拾取和放置到
需要意图识别和视觉推理的任务。受到双重启发
认知科学中的过程理论，提出了两个平行的系统
人类决策中的快速思维和慢速思维，我们引入机器人技术
快慢思维（RFST），模仿人类认知的框架
对任务进行分类并基于两个系统做出决策的架构
指令类型。我们的 RFST 由两个关键部分组成：1) 指令
判别器根据
当前的用户指令，以及 2) 一个思维缓慢的系统，由
与政策网络相一致的微调视觉语言模型，允许
机器人识别用户意图或执行推理任务。评估我们的
方法论中，我们构建了一个包含真实世界轨迹的数据集，捕获
行动范围从自发冲动到需要深思熟虑的任务
沉思。我们在模拟和现实场景中的结果，
确认我们的方法能够熟练地管理需要意图的复杂任务
识别和推理。该项目可在
https://jlm-z.github.io/RSFT/
]]></description>
      <guid>http://arxiv.org/abs/2401.04181</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>用于文本到图像创建的语义绘图工程。 （arXiv：2401.04116v1 [cs.HC]）</title>
      <link>http://arxiv.org/abs/2401.04116</link>
      <description><![CDATA[文本到图像的生成是通过生成对抗网络进行的
（GAN）或变压器模型。然而，当前的挑战在于准确
根据文本描述生成图像，特别是在以下场景中
目标图像的内容和主题不明确。在本文中，我们
提出一种利用人工智能模型进行主题研究的方法
创意，其次是实际绘画的分类建模
过程。该方法涉及将所有视觉元素转换为可量化的
创建图像之前的数据结构。我们评估这一举措的有效性
语义准确性、图像再现性和
与现有图像生成相比，计算效率
算法。
]]></description>
      <guid>http://arxiv.org/abs/2401.04116</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>RHOBIN 挑战：重建人机交互。 （arXiv：2401.04143v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04143</link>
      <description><![CDATA[对人与物体之间的交互进行建模已经成为一个新兴的领域
近年来的研究方向。捕捉人与物体的交互是
然而，由于严重的遮挡和复杂的动力学，这是一项非常具有挑战性的任务，
这不仅需要理解 3D 人体姿势和物体姿势，还需要理解
他们之间的互动。 3D人物和物体的重建
长期以来，计算机视觉领域是两个独立的研究领域。我们因此
提出第一个 RHOBIN 挑战：人物重建
与 RHOBIN 研讨会的互动。其目的是带来
人类和物体重建的研究团体以及
一起讨论交互建模技术并交流想法。我们的
挑战包括单目 RGB 3D 重建的三个轨道
专注于处理具有挑战性的交互场景的图像。我们的
挑战吸引了超过100名参与者，提交了超过300份作品，
表明研究界的广泛兴趣。本文描述了
我们挑战的设置并讨论每个赛道的获胜方法
更详细地说。我们观察到人类重建任务正在成为
即使在严重遮挡设置下也能成熟，同时进行物体姿态估计和
联合重建仍然是一项充满挑战的任务。随着人们越来越感兴趣
交互建模，我们希望这份报告能够提供有用的见解和
促进该方向的未来研究。我们的研讨会网站可以在
\href{https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}。
]]></description>
      <guid>http://arxiv.org/abs/2401.04143</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>基于时间线的流程发现。 （arXiv：2401.04114v1 [cs.HC]）</title>
      <link>http://arxiv.org/abs/2401.04114</link>
      <description><![CDATA[自动流程发现的一个关键问题是提供对
业务流程的性能方面。等待时间有特殊要求
在这方面的重要性。出于这个原因，令人惊讶的是，当前
自动流程发现技术直接生成以下图表并
类似的流程模型，但经常错过明确的机会
代表时间轴。在本文中，我们提出了一种方法
自动构建与时间明确一致的流程模型
轴。我们举例说明了直接跟随图的方法。我们的评价
使用两个 BPIC 数据集和一个专有数据集突出了以下优点：
这种表示与标准布局技术相比。
]]></description>
      <guid>http://arxiv.org/abs/2401.04114</guid>
      <pubDate>Wed, 10 Jan 2024 06:18:22 GMT</pubDate>
    </item>
    </channel>
</rss>