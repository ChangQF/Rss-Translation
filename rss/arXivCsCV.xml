<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 17 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>介绍 HOT3D：用于 3D 手部和物体跟踪的自我中心数据集</title>
      <link>https://arxiv.org/abs/2406.09598</link>
      <description><![CDATA[arXiv:2406.09598v1 公告类型：新
摘要：我们介绍了 HOT3D，这是一个公开可用的数据集，用于 3D 中的自我中心手部和物体跟踪。该数据集提供了超过 833 分钟（超过 370 万张图像）的多视图 RGB/单色图像流，展示了 19 个主体与 33 个不同的刚性物体的交互，多模态信号（例如眼球注视或场景点云）以及全面的地面真实注释，包括物体、手和相机的 3D 姿势以及手和物体的 3D 模型。除了简单的拿起/观察/放下动作外，HOT3D 还包含类似于厨房、办公室和客厅环境中的典型动作的场景。该数据集由 Meta 的两个头戴式设备记录：Project Aria，轻量级 AR/AI 眼镜的研究原型，以及 Quest 3，一款销量达数百万台的生产 VR 耳机。真实姿势由专业动作捕捉系统使用附在手和物体上的小型光学标记获得。手部注释以 UmeTrack 和 MANO 格式提供，物体由内部扫描仪获得的带有 PBR 材料的 3D 网格表示。我们旨在通过公开 HOT3D 数据集并在 ECCV 2024 上共同组织数据集的公开挑战来加速以自我为中心的手部物体交互的研究。可以从项目网站下载数据集：https://facebookresearch.github.io/hot3d/。]]></description>
      <guid>https://arxiv.org/abs/2406.09598</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>事实证明我不是真实的：实现对人工智能生成视频的稳健检测</title>
      <link>https://arxiv.org/abs/2406.09601</link>
      <description><![CDATA[arXiv:2406.09601v1 公告类型：新 
摘要：生成模型在创建高质量视频方面取得了令人瞩目的成就，引发了人们对数字完整性和隐私漏洞的担忧。最近针对 Deepfakes 视频的研究已经开发出能够高度准确地识别 GAN 生成的样本的检测器。然而，这些检测器对由视频创建工具（例如 OpenAI 的 SORA、Runway Gen-2 和 Pika 等）生成的扩散生成视频的鲁棒性仍未得到探索。在本文中，我们提出了一个新颖的框架，用于检测由多个最先进 (SOTA) 生成模型（例如稳定视频扩散）合成的视频。我们发现用于检测扩散生成图像的 SOTA 方法在识别扩散生成的视频方面缺乏鲁棒性。我们的分析表明，当应用于域外视频时，这些检测器的有效性会降低，主要是因为它们难以跟踪帧之间的时间特征和动态变化。为了应对上述挑战，我们使用 SOTA 视频创建工具收集了一个新的扩散生成视频基准视频数据集。我们从视频帧的扩散模型中提取显性知识内的表示，并使用 CNN + LSTM 架构训练我们的检测器。评估表明，我们的框架可以很好地捕捉帧之间的时间特征，对域内视频的检测准确率达到 93.7%，并将域外视频的准确率提高了多达 16 个点。]]></description>
      <guid>https://arxiv.org/abs/2406.09601</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>ImageNet3D：面向通用对象级 3D 理解</title>
      <link>https://arxiv.org/abs/2406.09613</link>
      <description><![CDATA[arXiv:2406.09613v1 公告类型：新
摘要：具有通用对象级 3D 理解的视觉模型应该能够推断自然图像中任意刚性对象的 2D（例如，类名和边界框）和 3D 信息（例如，3D 位置和 3D 视点）。这是一项具有挑战性的任务，因为它涉及从 2D 信号推断 3D 信息，最重要的是，从看不见的类别推广到刚性物体。然而，现有的具有对象级 3D 注释的数据集通常受到类别数量或注释质量的限制。在这些数据集上开发的模型成为某些类别或领域的专家，并且无法概括。在这项工作中，我们提出了 ImageNet3D，这是一个用于通用对象级 3D 理解的大型数据集。 ImageNet3D 使用 2D 边界框、3D 姿势、3D 位置注释和交错有 3D 信息的图像说明，从 ImageNet 数据集中扩充了 200 个类别。借助 ImageNet3D 中提供的新注释，我们可以 (i) 分析视觉基础模型的对象级 3D 感知，(ii) 研究和开发通用模型，推断自然图像中任意刚性对象的 2D 和 3D 信息，(iii) 将统一的 3D 模型与大型语言模型相结合，进行 3D 相关推理。除了标准分类和姿势估计之外，我们还考虑了两个新任务，即探索对象级 3D 感知和开放词汇姿势估计。ImageNet3D 上的实验结果证明了我们的数据集在构建具有更强的通用对象级 3D 理解的视觉模型方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.09613</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:13 GMT</pubDate>
    </item>
    <item>
      <title>Q-Mamba：Vision Mamba 在图像质量评估方面的首次探索</title>
      <link>https://arxiv.org/abs/2406.09546</link>
      <description><![CDATA[arXiv:2406.09546v1 公告类型：新
摘要：在这项工作中，我们首次探索了最近流行的基础模型，即状态空间模型/Mamba，在图像质量评估中的应用，旨在观察和挖掘视觉 Mamba 中的感知潜力。关于 Mamba 的一系列工作已经展示了它在分割和分类等各个领域的巨大潜力。然而，Mamba 的感知能力尚未得到充分开发。因此，我们通过重新审视和调整 Mamba 模型来提出 Q-Mamba，以用于三个关键的 IQA 任务，即任务特定的、通用的和可迁移的 IQA，这表明 Mamba 模型与现有的基础模型（例如 Swin Transformer、ViT 和 CNN）相比，在 IQA 的感知和计算成本方面具有明显的优势。为了提高 Q-Mamba 的可迁移性，我们提出了 StylePrompt 调整范式，其中注入了基本的轻量级均值和方差提示，以协助预训练的 Q-Mamba 针对不同的下游 IQA 任务进行任务自适应迁移学习。与现有的提示调整策略相比，我们提出的 StylePrompt 能够以更低的计算成本实现更好的感知迁移能力。在多个合成、真实 IQA 数据集和跨 IQA 数据集上进行的大量实验证明了我们提出的 Q-Mamba 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.09546</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:12 GMT</pubDate>
    </item>
    <item>
      <title>我的身体我的选择：以人为中心的全身匿名化</title>
      <link>https://arxiv.org/abs/2406.09553</link>
      <description><![CDATA[arXiv:2406.09553v1 公告类型：新
摘要：在人们对在线存在的隐私担忧日益增加的时代，我们建议，出现在内容中的决定权应该只属于身体的所有者。尽管已经提出了一些全身匿名化的自动方法，但人为引导的匿名化可以适应各种情况，例如文化规范、个人关系、审美问题和安全问题。&#39;&#39;我的身体我的选择&#39;&#39;（MBMC）通过针对四个任务的移除和交换方法实现物理和对抗性匿名化，这些方法由单个或多个 ControlNet 或 GAN 模块设计，结合了多个扩散模型。我们在七个数据集上评估匿名化；与 SOTA 修复和匿名化方法进行比较；通过图像、对抗和生成指标进行评估；并进行重新识别实验。]]></description>
      <guid>https://arxiv.org/abs/2406.09553</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:12 GMT</pubDate>
    </item>
    <item>
      <title>CARLOR @ Ego4D 步骤接地挑战：用于测试时间细化的贝叶斯时间顺序先验</title>
      <link>https://arxiv.org/abs/2406.09575</link>
      <description><![CDATA[arXiv:2406.09575v1 公告类型：新
摘要：步骤接地任务的目标是根据自然语言描述定位活动的时间边界。本技术报告介绍了一种 Bayesian-VSLNet，以解决在冗长、未修剪的自我中心视频中识别此类时间段的挑战。我们的模型通过在推理过程中结合一种新颖的贝叶斯时间顺序先验，显著改进了传统模型，提高了时刻预测的准确性。此先验可调整视频中的循环和重复动作。我们的评估表明，与现有方法相比，我们的性能更优越，在 Ego4D Goal-Step 数据集上取得了最先进的结果，在 0.3 IoU 时 Top-1 的召回率为 35.18，在测试集上 Top-1 的召回率为 0.5 IoU 时为 20.48。]]></description>
      <guid>https://arxiv.org/abs/2406.09575</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:12 GMT</pubDate>
    </item>
    <item>
      <title>颜色等变网络</title>
      <link>https://arxiv.org/abs/2406.09588</link>
      <description><![CDATA[arXiv:2406.09588v1 公告类型：新
摘要：组等变卷积神经网络已被设计用于各种几何变换，从二维和三维旋转组到半组（如比例）。尽管这些架构提供了更好的可解释性、准确性和可推广性，但组等变网络在色调和饱和度等感知量方面的应用有限，即使它们的变化会导致分类性能显著下降。在本文中，我们引入了设计为与色调和饱和度变化等变的卷积神经网络。为了实现这一点，我们利用了色调和饱和度变换可以分别与二维旋转和一维平移组识别的观察结果。我们的色调、饱和度和全颜色等变网络实现了与这些感知变换的等变，而无需增加网络参数。我们展示了我们的网络在颜色和光照变化很常见的合成和现实世界数据集上的实用性。]]></description>
      <guid>https://arxiv.org/abs/2406.09588</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:12 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型安全吗？梯度引导扩散模型导致严重数据泄露</title>
      <link>https://arxiv.org/abs/2406.09484</link>
      <description><![CDATA[arXiv:2406.09484v1 公告类型：新
摘要：梯度泄漏已被确定为现代图像处理系统中隐私泄露的潜在来源，攻击者可以从泄漏的梯度中完全重建训练图像。然而，现有的方法仅限于重建低分辨率图像，而图像处理系统的数据泄漏风险尚未得到充分探索。在本文中，通过利用扩散模型，我们提出了一种创新的梯度引导微调方法，并介绍了一种新的重建攻击，该攻击能够通过泄漏的梯度从图像处理系统中窃取隐私的高分辨率图像，而严重的数据泄漏会遇到这种攻击。我们的攻击方法易于实施，几乎不需要先验知识。实验结果表明，目前的重建攻击只能窃取分辨率高达 $128 \times 128$ 像素的图像，而我们的攻击方法可以成功恢复和窃取分辨率高达 $512 \times 512$ 像素的图像。我们的攻击方法在像素级精度和图像重建时间效率方面都明显优于 SOTA 攻击基线。此外，我们的攻击可以在一定程度上使差分隐私失效。]]></description>
      <guid>https://arxiv.org/abs/2406.09484</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:11 GMT</pubDate>
    </item>
    <item>
      <title>SeMOPO：从低质量离线视觉数据集中学习高质量模型和策略</title>
      <link>https://arxiv.org/abs/2406.09486</link>
      <description><![CDATA[arXiv:2406.09486v1 公告类型：新
摘要：基于模型的离线强化学习 (RL) 是一种很有前途的方法，它在许多实际应用中有效地利用现有数据，尤其是那些涉及图像和视频等高维输入的应用。为了缓解离线 RL 中的分布偏移问题，现有的基于模型的方法严重依赖于学习动态的不确定性。然而，当观察结果包含具有非平凡动态的复杂干扰项时，模型不确定性估计会出现明显偏差。为了应对这一挑战，我们提出了一种新方法 - \emph{基于分离模型的离线策略优化} (SeMOPO) - 通过保守采样将潜在状态分解为内生和外生部分，并仅估计内生状态的模型不确定性。我们为 SeMOPO 的模型不确定性和性能界限提供了理论保证。为了评估其有效性，我们构建了用于 RL 的低质量视觉深度数据驱动数据集 (LQV-D4RL)，其中数据由非专家策略收集，观察结果包括移动干扰项。实验结果表明，我们的方法大大优于所有基线方法，进一步的分析实验验证了我们方法中的关键设计。该项目网站是 \href{https://sites.google.com/view/semopo}{https://sites.google.com/view/semopo}。]]></description>
      <guid>https://arxiv.org/abs/2406.09486</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:11 GMT</pubDate>
    </item>
    <item>
      <title>语言驱动的抓握检测</title>
      <link>https://arxiv.org/abs/2406.09489</link>
      <description><![CDATA[arXiv:2406.09489v1 公告类型：新
摘要：抓握检测是各种工业应用中的持久而复杂的挑战。最近，已经提出了许多方法和数据集来解决抓握检测问题。然而，他们中的大多数都没有考虑使用自然语言作为检测抓握姿势的条件。在本文中，我们介绍了 Grasp-Anything++，这是一个新的语言驱动的抓握检测数据集，具有 1M 个样本、超过 3M 个对象和超过 10M 个抓握指令。我们利用基础模型创建一个带有相应图像和抓握提示的大规模场景语料库。我们将语言驱动的抓握检测任务作为条件生成问题来处理。借鉴扩散模型在生成任务中的成功，并考虑到语言在此任务中起着至关重要的作用，我们提出了一种基于扩散模型的新型语言驱动的抓握检测方法。我们的主要贡献是对比训练目标，它明确地有助于去噪过程，以便在给定语言指令的情况下检测抓握姿势。我们说明我们的方法在理论上是支持的。密集的实验表明，我们的方法优于最先进的方法，并允许现实世界的机器人抓取。最后，我们证明我们的大规模数据集能够实现零短抓取检测，并且是未来工作的具有挑战性的基准。项目网站：https://airvlab.github.io/grasp-anything/]]></description>
      <guid>https://arxiv.org/abs/2406.09489</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:11 GMT</pubDate>
    </item>
    <item>
      <title>Pandora：面向具有自然语言动作和视频状态的通用世界模型</title>
      <link>https://arxiv.org/abs/2406.09455</link>
      <description><![CDATA[arXiv:2406.09455v1 公告类型：新
摘要：世界模型模拟世界对不同动作的未来状态。它们促进了交互式内容的创建，并为扎实的长期推理奠定了基础。当前的基础模型不能完全满足一般世界模型的能力：大型语言模型 (LLM) 受到对语言模态的依赖和对物理世界的有限理解的限制，而视频模型缺乏对世界模拟的交互式动作控制。本文通过介绍 Pandora 向构建通用世界模型迈出了一步，Pandora 是一种混合自回归扩散模型，它通过生成视频来模拟世界状态，并允许使用自由文本操作进行实时控制。Pandora 通过大规模预训练和指令调整实现了领域通用性、视频一致性和可控性。至关重要的是，Pandora 通过集成预训练的 LLM (7B) 和预训练的视频模型，避免了从头开始训练的成本，只需要额外的轻量级微调。我们展示了 Pandora 在不同领域（室内/室外、自然/城市、人类/机器人、2D/3D 等）的大量输出。结果表明，通过更大规模的训练构建更强大的通用世界模型具有巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.09455</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:10 GMT</pubDate>
    </item>
    <item>
      <title>更新 CLIP，使其更倾向于使用描述而非标题</title>
      <link>https://arxiv.org/abs/2406.09458</link>
      <description><![CDATA[arXiv:2406.09458v1 公告类型：新
摘要：尽管 CLIPScore 是一种强大的通用度量标准，可以捕捉文本和图像之间的相似性，但它无法区分用于补充图像信息的标题和用于完全替换图像的描述（例如，为了可访问性）。我们通过使用 Concadia 数据集更新 CLIP 模型来解决这一缺点，使用参数有效的微调和从因果可解释性工作中得出的损失目标，为描述分配比标题更高的分数。该模型与盲人和视力低下人士的判断相关，同时保留了传输能力，并且具有可解释的结构，可以阐明标题-描述的区别。]]></description>
      <guid>https://arxiv.org/abs/2406.09458</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:10 GMT</pubDate>
    </item>
    <item>
      <title>SViTT-Ego：用于自我中心视频的稀疏视频文本转换器</title>
      <link>https://arxiv.org/abs/2406.09462</link>
      <description><![CDATA[arXiv:2406.09462v1 公告类型：新
摘要：预训练以自我为中心的视觉语言模型对于改进下游以自我为中心的视频文本任务至关重要。这些以自我为中心的基础模型通常使用转换器架构。这些模型在预训练期间的内存占用可能很大。因此，我们对 SViTT-Ego 进行了预训练，这是第一个集成边缘和节点稀疏化的稀疏以自我为中心的视频文本转换器模型。我们在 EgoClip 数据集上进行预训练，并结合以自我为中心的目标 EgoNCE，而不是经常使用的 InfoNCE。最值得注意的是，与 LAVILA large 相比，SViTT-Ego 在 EgoMCQ（视频内）准确率上获得了 +2.8% 的增益，除了标准图像增强之外没有其他数据增强技术，但可以在内存有限的设备上进行预训练。]]></description>
      <guid>https://arxiv.org/abs/2406.09462</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:10 GMT</pubDate>
    </item>
    <item>
      <title>ELF-UA：凝视估计中的高效无标签用户自适应</title>
      <link>https://arxiv.org/abs/2406.09481</link>
      <description><![CDATA[arXiv:2406.09481v1 公告类型：新
摘要：我们考虑用户自适应 3D 凝视估计的问题。由于人际解剖学差异，与人无关的凝视估计的性能受到限制。我们的目标是提供专门针对目标用户的个性化凝视估计模型。以前关于用户自适应凝视估计的工作需要一些目标人物数据的标记图像来在测试时对模型进行微调。然而，这在实际应用中可能是不切实际的，因为最终用户提供标记图像很麻烦。此外，以前的工作要求训练数据同时具有凝视标签和人员 ID。这种数据要求使得使用一些可用数据变得不可行。为了应对这些挑战，本文提出了一个称为凝视估计中高效无标签用户自适应的新问题。我们的模型只需要目标用户的一些未标记图像即可进行模型自适应。在离线训练期间，我们有一些没有人员 ID 的标记源数据和一些未标记的人员特定数据。我们提出的方法使用元学习方法来学习如何仅使用少量未标记图像来适应新用户。我们的关键技术创新是使用来自领域适应的泛化边界来定义元学习中的损失函数，以便我们的方法可以在训练期间有效地利用标记的源数据和未标记的人员特定数据。大量实验在几个具有挑战性的基准上验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.09481</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 模型和迁移学习推进道路标志检测</title>
      <link>https://arxiv.org/abs/2406.09437</link>
      <description><![CDATA[arXiv:2406.09437v1 公告类型：新
摘要：道路标志检测和识别是高级驾驶辅助系统 (ADAS) 中必不可少的元素。几种人工智能方法已被广泛使用，其中包括 YOLOv5 和 YOLOv8。在本文中，我们使用改进的 YOLOv5 和 YOLOv8 来检测和分类不同照明条件下的不同道路标志。实验结果表明，对于 YOLOv8 模型，改变 epoch 数和批次大小可获得一致的 MAP50 分数，在测试集上范围从 94.6% 到 97.1%。YOLOv5 模型表现出竞争力，MAP50 分数范围从 92.4% 到 96.9%。这些结果表明，这两种模型在不同的训练设置中都表现良好，YOLOv8 通常会获得略高的 MAP50 分数。这些发现表明，这两种模型在不同的训练设置下都能表现良好，为寻求可靠且适应性强的物体检测应用解决方案的从业者提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2406.09437</guid>
      <pubDate>Tue, 18 Jun 2024 03:17:09 GMT</pubDate>
    </item>
    </channel>
</rss>