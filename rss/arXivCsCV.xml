<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 02 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有深度学习和自然语言功能的辅助图像注释系统：综述</title>
      <link>https://arxiv.org/abs/2407.00252</link>
      <description><![CDATA[arXiv:2407.00252v1 公告类型：新
摘要：虽然监督学习在计算机视觉任务中取得了重大成功，但获取高质量的注释数据仍然是一个瓶颈。本文探讨了人工智能辅助深度学习图像注释系统中的学术和非学术工作，这些系统为注释者提供输入图像的文本建议、标题或描述。这可能会提高注释效率和质量。我们的探索涵盖了一系列计算机视觉任务的注释，包括图像分类、对象检测、回归、实例、语义分割和姿势估计。我们回顾了各种数据集以及它们如何有助于人工智能辅助注释系统的训练和评估。我们还研究了利用神经符号学习、深度主动学习和自监督学习算法的方法，这些方法可以实现语义图像理解并生成自由文本输出。这些包括图像字幕、视觉问答和多模态推理。尽管潜力巨大，但具有文本输出功能的 AI 辅助图像注释的公开研究有限。最后，我们提出了推动这一领域的未来研究方向，强调需要更多公开的数据集以及学术界和业界之间的合作努力。]]></description>
      <guid>https://arxiv.org/abs/2407.00252</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:57 GMT</pubDate>
    </item>
    <item>
      <title>学习乳腺超声病变检测的临床相关概念瓶颈</title>
      <link>https://arxiv.org/abs/2407.00267</link>
      <description><![CDATA[arXiv:2407.00267v1 公告类型：新
摘要：检测和分类乳腺超声图像中的病变是人工智能 (AI) 的一个有前途的应用，可用于减轻乳腺造影受限地区的癌症负担。如果可以向放射科医生解释它们的预测，此类 AI 系统更有可能在临床环境中发挥作用。这项工作提出了一个可解释的 AI 模型，该模型使用美国放射学院乳腺成像和报告数据系统 (BI-RADS) 的标准词典提供可解释的预测。该模型是一个深度神经网络，具有概念瓶颈层，其中已知的 BI-RADS 特征在进行最终癌症分类之前被预测。这使放射科医生能够轻松查看 AI 系统的预测，并通过修改概念预测来实时修复错误。在实验中，基于来自 994 名女性的 8,854 张图像开发了一个模型，这些图像具有专家注释和组织学癌症标签。该模型在保留测试集上的平均准确率为 48.9，优于最先进的病变检测框架，对于癌症分类，概念干预可将受试者工作特征曲线下面积从 0.876 提高到 0.885。训练和评估代码可在 https://github.com/hawaii-ai/bus-cbm 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.00267</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:57 GMT</pubDate>
    </item>
    <item>
      <title>注意差距：使用基于 Transformer 的转录分析缺失</title>
      <link>https://arxiv.org/abs/2407.00250</link>
      <description><![CDATA[arXiv:2407.00250v1 公告类型：新
摘要：历史文献经常遭受损坏和不一致，包括由于孔洞、墨水问题和存储损坏等问题导致的文本缺失或难以辨认。这些缺失的部分或间隙称为空白。在本研究中，我们采用基于变压器的光学字符识别 (OCR) 模型，该模型以监督方式对包含空白的合成数据进行训练。我们证明了它们在检测和恢复空白方面的有效性，成功率为 65%，而缺乏空白知识的基础模型只能实现 5% 的恢复。此外，我们研究了该模型的机械特性，例如转录的对数概率，它可以识别线图像中的空白和其他错误（例如，由于复杂的书写或墨水问题导致的错误转录），而无需直接检查图像。对于寻求区分包含空白或错误​​的图像和干净图像的学者来说，此功能可能很有价值。尽管我们探索了注意力机制在标记缺失和转录错误方面的潜力，但我们的研究结果表明它不是一个重要因素。我们的工作突出了利用基于转换器的 OCR 模型来恢复或分析损坏的历史文档的一个有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2407.00250</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:56 GMT</pubDate>
    </item>
    <item>
      <title>在沉浸式可穿戴设备上部署基于 CNN 的计算机视觉模型的方法</title>
      <link>https://arxiv.org/abs/2407.00233</link>
      <description><![CDATA[arXiv:2407.00233v1 公告类型：新
摘要：卷积神经网络 (CNN) 模型通常缺乏整合人类输入的能力，而增强现实 (AR) 头戴设备可以解决这个问题。然而，目前的 AR 头戴设备在处理能力方面存在限制，这阻碍了研究人员使用 AR 头戴设备中的 CNN 执行实时、复杂的图像识别任务。本文提出了一种在 AR 头戴设备上部署 CNN 模型的方法，即在计算机上训练它们并将优化后的权重矩阵传输到头戴设备。该方法将图像数据和 CNN 层转换为适合 AR 平台的一维格式。我们通过使用 PyTorch 在 MNIST 数据集上训练 LeNet-5 CNN 模型并将其部署在 HoloLens AR 头戴设备上来演示该方法。结果表明，该模型保持了约 98% 的准确率，与其在计算机上的性能相似。 CNN 与 AR 的融合使得 AR 头戴设备能够进行实时图像处理，从而将人类输入纳入 AI 模型。]]></description>
      <guid>https://arxiv.org/abs/2407.00233</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:55 GMT</pubDate>
    </item>
    <item>
      <title>使用图像透视进行快速细化以实现文本到图像的生成</title>
      <link>https://arxiv.org/abs/2407.00247</link>
      <description><![CDATA[arXiv:2407.00247v1 公告类型：新
摘要：对于文本到图像的生成，自动将用户提供的自然语言提示细化为系统喜欢的富含关键字的提示对于用户体验至关重要。这种提示细化过程类似于将提示从“用户语言”翻译成“系统语言”。然而，这种平行语料库的稀缺使得训练提示细化模型变得困难。受零样本机器翻译技术的启发，我们引入了使用图像枢轴的提示细化 (PRIP)。PRIP 创新地使用用户首选图像的潜在表示作为用户和系统语言之间的中间“枢轴”。它将细化过程分解为两个数据丰富的任务：从用户语言推断用户首选图像的表示，然后将图像表示翻译成系统语言。因此，它可以利用丰富的数据进行训练。大量实验表明，PRIP 的表现远远优于各种基线，并且能够以零样本方式有效地转移到未知系统。]]></description>
      <guid>https://arxiv.org/abs/2407.00247</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:55 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformer 的图像和视频修复：当前挑战和未来方向</title>
      <link>https://arxiv.org/abs/2407.00226</link>
      <description><![CDATA[arXiv:2407.00226v1 公告类型：新
摘要：图像修复目前是计算机视觉领域的热门话题。它为各种应用提供了可行的解决方案，包括照片修复、视频编辑和医学成像。深度学习的进步，尤其是卷积神经网络 (CNN) 和生成对抗网络 (GAN)，通过结合上下文相关的细节，显著增强了修复任务，提高了填充图像或视频中缺失或损坏区域的能力。这些进步还改善了其他方面，包括效率、信息保存以及实现逼真的纹理和结构。最近，视觉变换器已被利用并为图像或视频修复提供了一些改进。基于变换器的架构的出现最初是为自然语言处理而设计的，也已被集成到计算机视觉任务中。这些方法利用自注意力机制，擅长捕捉数据中的长距离依赖关系；因此，它们对于需要全面了解图像或视频全局上下文的任务特别有效。在本文中，我们全面回顾了当前的图像或视频修复方法，特别关注基于变换器的技术，目的是突出重大改进并为使用视觉变换器进行图像或视频修复领域的新研究人员提供指导。我们根据基于变换器的技术的架构配置、损坏类型和性能指标对其进行了分类。此外，我们还对当前的挑战进行了有组织的综合，并提出了图像或视频修复领域未来研究的方向。]]></description>
      <guid>https://arxiv.org/abs/2407.00226</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:54 GMT</pubDate>
    </item>
    <item>
      <title>SemUV：基于深度学习的虚拟人头部 UV 纹理图语义操作</title>
      <link>https://arxiv.org/abs/2407.00229</link>
      <description><![CDATA[arXiv:2407.00229v1 公告类型：新
摘要：设计和操作虚拟人头对于各种应用都至关重要，包括 AR、VR、游戏、人机交互和 VFX。传统的基于图形的方法需要人工和资源才能准确呈现人头。虽然现代深度学习技术可以生成和编辑高度逼真的面部图像，但它们的重点仍然主要放在 2D 面部图像上。这种限制使它们不太适合 3D 应用。认识到在 UV 纹理空间内编辑作为 3D 图形管道中的关键组件的重要作用，我们的工作重点是这一方面，通过提供增强的外观操作控制和精度，使图形设计师受益。对 UV 纹理空间内现有方法的研究有限、复杂且具有挑战性。在本文中，我们介绍了 SemUV：一种使用 FFHQ-UV 数据集直接在 UV 纹理空间内进行语义操作的简单有效的方法。我们在公开可用的 FFHQ-UV 数据集上训练 StyleGAN 模型，随后训练边界以进行插值和语义特征操作。通过将我们的方法与 2D 操作技术进行比较的实验，我们证明了其在有效修改年龄、性别和面部毛发等语义特征的同时保留身份的卓越能力。我们的方法很简单，与其他 3D 组件（例如结构、照明和渲染）无关，并且能够无缝集成到标准 3D 图形管道中，而无需大量的领域专业知识、时间或资源。]]></description>
      <guid>https://arxiv.org/abs/2407.00229</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:54 GMT</pubDate>
    </item>
    <item>
      <title>PathGen-1.6M：通过多代理协作生成 160 万个病理图像-文本对</title>
      <link>https://arxiv.org/abs/2407.00203</link>
      <description><![CDATA[arXiv:2407.00203v1 公告类型：新
摘要：像 CLIP 这样的视觉语言模型 (VLM) 在病理学领域引起了广泛关注，成为零样本图像分类和全幻灯片图像 (WSI) 分析等应用的骨干。此外，当与大型语言模型 (LLM) 结合使用时，它们可以用作视觉编码器，以支持更广泛的功能。当前训练病理学 VLM 的努力依赖于来自 PubMed、YouTube 和 Twitter 等平台的病理学图像文本对，这些平台提供的数据有限、不可扩展且图像质量通常不理想。在这项工作中，我们利用 TCGA 等大规模 WSI 数据集来提取大量高质量图像块。然后，我们训练一个大型多模态模型来为这些图像生成字幕，创建 PathGen-1.6M，这是一个包含 160 万个高质量图像字幕对的数据集。我们的方法涉及多个代理模型协作提取代表性的 WSI 补丁，生成和细化字幕以获得高质量的图像文本对。大量实验表明，将这些生成的对与现有数据集整合在一起，以训练病理学特定的 CLIP 模型 PathGen-CLIP，可显著增强其分析病理图像的能力，在九项病理学相关的零样本图像分类任务和三项全切片图像任务中取得了显著的进步。此外，我们基于 PathGen-1.6M 构建了 200K 指令调整数据，并将 PathGen-CLIP 与 Vicuna LLM 集成，通过指令调整创建更强大的多模态模型。总体而言，我们为病理学中的高质量数据生成提供了一条可扩展的途径，为下一代通用病理学模型铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2407.00203</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:53 GMT</pubDate>
    </item>
    <item>
      <title>用于癌症生存预测的多模式原型设计</title>
      <link>https://arxiv.org/abs/2407.00224</link>
      <description><![CDATA[arXiv:2407.00224v1 公告类型：新
摘要：结合千兆像素组织学全切片图像 (WSI) 和转录组谱的多模态生存方法对于患者预测和分层特别有前景。当前的方法包括将 WSI 标记为较小的块（&gt;10,000 个块）并将转录组标记为基因组，然后使用 Transformer 将其集成以预测结果。但是，此过程会生成许多标记，这会导致计算注意力所需的内存要求很高，并使事后可解释性分析变得复杂。相反，我们假设我们可以：(1) 通过使用形态原型压缩其构成标记来有效地总结 WSI 的形态内容，实现超过 300 倍的压缩；(2) 通过使用生物通路原型对转录组谱进行编码来准确表征细胞功能，所有这些都以无监督的方式进行。随后，融合网络会使用 Transformer 或最佳传输交叉比对来处理生成的多模态标记，该网络现在使用少量且固定数量的标记进行操作，无需近似。对六种癌症类型的广泛评估表明，我们的框架以更少的计算量超越了最先进的方法，同时解锁了新的可解释性分析。]]></description>
      <guid>https://arxiv.org/abs/2407.00224</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:53 GMT</pubDate>
    </item>
    <item>
      <title>通过多尺度分数匹配分析定位异常</title>
      <link>https://arxiv.org/abs/2407.00148</link>
      <description><![CDATA[arXiv:2407.00148v1 公告类型：新
摘要：医学成像中的异常检测和定位仍然是医疗保健领域的关键挑战。本文介绍了 Spatial-MSMA（多尺度分数匹配分析），这是一种用于体积脑 MRI 中异常定位的新型无监督方法。在 MSMA 框架的基础上，我们的方法结合了空间信息和条件可能性来增强异常检测能力。我们采用一种灵活的正则化流模型，该模型以斑块位置和全局图像特征为条件来估计斑块异常分数。该方法在 1,650 个 T1 和 T2 加权脑 MRI 数据集上进行评估，这些 MRI 来自正常发育的儿童，并在测试集中添加了模拟病变。在病变检测和分割任务中，Spatial-MSMA 明显优于现有方法，包括基于重建、基于生成和基于解释的方法。我们的模型在基于距离的指标（第 99 百分位豪斯多夫距离：$7.05 \pm 0.61$，平均表面距离：$2.10 \pm 0.43$）和组件指标（真阳性率：$0.83 \pm 0.01$，阳性预测值：$0.96 \pm 0.01$）方面均取得了优异的表现。这些结果证明了 Spatial-MSMA 在医学成像中准确且可解释的异常定位方面的潜力，这对临床环境中改善诊断和治疗计划具有重要意义。我们的代码可在~\url{https://github.com/ahsanMah/sade/} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.00148</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:52 GMT</pubDate>
    </item>
    <item>
      <title>采用机器学习和深度学习技术的基于网络的自动化疟疾检测系统</title>
      <link>https://arxiv.org/abs/2407.00120</link>
      <description><![CDATA[arXiv:2407.00120v1 公告类型：新
摘要：疟疾寄生虫给全球带来了巨大的健康负担，造成了广泛的痛苦和死亡。准确检测疟疾感染对于有效治疗和控制至关重要。然而，现有的自动检测技术在准确性和普遍性方面表现出局限性。许多研究都集中在特定的特征上，而没有探索更全面的方法。在我们的案例中，我们使用传统的 CNN 和迁移学习模型（特别是 VGG19、InceptionV3 和 Xception）制定了一种用于疟疾感染细胞分类的深度学习技术。这些模型使用 NIH 数据集进行训练，并使用不同的性能指标（如准确率、精确率、召回率和 F1 分数）进行测试。测试结果表明，深度 CNN 的准确率最高——97%，其次是 Xception，准确率为 95%。机器学习模型 SVM 的准确率为 83%，而 Inception-V3 的准确率为 94%。此外，该系统可通过网络界面访问，用户可上传血涂片图像以进行疟疾检测。]]></description>
      <guid>https://arxiv.org/abs/2407.00120</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:51 GMT</pubDate>
    </item>
    <item>
      <title>无人机影像中的多物种物体检测用于濒危动物种群监测</title>
      <link>https://arxiv.org/abs/2407.00127</link>
      <description><![CDATA[arXiv:2407.00127v1 公告类型：新
摘要：全球动物种群正在迅速减少，能够准确计数濒危物种的技术对于监测未来几年的种群变化至关重要。这项研究的重点是微调无人机图像的物体检测模型，以准确计数动物物种。数百张使用无人机拍摄的图像和大型、公开可用的无人机图像数据集用于使用基线 YOLOv8 架构微调机器学习模型。我们训练了 30 种不同的模型，其中最大的模型有 4370 万个参数和 365 个层，并使用超参数调整和数据增强技术来提高准确性。虽然最先进的 YOLOv8 基线在野生动物数据集上的准确率只有 0.7%，但我们的模型在同一数据集上的准确率高达 95%。最后，我们在 Jetson Orin Nano 上部署了模型，以演示低功耗实时物种检测，以便在无人机上轻松推理。]]></description>
      <guid>https://arxiv.org/abs/2407.00127</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:51 GMT</pubDate>
    </item>
    <item>
      <title>支持随机访问的神经图形纹理压缩</title>
      <link>https://arxiv.org/abs/2407.00021</link>
      <description><![CDATA[arXiv:2407.00021v1 公告类型：新
摘要：渲染技术的进步导致纹理资产（包括分辨率、复杂性和新颖的纹理组件）的大幅增长，但数据量的这种增长并没有与其压缩技术的进步相匹配。与此同时，神经图像压缩 (NIC) 取得了显着进步并显示出有希望的结果，但所提出的方法不能直接适用于神经纹理压缩。首先，纹理压缩需要在并行渲染期间进行按需和实时解码并随机访问（例如，在 GPU 上进行块纹理解压缩）。此外，NIC 不支持多分辨率重建（mip 级别），也不具备有效联合压缩不同纹理通道集的能力。在这项工作中，我们介绍了一种新颖的纹理集压缩方法，该方法集成了传统的 GPU 纹理表示和 NIC 技术，旨在实现随机访问并支持多通道纹理集。为了实现这一目标，我们提出了一种非对称自动编码器框架，该框架采用卷积编码器来捕获瓶颈潜在空间中的详细信息，在解码器端，我们利用全连接网络，其输入是给定纹理坐标和 mip 级别的采样潜在特征加上位置信息。此潜在数据的定义是为了通过简单地更改扫描步幅来简化对多分辨率数据的访问。实验结果表明，这种方法比传统的纹理压缩提供了更好的结果，并且比使用神经网络的最新方法有显著的改进。]]></description>
      <guid>https://arxiv.org/abs/2407.00021</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:50 GMT</pubDate>
    </item>
    <item>
      <title>LMVD：用于野外抑郁症检测的大规模多模态 Vlog 数据集</title>
      <link>https://arxiv.org/abs/2407.00024</link>
      <description><![CDATA[arXiv:2407.00024v1 公告类型：新
摘要：抑郁症会严重影响个人生活的许多方面，包括个人和社会功能、学业和工作表现以及整体生活质量。情感计算领域的许多研究人员正在采用深度学习技术来探索与抑郁症检测相关的潜在模式。然而，由于受试者的隐私保护问题，该领域的数据仍然稀缺，这对用于检测抑郁症的深度判别模型提出了挑战。为了克服这些障碍，建立了一个用于野外抑郁症识别的大规模多模态视频博客数据集 (LMVD)。在 LMVD 中，有 1823 个样本，共 214 小时，来自四个多媒体平台（新浪微博、哔哩哔哩、抖音和 YouTube）。提出了一种称为 MDDformer 的新型架构来学习个人的非语言行为。在 LMVD 数据集上进行了广泛的验证，证明了其在抑郁症检测方面的卓越性能。我们期待 LMVD 将为抑郁症检测社区做出宝贵贡献。数据和代码将在链接上发布：https://github.com/helang818/LMVD/。]]></description>
      <guid>https://arxiv.org/abs/2407.00024</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:50 GMT</pubDate>
    </item>
    <item>
      <title>基于视觉语言模型的跨模态语义通信系统</title>
      <link>https://arxiv.org/abs/2407.00020</link>
      <description><![CDATA[arXiv:2407.00020v1 公告类型：新 
摘要：语义通信（SC）近年来已成为一种新颖的通信范式，通过创新的语义传输概念成功超越了香农物理容量限制。然而，现有的图像语义通信（ISC）系统在动态环境中面临着许多挑战，包括语义密度低、灾难性遗忘和不确定的信噪比（SNR）。为了应对这些挑战，我们提出了一种基于视觉语言模型的新型跨模态语义通信（VLM-CSC）系统。VLM-CSC 包含三个新颖的组件：（1）跨模态知识库（CKB）用于从发送端的语义稀疏图像中提取高密度文本语义，并在接收端根据文本语义重建原始图像。高密度语义的传输有助于缓解带宽压力。 （2）记忆辅助编码器和解码器（MED）采用混合长短期记忆机制，使语义编码器和解码器能够在动态环境中克服语义特征分布漂移导致的灾难性遗忘。（3）噪声注意模块（NAM）采用注意机制，根据信噪比自适应地调整语义编码和信道编码，保证CSC系统的鲁棒性。实验仿真验证了CSC系统的有效性、自适应性和鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2407.00020</guid>
      <pubDate>Tue, 02 Jul 2024 06:21:49 GMT</pubDate>
    </item>
    </channel>
</rss>