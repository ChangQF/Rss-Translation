<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 23 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>YOLO-TLA：基于YOLOv5的高效轻量小物体检测模型</title>
      <link>https://arxiv.org/abs/2402.14309</link>
      <description><![CDATA[arXiv:2402.14309v1 公告类型：新
摘要：目标检测是计算机视觉的一个重要方面，在准确性和鲁棒性方面取得了显着进步。尽管取得了这些进步，实际应用仍然面临着显着的挑战，主要是小物体检测不准确或漏检。在本文中，我们提出了 YOLO-TLA，这是一种基于 YOLOv5 的高级目标检测模型。我们首先在颈部网络金字塔架构中引入一个针对小物体的附加检测层，从而产生更大尺度的特征图来辨别小物体的更精细特征。此外，我们将 C3CrossCovn 模块集成到主干网络中。该模块采用滑动窗口特征提取，有效减少了计算需求和参数数量，使模型更加紧凑。此外，我们还在主干网络中纳入了全局注意力机制。该机制将通道信息与全局信息相结合以创建加权特征图。该特征图经过定制，可以突出显示感兴趣对象的属性，同时有效地忽略不相关的细节。与基线 YOLOv5s 模型相比，我们新开发的 YOLO-TLA 模型在 MS COCO 验证数据集上显示出相当大的改进，mAP@0.5 增加了 4.6%，mAP@0.5:0.95 增加了 4%，同时保持模型不变尺寸紧凑，参数为 9.49M。将这些改进进一步扩展到YOLOv5m模型，增强版本的mAP@0.5和mAP@0.5:0.95分别增加了1.7%和1.9%，总共有27.53M参数。这些结果验证了YOLO-TLA模型在小物体检测方面高效且有效的性能，以更少的参数和计算需求实现了高精度。]]></description>
      <guid>https://arxiv.org/abs/2402.14309</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:56 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散模型进行字体样式插值</title>
      <link>https://arxiv.org/abs/2402.14311</link>
      <description><![CDATA[arXiv:2402.14311v1 公告类型：新
摘要： 字体的风格差异很大，给读者带来不同的印象。因此，生成新的字体值得给读者带来新的印象。在本文中，我们采用扩散模型通过插入一对具有不同样式的参考字体来生成新的字体样式。更具体地说，我们提出了三种不同的插值方法，即图像混合、条件混合和噪声混合以及扩散模型。我们进行定性和定量实验分析，以了解三种方法的风格生成能力。根据实验结果，提出的三种方法不仅可以生成预期的字体样式，还可以生成一些偶然的字体样式。我们还将这些方法与最先进的风格条件拉丁字体生成网络模型进行比较，以确认使用扩散模型进行风格插值任务的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.14311</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:56 GMT</pubDate>
    </item>
    <item>
      <title>学习紧缩——最佳字母空间的集合估计</title>
      <link>https://arxiv.org/abs/2402.14313</link>
      <description><![CDATA[arXiv:2402.14313v1 公告类型：新
摘要：字距调整是为某种字体的所有可能的字母对设置适当的水平间距的任务。字距调整的困难之一是每个字母对的适当间距不同。因此，总共52个大小写字母，我们需要调整$52 \times 52 = 2704$不同的空格。另一个困难是自动字距调整既没有通用的程序也没有标准。因此，字距调整仍然是手动或启发式完成的。在本文中，我们通过提出两种机器学习模型来解决字距调整问题，称为成对模型和集合模型。前者是一个简单的深度神经网络，用于估计两个给定字母图像的字母空间。相比之下，后者是基于 Transformer 的模型，可估计三个或更多给定字母图像的字母空间。例如，set-wise 模型同时估计某种字体的 52 个字母图像的 2704 个空格。在这两个模型中，set-wise 模型不仅更高效，而且更准确，因为其内部的自注意力机制允许所有字母的字距调整更加一致。对约2500种谷歌字体的实验结果及其定量和定性分析表明，当所有字体和字母对的平均字母间距约为115像素时，set-wise模型的平均估计误差仅为约5.3像素。]]></description>
      <guid>https://arxiv.org/abs/2402.14313</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:56 GMT</pubDate>
    </item>
    <item>
      <title>地标感知视觉导航数据集</title>
      <link>https://arxiv.org/abs/2402.14281</link>
      <description><![CDATA[arXiv:2402.14281v1 公告类型：新
摘要：通过专家演示学习的地图表示已显示出良好的研究价值。然而，由于现实世界中缺乏有效的环境监督表示学习的人类数据集，视觉导航领域的最新进展面临着挑战。我们提出了一个地标感知视觉导航（LAVN）数据集，以允许对以人为中心的探索策略和地图构建进行监督学习。当人类注释者探索虚拟和现实世界环境时，我们收集 RGB 观察和人类点击对，以实现对空间的全覆盖探索。人类注释者还沿着每个轨迹提供不同的地标示例，我们直觉地认为这将简化地图或图形构建和定位的任务。当学习在环境中探索时，这些人类点击可以作为路径点预测的直接监督。我们的数据集涵盖了广泛的场景，包括室内环境中的房间以及室外的走道。数据集可从 DOI 获取：10.5281/zenodo.10608067。]]></description>
      <guid>https://arxiv.org/abs/2402.14281</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:55 GMT</pubDate>
    </item>
    <item>
      <title>将视觉上下文学习与掩模图像建模相结合以改进超声分割的简单框架</title>
      <link>https://arxiv.org/abs/2402.14300</link>
      <description><![CDATA[arXiv:2402.14300v1 公告类型：新
摘要：传统的深度学习模型对图像进行一张一张的处理，在医学影像领域需要昂贵且耗时的专家标记，并且特定领域的限制限制了模型的泛化性。视觉情境学习（ICL）是计算机视觉领域一个令人兴奋的新研究领域。与传统的深度学习不同，ICL 强调模型根据给定示例快速适应新任务的能力。受 MAE-VQGAN 的启发，我们提出了一种名为 SimICL 的新的简单视觉 ICL 方法，将视觉 ICL 配对图像与专为自监督学习而设计的掩模图像建模 (MIM) 相结合。我们在带有有限注释的腕部超声（美国）数据集中验证了我们的骨结构分割方法，其中临床目标是分割骨结构以帮助进一步的骨折检测。我们使用包含来自 18 名患者的 3822 个图像的测试集进行骨区域分割。 SimICL 实现了非常高的 Dice 系数 (DC) 0.96 和 Jaccard 指数 (IoU) 0.92，超越了最先进的分割和视觉 ICL 模型（最大 DC 0.86 和 IoU 0.76），并且 SimICL DC 和 IoU 不断增加高达 0.10 和 0.16。这种与有限的手动注释的高度一致性表明 SimICL 可以用于训练 AI 模型，甚至可以在美国的小型数据集上进行训练。与传统方法相比，这可以大大减少人类专家进行图像标记所需的时间，并增强人工智能辅助在美国图像分析中的实际使用。]]></description>
      <guid>https://arxiv.org/abs/2402.14300</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:55 GMT</pubDate>
    </item>
    <item>
      <title>Swin3D++：用于 3D 室内场景理解的有效多源预训练</title>
      <link>https://arxiv.org/abs/2402.14215</link>
      <description><![CDATA[arXiv:2402.14215v1 公告类型：新
摘要：数据多样性和丰富性对于提高自然语言处理和 2D 视觉模型的性能和泛化能力至关重要。然而，3D 视觉领域缺乏 3D 数据，并且简单地组合多个 3D 数据集来预训练 3D 主干网并不能产生显着的改进，因为不同 3D 数据集之间的领域差异阻碍了有效的特征学习。在这项工作中，我们确定了 3D 室内场景数据集之间域差异的主要来源，并提出了 Swin3D++，这是一种基于 Swin3D 的增强架构，用于对多源 3D 点云进行有效的预训练。 Swin3D++ 在 Swin3D 的模块中引入了特定领域的机制，以解决领域差异并增强网络在多源预训练上的能力。此外，我们设计了一种简单的源增强策略来增加预训练数据规模并促进监督预训练。我们验证了设计的有效性，并证明 Swin3D++ 在典型的室内场景理解任务上超越了最先进的 3D 预训练方法。我们的代码和模型将在 https://github.com/microsoft/Swin3D 发布]]></description>
      <guid>https://arxiv.org/abs/2402.14215</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:54 GMT</pubDate>
    </item>
    <item>
      <title>自监督压力图人体关键点检测方法：优化跨数据集的泛化和计算效率</title>
      <link>https://arxiv.org/abs/2402.14241</link>
      <description><![CDATA[arXiv:2402.14241v1 公告类型：新
摘要：在 RGB 图像不足的环境中，压力图是一种可行的替代方案，引起了学术界的关注。本研究引入了一种新颖的自监督压力图关键点检测（SPMKD）方法，解决了当前从压力图中提取人体关键点的专门设计中的差距。我们贡献的核心是编码器-融合器-解码器（EFD）模型，它是一个强大的框架，集成了用于精确人体关键点检测的轻量级编码器、用于高效梯度传播的融合器以及将人体关键点转换为重建压力图的解码器。这种结构通过分类到回归权重转移（CRWT）方法进一步增强，该方法通过初始分类任务训练来微调准确性。这项创新不仅在无需手动注释的情况下增强了人类关键点泛化能力，而且还展示了卓越的效率和泛化能力，与基线方法相比，FLOP 次数减少至仅 $5.96\%$，参数数量减少至 $1.11\%$。]]></description>
      <guid>https://arxiv.org/abs/2402.14241</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:54 GMT</pubDate>
    </item>
    <item>
      <title>MVD$^2$：用于多视图扩散的高效多视图 3D 重建</title>
      <link>https://arxiv.org/abs/2402.14253</link>
      <description><![CDATA[arXiv:2402.14253v1 公告类型：新
摘要：作为一种有前景的 3D 生成技术，多视图扩散（MVD）由于其在通用性、质量和效率方面的优势而受到广泛关注。通过使用 3D 数据微调预训练的大型图像扩散模型，MVD 方法首先根据图像或文本提示生成 3D 对象的多个视图，然后通过多视图 3D 重建来重建 3D 形状。然而，生成图像中的稀疏视图和不一致的细节使得 3D 重建具有挑战性。我们提出了 MVD$^2$，一种用于多视图扩散 (MVD) 图像的高效 3D 重建方法。 MVD$^2$通过投影和卷积将图像特征聚合成3D特征体积，然后将体积特征解码成3D网格。我们使用 3D 形状集合和 3D 形状渲染视图提示的 MVD 图像来训练 MVD$^2$。为了解决生成的多视图图像与 3D 形状的真实视图之间的差异，我们设计了一种简单而有效的视图相关训练方案。 MVD$^2$ 提高了 MVD 的 3D 生成质量，并且对各种 MVD 方法快速且鲁棒。训练后，它可以在一秒内有效地从多视图图像中解码 3D 网格。我们使用 Zero-123++ 和 ObjectVerse-LVIS 3D 数据集训练 MVD$^2$，并使用合成图像和真实图像作为提示，展示了其从不同 MVD 方法生成的多视图图像生成 3D 模型的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.14253</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:54 GMT</pubDate>
    </item>
    <item>
      <title>T-Stitch：通过轨迹缝合加速预训练扩散模型中的采样</title>
      <link>https://arxiv.org/abs/2402.14167</link>
      <description><![CDATA[arXiv:2402.14167v1 公告类型：新
摘要：对于高质量图像生成来说，从扩散概率模型 (DPM) 中采样通常成本高昂，并且大型模型通常需要许多步骤。在本文中，我们介绍了采样轨迹缝合 T-Stitch，这是一种简单而有效的技术，可以提高采样效率，而几乎没有生成退化。 T-Stitch 不是在整个采样轨迹中单独使用较大的 DPM，而是首先在初始步骤中利用较小的 DPM 作为较大 DPM 的廉价替代品，并在稍后阶段切换到较大的 DPM。我们的主要见解是，不同的扩散模型在相同的训练数据分布下学习相似的编码，并且较小的模型能够在早期步骤中生成良好的全局结构。大量实验表明，T-Stitch 无需训练，通常适用于不同的架构，并通过灵活的速度和质量权衡补充了大多数现有的快速采样技术。例如，在 DiT-XL 上，40% 的早期时间步可以安全地替换为速度快 10 倍的 DiT-S，而不会降低类条件 ImageNet 生成的性能。我们进一步表明，我们的方法还可以用作一种直接技术，不仅可以加速流行的预训练稳定扩散（SD）模型，还可以提高公共模型动物园中风格化 SD 模型的即时对齐。代码发布于 https://github.com/NVlabs/T-Stitch]]></description>
      <guid>https://arxiv.org/abs/2402.14167</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:53 GMT</pubDate>
    </item>
    <item>
      <title>提示：具有掩模感知编码和增强注意力的高质量 INPainting Transformer</title>
      <link>https://arxiv.org/abs/2402.14185</link>
      <description><![CDATA[arXiv:2402.14185v1 公告类型：新
摘要：现有的图像修复方法利用基于卷积的下采样方法来减少空间维度。这可能会导致损坏图像中的信息丢失，其中可用信息本质上是稀疏的，特别是对于大面积缺失区域的情况。 Transformer 内自注意力机制的最新进展导致了包括修复在内的许多计算机视觉任务的显着改进。然而，受计算成本的限制，现有方法无法充分利用此类模型的远程建模能力的功效。在本文中，我们提出了一种端到端的高质量 INpainting Transformer，缩写为 HINT，它由一种新颖的掩模感知像素洗牌下采样模块（MPD）组成，用于保留从损坏图像中提取的可见信息，同时保持可用于模型内进行高级推论的信息的完整性。此外，我们提出了空间激活通道注意层（SCAL），这是一种有效的自注意机制，解释空间意识以在多个尺度上对损坏的图像进行建模。为了进一步提高 SCAL 的有效性，受语音识别领域最新进展的推动，我们引入了一种三明治结构，将前馈网络放置在 SCAL 模块之前和之后。我们在 CelebA、CelebA-HQ、Places2 和 Dunhuang 四个数据集上展示了 HINT 与当代最先进模型相比的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.14185</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:53 GMT</pubDate>
    </item>
    <item>
      <title>Mip-Grid：神经辐射场的抗锯齿网格表示</title>
      <link>https://arxiv.org/abs/2402.14196</link>
      <description><![CDATA[arXiv:2402.14196v1 公告类型：新
摘要：尽管神经辐射场（NeRF）在表示 3D 场景和生成新颖的视图图像方面取得了显着的成就，但大多数现有方法仍然没有解决在不同相机距离渲染“锯齿”或“模糊”图像的混叠问题。最近提出的 mip-NeRF 通过渲染截头圆锥体而不是射线解决了这一挑战。然而，它依赖 MLP 架构来表示辐射场，错过了最新的基于网格的方法所提供的快速训练速度。在这项工作中，我们提出了 mip-Grid，这是一种新颖的方法，它将抗锯齿技术集成到基于网格的辐射场表示中，在享受快速训练时间的同时减轻锯齿伪影。所提出的方法通过在共享网格表示上应用简单的卷积运算来生成多尺度网格，并使用尺度感知坐标从生成的多尺度网格中检索不同尺度的特征。为了测试有效性，我们将所提出的方法集成到最近两种代表性的基于网格的方法 TensoRF 和 K-Planes 中。实验结果表明，mip-Grid 极大地提高了两种方法的渲染性能，甚至在多尺度数据集上优于 mip-NeRF，同时显着缩短了训练时间。代码和演示视频请参见https://stnamjef.github.io/mipgrid.github.io/。]]></description>
      <guid>https://arxiv.org/abs/2402.14196</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:53 GMT</pubDate>
    </item>
    <item>
      <title>SecurePose：从临床环境中录制的视频中自动进行面部模糊和人体运动运动学提取</title>
      <link>https://arxiv.org/abs/2402.14143</link>
      <description><![CDATA[arXiv:2402.14143v1 公告类型：新
摘要：运动障碍通常是通过对临床获取的患者视频进行基于共识的专家评估来诊断的。然而，如此广泛地共享患者视频会给患者隐私带来风险。人脸模糊可用于对视频进行去识别化，但此过程通常是手动且耗时的。现有的自动面部模糊技术可能会出现面部模糊过度、不一致或不足的情况，所有这些对于视频评估和患者隐私来说都可能是灾难性的。此外，评估这些视频中的运动障碍通常是主观的。提取可量化的运动学特征可以帮助评估这些视频中的运动障碍，但如果使用预先模糊的视频，现有的方法很容易出错。我们开发了一款名为 SecurePose 的开源软件，该软件可以在使用 iPad 在诊所环境中录制的患者视频中实现可靠的面部模糊和自动运动提取。 SecurePose，使用姿势估计方法（OpenPose）提取运动学，跟踪并唯一识别视频中的所有个体，识别患者并执行面部模糊。该软件在 116 名脑瘫儿童门诊就诊时记录的步态视频上进行了验证。验证涉及评估运动学提取和手动模糊（地面实况）面部模糊的中间步骤。此外，当 SecurePose 与六种选定的现有方法进行比较时，它在自动人脸检测方面优于其他方法，并且比强大的手动人脸模糊方法节省了 91.08% 的时间，实现了最高准确度。此外，十名经验丰富的研究人员发现 SecurePose 易于学习和使用，系统可用性量表证明了这一点。这项工作的结果验证了 SecurePose 在临床记录的步态视频上用于面部模糊和运动学提取的性能和可用性。]]></description>
      <guid>https://arxiv.org/abs/2402.14143</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:52 GMT</pubDate>
    </item>
    <item>
      <title>用于医学影像分析的大型视觉语言模型：实证研究</title>
      <link>https://arxiv.org/abs/2402.14162</link>
      <description><![CDATA[arXiv:2402.14162v1 公告类型：新
摘要：最近，大语言模型（LLM）在自然语言处理领域引起了人们的关注。此外，将法学硕士与视觉相结合，使用户能够利用多模式数据探索新兴能力。视觉语言模型 (VLM)，例如 LLaVA、Flamingo 或 CLIP，在各种视觉语言任务中表现出了令人印象深刻的性能。因此，大型模型在生物医学成像领域有巨大的应用前景。沿着这个方向，缺乏相关工作来展示大型模型诊断疾病的能力。在这项工作中，我们研究了 VLM 在医学成像分析任务中的零样本和少样本鲁棒性。我们的综合实验证明了 VLM 在分析生物医学图像（例如脑部 MRI、血细胞显微图像和胸部 X 射线）方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.14162</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:52 GMT</pubDate>
    </item>
    <item>
      <title>乳腺病灶分割的多器官自监督对比学习</title>
      <link>https://arxiv.org/abs/2402.14114</link>
      <description><![CDATA[arXiv:2402.14114v1 公告类型：新
摘要：自监督学习已被证明是在注释标签稀缺的领域（例如医学成像）学习表示的有效方法。为此目的广泛采用的框架是对比学习，它已应用于不同的场景。本文旨在通过探索一种新颖的视角来增进我们对对比学习框架的理解：利用多器官数据集来针对特定器官相关目标任务定制预训练模型。更具体地说，我们的目标任务是超声图像中的乳腺肿瘤分割。预训练数据集包括来自其他器官（例如肺和心脏）的超声图像以及自然图像的大型数据集。我们的结果表明，与监督基线方法相比，传统的对比学习预训练可以提高性能。此外，我们的预训练模型在仅使用一半的可用标记数据进行微调时即可实现可比的性能。我们的研究结果还显示了对不同器官数据进行预训练对于提高下游任务性能的优势。]]></description>
      <guid>https://arxiv.org/abs/2402.14114</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:51 GMT</pubDate>
    </item>
    <item>
      <title>跨架构的零样本泛化用于视觉分类</title>
      <link>https://arxiv.org/abs/2402.14095</link>
      <description><![CDATA[arXiv:2402.14095v1 公告类型：新
摘要：对未见过的数据的泛化是深度网络的一个关键需求，但其与分类准确性的关系尚不清楚。使用极简视觉数据集和通用性度量，我们表明，从深度卷积网络 (CNN) 到 Transformer，流行的网络在跨层和跨架构外推到不可见类的能力上各不相同。准确性并不能很好地预测泛化能力，而且泛化能力随层深度非单调变化。代码可在 https://github.com/dyballa/zero-shot-generalization 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.14095</guid>
      <pubDate>Fri, 23 Feb 2024 15:14:50 GMT</pubDate>
    </item>
    </channel>
</rss>