<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 06 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过隐式面部关键点编辑实现可控的说话面孔生成</title>
      <link>https://arxiv.org/abs/2406.02880</link>
      <description><![CDATA[arXiv:2406.02880v1 公告类型：新
摘要：音频驱动的说话脸部生成在数字人类研究领域引起了极大兴趣。现有的方法受到复杂的模型架构的阻碍，这些架构错综复杂地相互依赖，使重新编辑图像或视频输入的过程变得复杂。在这项工作中，我们提出了 ControlTalk，一种基于驱动音频控制面部表情变形的说话脸部生成方法，它可以以统一的方式为单个图像或连续视频输入构建头部姿势和面部表情（包括嘴唇运动）。通过利用预先训练的视频合成渲染器并提出轻量级自适应，ControlTalk 实现了精确而自然的唇部同步，同时能够定量控制张口形状。我们的实验表明，我们的方法在广泛使用的基准测试（包括 HDTF 和 MEAD）上优于最先进的性能。参数化适应性表现出了显著的泛化能力，能有效处理跨同ID和跨ID场景的表情变形，并将其实用性扩展到域外肖像，不受语言限制。]]></description>
      <guid>https://arxiv.org/abs/2406.02880</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>DenoDet：注意力机制作为可变形多子空间特征去噪方法，用于 SAR 图像中的目标检测</title>
      <link>https://arxiv.org/abs/2406.02833</link>
      <description><![CDATA[arXiv:2406.02833v1 公告类型：新
摘要：合成孔径雷达 (SAR) 目标检测长期以来一直受到固有斑点噪声和小而模糊目标普遍存在的阻碍。虽然深度神经网络已经推进了 SAR 目标检测，但它们固有的低频偏差和静态训练后权重会因相干噪声和在异质地形上保留细微细节而失效。受传统 SAR 图像去噪的启发，我们提出了 DenoDet，这是一个借助显式频域变换来校准卷积偏差并更加关注高频的网络，形成自然的多尺度子空间表示，从多子空间去噪的角度检测目标。我们设计了 TransDeno，这是一个动态频域注意模块，它充当变换域软阈值操作，通过保留显着的目标信号和衰减噪声来跨子空间动态去噪。为了自适应地调整子空间处理的粒度，我们还提出了一个可变形组全连接层 (DeGroFC)，该层可根据输入特征动态改变组。我们的即插即用 TransDeno 无需任何花哨的配置，即可在多个 SAR 目标检测数据集上取得最佳成绩。代码可在 https://github.com/GrokCV/GrokSAR 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.02833</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散特征的递归归一化切割进行零样本图像分割</title>
      <link>https://arxiv.org/abs/2406.02842</link>
      <description><![CDATA[arXiv:2406.02842v1 公告类型：新
摘要：基础模型已成为语言、视觉和多模态任务等各个领域的强大工具。虽然先前的研究已经解决了无监督图像分割问题，但它们明显落后于监督模型。在本文中，我们使用扩散 UNet 编码器作为基础视觉编码器，并引入 DiffCut，这是一种无监督零样本分割方法，它仅利用最终自注意力块的输出特征。通过大量实验，我们证明在基于图的分割算法中使用这些扩散特征，其性能明显优于以前最先进的零样本分割方法。具体而言，我们利用递归正则化切割算法，该算法可以软调节检测到的对象的粒度并生成定义明确的分割图，从而精确捕捉复杂的图像细节。我们的工作突出了扩散 UNet 编码器中嵌入的非常准确的语义知识，这些知识随后可以作为下游任务的基础视觉编码器。项目页面：https://diffcut-segmentation.github.io]]></description>
      <guid>https://arxiv.org/abs/2406.02842</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>重新思考指导信息以利用未标记样本：标签编码视角</title>
      <link>https://arxiv.org/abs/2406.02862</link>
      <description><![CDATA[arXiv:2406.02862v1 公告类型：新
摘要：经验风险最小化（ERM）在标记样本不足的情况下很脆弱。ERM 对未标记样本的一个普通扩展是熵最小化（EntMin），它使用未标记样本的软标签来指导它们的学习。然而，EntMin 强调预测可辨别性而忽略预测多样性。为了缓解这个问题，在本文中，我们重新考虑了利用未标记样本的指导信息。通过分析 ERM 的学习目标，我们发现特定类别中标记样本的指导信息是相应的标签编码。受此发现的启发，我们提出了一种标签编码风险最小化（LERM）。它首先通过未标记样本的预测平均值估计标签编码，然后将它们与相应的真实标签编码对齐。因此，LERM 既确保了预测可辨别性，又确保了多样性，并且可以作为插件集成到现有方法中。理论上，我们分析了 LERM 与 ERM 以及 EntMin 之间的关系。实证上，我们在几种标签不足的场景下验证了 LERM 的优越性。代码可从 https://github.com/zhangyl660/LERM 获取。]]></description>
      <guid>https://arxiv.org/abs/2406.02862</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>ORACLE：利用相互信息在扩散模型中使用 LoRA 实现一致的字符生成</title>
      <link>https://arxiv.org/abs/2406.02820</link>
      <description><![CDATA[arXiv:2406.02820v1 公告类型：新
摘要：文本到图像的扩散模型最近已成为促进漫画艺术、儿童文学、游戏开发和网页设计等一系列领域视觉创造力的关键工具。这些模型利用人工智能的力量将文本描述转换为生动的图像，从而使艺术家和创作者能够以前所未有的轻松方式将他们的富有想象力的概念变为现实。然而，仍然存在的重大障碍之一是在不同环境中保持角色生成的一致性的挑战。文本提示的变化，即使是微小的变化，也会产生截然不同的视觉输出，这对需要在整个过程中统一字符表示的项目构成了相当大的问题。在本文中，我们介绍了一个新颖的框架，旨在从单个文本提示在不同设置中生成一致的字符表示。通过定量和定性分析，我们证明了我们的框架在生成具有一致视觉特征的角色方面优于现有方法，凸显了其改变创意产业的潜力。通过解决角色一致性的关键挑战，我们不仅增强了这些模型的实用性，而且还拓宽了艺术和创造性表达的视野。]]></description>
      <guid>https://arxiv.org/abs/2406.02820</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:03 GMT</pubDate>
    </item>
    <item>
      <title>提炼聚合知识以实现弱监督视频异常检测</title>
      <link>https://arxiv.org/abs/2406.02831</link>
      <description><![CDATA[arXiv:2406.02831v1 公告类型：新
摘要：视频异常检测旨在开发能够识别监控视频中异常事件的自动化模型。此任务的基准设置极具挑战性，因为：i）训练集的大小有限，ii）视频级标签方面的监督薄弱，以及 iii）异常事件稀缺导致的内在类别不平衡。在这项工作中，我们表明，将知识从多个主干的聚合表示中提炼成一个相对简单的模型可以实现最先进的性能。特别是，我们开发了一种双层蒸馏方法以及一种新颖的基于解缠结交叉注意的特征聚合网络。我们提出的方法 DAKD（使用解缠结注意力提炼聚合知识）与现有方法相比，在多个基准数据集上表现出卓越的性能。值得注意的是，我们在 UCF-Crime、ShanghaiTech 和 XD-Violence 数据集上分别取得了 1.36%、0.78% 和 7.02% 的显著提升。]]></description>
      <guid>https://arxiv.org/abs/2406.02831</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:03 GMT</pubDate>
    </item>
    <item>
      <title>适用于多模态任务的多层可学习注意力掩码</title>
      <link>https://arxiv.org/abs/2406.02761</link>
      <description><![CDATA[arXiv:2406.02761v1 公告类型：新
摘要：虽然 Transformer 模型中的自注意力机制已被证明在许多领域都是有效的，但我们观察到，由于每个 token 的粒度不同，长序列的计算要求高，它在更多样化的设置（例如多模态）中效果较差。为了应对挑战，我们引入了可学习注意力掩码 (LAM)，其战略设计用于全局调节注意力图并优先考虑序列中的关键 token。利用类似 BERT 的 Transformer 网络中的自注意力模块，我们的方法可以熟练地捕获 token 之间的关联。将 LAM 扩展到多层版本可容纳 Transformer 网络每一层嵌入的各种信息方面。对各种数据集（例如 MADv2、QVHighlights、ImageNet 1K 和 MSRVTT）的全面实验验证证明了 LAM 的有效性，体现了其在提高模型性能的同时减少冗余计算的能力。这种开创性的方法在增强对复杂场景的理解（例如电影理解）方面取得了重大进步。]]></description>
      <guid>https://arxiv.org/abs/2406.02761</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:02 GMT</pubDate>
    </item>
    <item>
      <title>针对半监督注视跟踪的扩散精炼 VQA 注释</title>
      <link>https://arxiv.org/abs/2406.02774</link>
      <description><![CDATA[arXiv:2406.02774v1 公告类型：新
摘要：训练凝视跟踪模型需要大量由人工注释者注释的凝视目标坐标图像，这是一个费力且本质上含糊不清的过程。我们提出了第一种半监督的凝视跟踪方法，为该任务引入了两个新的先验。我们使用一个大型预训练的视觉问答 (VQA) 模型获得第一个先验，其中我们通过用凝视跟踪问题“提示” VQA 模型来计算 Grad-CAM 热图。这些热图可能很嘈杂，不适合用于训练。需要改进这些嘈杂的注释，这导致我们加入第二个先验。我们利用在有限的人工注释上训练的扩散模型，并修改反向采样过程来改进 Grad-CAM 热图。通过调整扩散过程，我们在人工注释先验和 VQA 热图先验之间实现了权衡，这保留了有用的 VQA 先验信息，同时表现出与训练数据分布相似的属性。我们的方法在 GazeFollow 图像数据集上的表现优于简单的伪注释生成基线。更重要的是，我们的伪注释策略应用于广泛使用的监督注视跟踪模型 (VAT)，将注释需求减少了 50%。我们的方法在 VideoAttentionTarget 数据集上的表现也最好。]]></description>
      <guid>https://arxiv.org/abs/2406.02774</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:02 GMT</pubDate>
    </item>
    <item>
      <title>MeshVPR：使用 3D 网格进行城市范围的视觉地点识别</title>
      <link>https://arxiv.org/abs/2406.02776</link>
      <description><![CDATA[arXiv:2406.02776v1 公告类型：新
摘要：基于网格的场景表示为简化大规模分层视觉定位管道提供了一个有希望的方向，结合了基于全局特征（检索）的视觉位置识别步骤和基于局部特征的视觉定位步骤。虽然现有工作证明了网格用于视觉定位的可行性，但在视觉位置识别中使用由它们渲染的合成数据库的影响仍然很大程度上尚未探索。在这项工作中，我们研究使用密集的 3D 纹理网格进行大规模视觉位置识别 (VPR)，并发现与使用真实世界图像进行检索相比，使用基于网格的合成数据库时性能会显着下降。为了解决这个问题，我们提出了 MeshVPR，这是一种新颖的 VPR 管道，它利用轻量级特征对齐框架来弥合现实世界和合成域之间的差距。MeshVPR 利用预先训练的 VPR 模型，并且它对于城市范围的部署是高效且可扩展的。我们引入了新的数据集，其中包含来自柏林、巴黎和墨尔本的免费 3D 网格和手动收集的查询。广泛的评估表明，MeshVPR 在使用标准 VPR 管道时实现了具有竞争力的性能，为基于网格的定位系统铺平了道路。我们的贡献包括城市范围内基于网格的 VPR 的新任务、新的基准数据集 MeshVPR 以及对开放挑战的全面分析。数据、代码和交互式可视化可在 https://mesh-vpr.github.io 上找到]]></description>
      <guid>https://arxiv.org/abs/2406.02776</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:02 GMT</pubDate>
    </item>
    <item>
      <title>LADI v2：低空灾害图像的多标签数据集和分类器</title>
      <link>https://arxiv.org/abs/2406.02780</link>
      <description><![CDATA[arXiv:2406.02780v1 公告类型：新
摘要：基于 ML 的计算机视觉模型是支持自然灾害后应急管理行动的有前途的工具。从小型载人和无人机拍摄的航拍照片可以在灾难发生后很快获得，并从多个角度为态势感知和损害评估应用提供有价值的信息。然而，应急管理人员经常面临挑战，即在事件发生后可能拍摄的数万张照片中找到最相关的照片。虽然基于 ML 的解决方案可以更有效地使用航拍照片，但仍然缺乏从多个角度和多种危险类型拍摄的此类图像的训练数据。为了解决这个问题，我们提出了 LADI v2（低空灾害图像版本 2）数据集，这是一组精选的约 10,000 张灾难图像，由民航巡逻队 (CAP) 在美国为应对联邦宣布的紧急情况（2015-2023 年）而拍摄，并由经过培训的 CAP 志愿者注释以进行多标签分类。我们还提供了两个预训练的基线分类器，并将其性能与多标签分类中最先进的视觉语言模型进行比较。数据和代码已公开发布，以支持开发用于应急管理研究和应用的计算机视觉模型。]]></description>
      <guid>https://arxiv.org/abs/2406.02780</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:02 GMT</pubDate>
    </item>
    <item>
      <title>3D-HGS：3D 半高斯溅射</title>
      <link>https://arxiv.org/abs/2406.02720</link>
      <description><![CDATA[arXiv:2406.02720v1 公告类型：新
摘要：照片级逼真的 3D 重建是 3D 计算机视觉中的一个基本问题。由于最近的神经渲染技术的出现，该领域取得了长足的进步。这些技术主要旨在专注于学习 3D 场景的体积表示，并通过从渲染中得出的损失函数细化这些表示。其中，3D 高斯溅射 (3D-GS) 已成为一种重要的方法，超越了神经辐射场 (NeRF)。3D-GS 使用参数化的 3D 高斯来建模空间位置和颜色信息，并结合基于图块的快速渲染技术。尽管 3D 高斯核具有出色的渲染性能和速度，但它在准确表示不连续函数方面存在固有的局限性，尤其是在形状不连续的边缘和角落，以及不同纹理的颜色不连续性。为了解决这个问题，我们建议采用 3D 半高斯 (3D-HGS) 内核，它可以用作即插即用的内核。我们的实验证明了它们能够提高当前 3D-GS 相关方法的性能，并在不影响渲染速度的情况下在各种数据集上实现最先进的渲染性能。]]></description>
      <guid>https://arxiv.org/abs/2406.02720</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:01 GMT</pubDate>
    </item>
    <item>
      <title>通过视觉输入生成故事：技术、相关任务和挑战</title>
      <link>https://arxiv.org/abs/2406.02748</link>
      <description><![CDATA[arXiv:2406.02748v1 公告类型：新
摘要：从视觉数据中创建引人入胜的叙述对于自动数字媒体消费、辅助技术和交互式娱乐至关重要。本调查涵盖了生成这些叙述所使用的方法，重点关注它们的原理、优势和局限性。
调查还涵盖了与自动故事生成相关的任务，例如图像和视频字幕、视觉问答以及没有视觉输入的故事生成。这些任务与视觉故事生成面临共同的挑战，并为该领域使用的技术提供了灵感。我们分析了主要数据集和评估指标，从批判性的角度审视了它们的局限性。]]></description>
      <guid>https://arxiv.org/abs/2406.02748</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:01 GMT</pubDate>
    </item>
    <item>
      <title>ShadowRefiner：通过快速傅里叶变换实现无遮罩阴影去除</title>
      <link>https://arxiv.org/abs/2406.02559</link>
      <description><![CDATA[arXiv:2406.02559v1 公告类型：新
摘要：受阴影影响的图像通常会在颜色和照明方面表现出明显的空间差异，从而降低各种视觉应用（包括物体检测和分割系统）的性能。为了有效消除现实世界图像中的阴影，同时保留复杂的细节并产生视觉上引人注目的结果，我们通过快速傅里叶变换引入了无掩模阴影去除和细化网络 (ShadowRefiner)。具体而言，我们方法中的阴影去除模块旨在通过空间和频率表示学习在受阴影影响的图像和无阴影图像之间建立有效的映射。为了减轻像素错位并进一步提高图像质量，我们提出了一种新颖的基于快速傅里叶注意力的变换器 (FFAT) 架构，其中设计了一种创新的注意力机制来进行细致的细化。我们的方法在感知赛道中赢得了冠军，并在 NTIRE 2024 图像阴影去除挑战赛的保真度赛道中取得了第二好的表现。此外，综合实验结果也证明了我们提出的方法的有效性。代码已公开：https://github.com/movingforward100/Shadow_R。]]></description>
      <guid>https://arxiv.org/abs/2406.02559</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:00 GMT</pubDate>
    </item>
    <item>
      <title>对比语言视频时间预训练</title>
      <link>https://arxiv.org/abs/2406.02631</link>
      <description><![CDATA[arXiv:2406.02631v1 公告类型：新
摘要：我们介绍了 LAVITI，这是一种通过对比学习来学习长视频中的语言、视频和时间表示的新方法。与 EgoVLP 等视频文本对的预训练不同，LAVITI 旨在通过提取未修剪视频中有意义的时刻来对齐语言、视频和时间特征。我们的模型采用一组可学习的时刻查询来解码剪辑级视觉、语言和时间特征。除了视觉和语言对齐之外，我们还引入了相对时间嵌入 (TE) 来表示视频中的时间戳，从而实现时间的对比学习。与传统方法显着不同，特定时间戳的预测是通过计算预测的 TE 与所有 TE 之间的相似性得分来转换的。此外，现有的视频理解方法主要针对短视频，因为计算复杂度高且内存占用大。我们的方法一天内仅需 8 个 NVIDIA RTX-3090 GPU 即可在 Ego4D 数据集上进行训练。我们在 CharadesEgo 动作识别上验证了我们的方法，取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2406.02631</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 SegFormer 检测窗墙比</title>
      <link>https://arxiv.org/abs/2406.02706</link>
      <description><![CDATA[arXiv:2406.02706v1 公告类型：新
摘要：窗墙比 (WWR) 是评估建筑物能源、日光和通风性能的关键。研究表明，窗户面积对建筑性能和模拟有很大影响。然而，建立这些环境模型和模拟的数据通常是不可用的。相反，通常假设所有建筑物的 WWR 都是标准的 40%。本文利用现有的计算机视觉窗口检测方法，使用语义分割从外部街景图像预测建筑物的 WWR，展示了在建筑应用中采用成熟的计算机视觉技术的潜力]]></description>
      <guid>https://arxiv.org/abs/2406.02706</guid>
      <pubDate>Fri, 07 Jun 2024 03:17:00 GMT</pubDate>
    </item>
    </channel>
</rss>