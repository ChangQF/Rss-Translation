<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 19 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>TextCenGen：用于文本到图像生成的注意力引导的以文本为中心的背景适应</title>
      <link>https://arxiv.org/abs/2404.11824</link>
      <description><![CDATA[arXiv:2404.11824v1 公告类型：新
摘要：文本到图像（T2I）生成的最新进展见证了从使文本适应固定背景到围绕文本创建图像的转变。传统方法通常仅限于在静态图像中生成布局以实现有效的文本放置。我们提出的方法 TextCenGen 引入了空白区域的动态适应，以生成文本友好的图像，强调以文本为中心的设计和视觉和谐的生成。我们的方法在 T2I 模型中采用强制注意力引导来生成图像，策略性地为预定义文本区域保留空白，甚至对于黄金比例的文本或图标也是如此。观察交叉注意力图如何影响对象放置，我们使用力导向图方法检测并排斥冲突对象，并结合空间排除交叉注意力约束以实现空白区域的平滑注意力。作为图形设计中的一项新颖任务，实验表明 TextCenGen 的表现优于现有方法，构图更加和谐。此外，我们的方法显着增强了我们专门收集的提示数据集上的 T2I 模型结果，适应不同的文本位置。这些结果证明了 TextCenGen 在创建更加和谐和集成的文本图像组合方面的功效。]]></description>
      <guid>https://arxiv.org/abs/2404.11824</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>基于样本的医学图像分割的跨模型相互学习</title>
      <link>https://arxiv.org/abs/2404.11812</link>
      <description><![CDATA[arXiv:2404.11812v1 公告类型：新
摘要：医学图像分割通常需要大量密集注释来进行模型训练，这既耗时又需要大量技能。为了减轻这一负担，引入了基于样本的医学图像分割方法，仅用一张带注释的图像即可实现有效的训练。在本文中，我们介绍了一种新颖的基于示例的医学图像分割（CMEMS）的跨模型相互学习框架，该框架利用两个模型从多个粒度的未标记数据中相互挖掘隐含信息。 CMEMS 可以消除确认偏差，并通过在模型之间的不同粒度上强制执行一致性，使协作训练能够学习补充信息。具体来说，基于跨模型图像扰动的互学习是通过使用弱扰动图像生成高置信度伪标签来设计的，从而监督跨模型强扰动图像的预测。这种方法能够共同追求图像粒度上的预测一致性。此外，基于跨模型多级特征扰动的相互学习是通过让伪标签监督不同分辨率的扰动多级特征的预测来设计的，这可以拓宽扰动空间并增强我们框架的鲁棒性。 CMEMS 使用示例数据、合成数据和未标记数据以端到端的方式进行联合训练。两个医学图像数据集的实验结果表明，所提出的 CMEMS 在监督极其有限的情况下优于最先进的分割方法。]]></description>
      <guid>https://arxiv.org/abs/2404.11812</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>利用对抗性示例来减轻偏差和提高准确性</title>
      <link>https://arxiv.org/abs/2404.11819</link>
      <description><![CDATA[arXiv:2404.11819v1 公告类型：新
摘要：我们提出了一种通过利用反事实生成和微调来减轻计算机视觉模型中的偏差的新方法。虽然反事实已被用来分析和解决 DNN 模型中的偏差，但反事实本身通常是由有偏差的生成模型生成的，这可能会引入额外的偏差或虚假相关性。为了解决这个问题，我们建议使用对抗性图像，即欺骗深度神经网络而不是人类的图像，作为公平模型训练的反事实。
  我们的方法利用课程学习框架与细粒度的对抗性损失相结合，使用对抗性示例来微调模型。通过将对抗性图像合并到训练数据中，我们的目标是防止偏差通过管道传播。我们通过定性和定量评估来验证我们的方法，证明与现有方法相比，偏差缓解和准确性得到了改善。定性地，我们的结果表明，训练后，模型做出的决策对敏感属性的依赖性较小，并且我们的模型更好地理清了敏感属性和分类变量之间的关系。]]></description>
      <guid>https://arxiv.org/abs/2404.11819</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>基础模型何时有效？了解使用多光谱图像进行像素级分类的适用性</title>
      <link>https://arxiv.org/abs/2404.11797</link>
      <description><![CDATA[arXiv:2404.11797v1 公告类型：新
摘要：基础模型，即非常大的深度学习模型，在各种语言和视觉任务中表现出了令人印象深刻的性能，而使用较小尺寸的模型很难实现这些任务。 GPT 型语言模型的重大成功尤其令人兴奋，并提高了人们对卫星遥感等其他领域基础模型潜力的期望。在此背景下，人们付出了巨大的努力来构建基础模型，以测试其在更广泛应用中的能力，例子包括 NASA-IBM 的 Prithvi、Segment-Anything-Model、ViT 等。这引出了一个重要的问题：基础模型是否是基础模型？总是适合不同遥感任务的选择，何时或何时不？这项工作旨在通过与传统机器学习（ML）和常规尺寸深度学习模型的比较，增强对使用中等分辨率多光谱图像进行像素级分类的基础模型的现状和适用性的理解。有趣的是，结果表明，在许多场景中，与基础模型相比，传统机器学习模型仍然具有相似或更好的性能，特别是对于纹理对分类不太有用的任务。另一方面，对于标签部分依赖于纹理（例如烧伤疤痕）的任务，深度学习模型确实显示出了更有希望的结果，而基础模型和深度学习模型之间的性能差异并不明显。结果符合我们的分析：基础模型的适用性取决于自监督学习任务与实际下游任务之间的一致性，典型的屏蔽自动编码器范式不一定适合许多遥感问题。]]></description>
      <guid>https://arxiv.org/abs/2404.11797</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>建立 VR 中注视驱动的身份验证性能基线：对非常大的数据集进行广度优先研究</title>
      <link>https://arxiv.org/abs/2404.11798</link>
      <description><![CDATA[arXiv:2404.11798v1 公告类型：新
摘要：本文开展了关键工作，为注视驱动的身份验证性能建立基线，以开始使用来自 9202 人的注视记录的大型数据集来回答基本研究问题，其眼动追踪 (ET) 信号质量水平相当于现代消费者的水平-面向虚拟现实（VR）平台。所使用的数据集的大小至少比以前相关工作中的任何其他数据集大一个数量级。我们的模型需要对眼睛的光轴和视轴进行双眼估计以及注册和验证的最短持续时间，以在五万分之一的错误接受率 (FAR) 下实现低于 3% 的错误拒绝率 (FRR)。就识别准确度随画廊大小而降低而言，我们估计当画廊大小为 148,000 或更大时，我们的模型将低于机会级别准确度。我们的主要发现表明，在最先进的机器学习架构和足够大的训练数据集的驱动下，凝视身份验证可以达到 FIDO 标准要求的准确度。]]></description>
      <guid>https://arxiv.org/abs/2404.11798</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>TempBEV：通过组合图像和 BEV 空间时间聚合改进学习 BEV 编码器</title>
      <link>https://arxiv.org/abs/2404.11803</link>
      <description><![CDATA[arXiv:2404.11803v1 公告类型：新
摘要：自动驾驶需要准确地表示环境。实现高精度的策略是融合来自多个传感器的数据。学习鸟瞰 (BEV) 编码器可以通过将来自各个传感器的数据映射到一个联合潜在空间来实现这一目标。对于经济高效的纯摄像头系统，这提供了一种有效的机制来融合来自具有不同视图的多个摄像头的数据。通过随着时间的推移聚合传感器信息可以进一步提高准确性。这在单目相机系统中尤其重要，因为它可以解决缺乏明确的深度和速度测量的问题。因此，开发的 BEV 编码器的有效性主要取决于用于聚合时间信息的算子以及所使用的潜在表示空间。我们分析了文献中提出的 BEV 编码器并比较了它们的有效性，量化了聚合算子和潜在表示的影响。虽然大多数现有方法聚合图像或 BEV 潜在空间中的时间信息，但我们的分析和性能比较表明这些潜在表示表现出互补的优势。因此，我们开发了一种新颖的时间 BEV 编码器 TempBEV，它集成了来自两个潜在空间的聚合时间信息。我们将后续图像帧视为随时间变化的立体图像，并利用光流估计方法进行时间立体编码。对 NuScenes 数据集的实证评估表明，TempBEV 相对于 3D 对象检测和 BEV 分割的基线有显着改进。消融揭示了图像和 BEV 潜在空间中联合时间聚合的强大协同作用。这些结果表明了我们方法的整体有效性，并为聚合图像和 BEV 潜在空间中的时间信息提供了强有力的理由。]]></description>
      <guid>https://arxiv.org/abs/2404.11803</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>看不见的领域的多模态 3D 物体检测</title>
      <link>https://arxiv.org/abs/2404.11764</link>
      <description><![CDATA[arXiv:2404.11764v1 公告类型：新
摘要：自动驾驶 LiDAR 数据集在点云密度、范围和物体尺寸等属性上存在偏差。因此，在不同环境中训练和评估的目标检测网络经常会出现性能下降的情况。领域适应方法假设可以访问测试分布中的未注释样本来解决此问题。然而，在现实世界中，训练时可能无法获得部署和访问代表测试数据集的样本的确切条件。我们认为，更现实和更具挑战性的表述是要求对看不见的目标领域的性能具有鲁棒性。我们建议从两个方面来解决这个问题。首先，我们利用大多数自动驾驶数据集中存在的配对激光雷达图像数据来执行多模式物体检测。我们建议，通过利用图像和 LiDAR 点云进行场景理解任务来处理多模态特征，可以使目标检测器对看不见的域变化更加鲁棒。其次，我们训练 3D 对象检测器来学习不同分布的多模态对象特征，并促进这些源域之间的特征不变性，以提高对未见过的目标域的泛化能力。为此，我们提出了 CLIX$^\text{3D}$，一种用于 3D 对象检测的多模态融合和监督对比学习框架，该框架对来自不同域的同类样本的对象特征进行对齐，同时将来自不同类的特征分开。我们证明 CLIX$^\text{3D}$ 在多个数据集转换下产生最先进的域泛化性能。]]></description>
      <guid>https://arxiv.org/abs/2404.11764</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>基于事件的眼动追踪。 AIS 2024 挑战调查</title>
      <link>https://arxiv.org/abs/2404.11770</link>
      <description><![CDATA[arXiv:2404.11770v1 公告类型：新
摘要：本调查回顾了 AIS 2024 基于事件的眼动追踪 (EET) 挑战赛。挑战的任务重点是处理事件摄像机记录的眼球运动并预测眼睛的瞳孔中心。该挑战强调使用事件摄像机进行有效的眼动追踪，以实现良好的任务准确性和效率权衡。挑战期间，共有 38 名参与者报名参加 Kaggle 竞赛，并有 8 个团队提交了挑战情况说明书。本次调查对提交的情况说明书中新颖且多样化的方法进行了回顾和分析，以推进未来基于事件的眼动追踪研究。]]></description>
      <guid>https://arxiv.org/abs/2404.11770</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>CU-Mamba：具有通道学习的选择性状态空间模型用于图像恢复</title>
      <link>https://arxiv.org/abs/2404.11778</link>
      <description><![CDATA[arXiv:2404.11778v1 公告类型：新
摘要：重建退化图像是图像处理中的一项关键任务。尽管基于 CNN 和 Transformer 的模型在该领域很普遍，但它们表现出固有的局限性，例如远程依赖建模不足和计算成本较高。为了克服这些问题，我们引入了通道感知 U 形 Mamba (CU-Mamba) 模型，它将双状态空间模型 (SSM) 框架合并到 U-Net 架构中。 CU-Mamba 采用空间 SSM 模块进行全局上下文编码，并采用通道 SSM 组件来保留通道相关特征，两者的计算复杂度均与特征图大小呈线性关系。大量的实验结果验证了 CU-Mamba 相对于现有最先进方法的优越性，强调了在图像恢复中整合空间和通道上下文的重要性。]]></description>
      <guid>https://arxiv.org/abs/2404.11778</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>广义少镜头分割的视觉提示：一种多尺度方法</title>
      <link>https://arxiv.org/abs/2404.11732</link>
      <description><![CDATA[arXiv:2404.11732v1 公告类型：新
摘要：基于注意力的 Transformer 模型的出现，由于其卓越的泛化和传输特性，使其在各种任务中得到了广泛的应用。最近的研究表明，如果得到适当的提示，此类模型非常适合进行少样本推理。然而，对于语义分割等密集预测任务，此类技术尚未得到充分探索。在这项工作中，我们检查了使用学习到的视觉提示来提示变压器解码器以完成广义少镜头分割（GFSS）任务的有效性。我们的目标是不仅在示例有限的新类别上实现强劲的性能，而且在基本类别上保持性能。我们提出了一种通过有限的例子来学习视觉提示的方法。这些学习到的视觉提示用于提示多尺度转换器解码器以促进准确的密集预测。此外，我们在通过有限示例学习的新颖提示和通过大量数据学习的基本提示之间引入了单向因果注意机制。这种机制丰富了新颖的提示，而不会降低基类的性能。总的来说，这种形式的提示有助于我们在两个不同的基准数据集上实现 GFSS 最先进的性能：COCO-$20^i$ 和 Pascal-$5^i$，而不需要测试时优化（或转换） ）。此外，利用未标记的测试数据的测试时优化可用于改进提示，我们将其称为传导提示调整。]]></description>
      <guid>https://arxiv.org/abs/2404.11732</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>LiDAR 物体检测的等变时空自监督</title>
      <link>https://arxiv.org/abs/2404.11737</link>
      <description><![CDATA[arXiv:2404.11737v1 公告类型：新
摘要：流行的表示学习方法鼓励输入变换下的特征不变性。然而，在对象定位和分割等 3D 感知任务中，输出自然与某些变换（例如旋转）等效。使用在某些变换下鼓励特征等变的预训练损失函数提供了强大的自监督信号，同时还保留了变换后的特征表示之间的几何关系信息。这可以提高与此类转换等效的下游任务的性能。在本文中，我们通过联合考虑空间和时间增强，提出了时空等变学习框架。我们的实验表明，最好的性能来自于鼓励平移、缩放、翻转、旋转和场景流等变的预训练方法。对于空间增强，我们发现根据变换，对比目标或分类等方差目标会产生最佳结果。为了利用现实世界的物体变形和运动，我们考虑了连续的 LiDAR 场景对，并开发了一种新颖的基于 3D 场景流的等方差目标，从而提高了整体性能。我们展示了用于 3D 对象检测的预训练方法，该方法在许多设置中优于现有的等变和不变方法。]]></description>
      <guid>https://arxiv.org/abs/2404.11737</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>IrrNet：通过遥感图像增量斑块大小训练推进灌溉绘图</title>
      <link>https://arxiv.org/abs/2404.11762</link>
      <description><![CDATA[arXiv:2404.11762v1 公告类型：新
摘要：灌溉测绘在有效的水资源管理中发挥着至关重要的作用，对于保持水质和水量至关重要，也是缓解全球水资源短缺问题的关键。农田的复杂性以及各种灌溉方式，尤其是当多个系统近距离共存时，构成了独特的挑战。 Landsat 遥感数据的性质进一步加剧了这种复杂性，其中每个像素都富含密集的信息，从而使精确灌溉绘图的任务变得更加复杂。在本研究中，我们介绍了一种采用渐进式训练方法的创新方法，该方法在整个训练过程中战略性地增加补丁大小，利用来自 Landsat 5 和 7 的数据集，并使用 WRLU 数据集进行精确标记。这种最初的焦点允许模型捕获详细的特征，随着补丁大小的扩大，逐渐转向更广泛、更一般的特征。值得注意的是，我们的方法将现有最先进模型的性能提高了大约 20%。此外，我们的分析深入探讨了将各种光谱带纳入模型的重要性，评估它们对性能的影响。研究结果表明，额外的波段有助于模型更有效地识别更精细的细节。这项工作为在灌溉测绘中利用遥感图像设定了新标准。]]></description>
      <guid>https://arxiv.org/abs/2404.11762</guid>
      <pubDate>Fri, 19 Apr 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>SNP：结构化神经元级修剪以保持注意力分数</title>
      <link>https://arxiv.org/abs/2404.11630</link>
      <description><![CDATA[arXiv:2404.11630v1 公告类型：新
摘要：多头自注意力（MSA）是视觉变换器（ViTs）的关键组成部分，在各种视觉任务中取得了巨大的成功。然而，它们的高计算成本和内存占用阻碍了它们在资源受限的设备上的部署。传统的剪枝方法只能使用头部剪枝来压缩和加速 MSA 模块，尽管头部不是原子单元。为了解决这个问题，我们提出了一种新颖的图感知神经元级剪枝方法，即结构化神经元级剪枝（SNP）。 SNP 修剪注意力分数信息较少的神经元，并消除头部之间的冗余。具体来说，它会修剪图形连接的查询和具有最少信息注意力分数的关键层，同时保留总体注意力分数。可以独立修剪的值层被修剪以消除头间冗余。我们提出的方法有效地压缩和加速了边缘设备和服务器处理器的基于 Transformer 的模型。例如，带有 SNP 的 DeiT-Small 的运行速度比原始模型快 3.1$\times$，性能比 DeiT-Tiny 快 21.94\%，高 1.12\%。此外，SNP 与传统的头部或块修剪方法成功结合。具有头部修剪的 SNP 可以将 DeiT-Base 压缩 80% 的参数和计算成本，并在 RTX3090 上实现 3.85$\times$ 的推理速度，在 Jetson Nano 上实现 4.93$\times$ 的推理速度。]]></description>
      <guid>https://arxiv.org/abs/2404.11630</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>用于快速稀疏输入动态视图合成的分解运动场</title>
      <link>https://arxiv.org/abs/2404.11669</link>
      <description><![CDATA[arXiv:2404.11669v1 公告类型：新
摘要：设计动态场景的 3D 表示以实现快速优化和渲染是一项具有挑战性的任务。虽然最近的显式表示能够快速学习和渲染动态辐射场，但它们需要一组密集的输入视点。在这项工作中，我们专注于学习具有稀疏输入视点的动态辐射场的快速表示。然而，稀疏输入的优化受到约束不足，需要使用运动先验来约束学习。现有的快速动态场景模型没有明确地对运动进行建模，这使得它们很难受到运动先验的约束。我们设计了一个显式运动模型作为分解的 4D 表示，该模型速度快并且可以利用运动场的时空相关性。然后，我们引入可靠的流先验，包括跨摄像机的稀疏流先验和摄像机内的密集流先验的组合，以规范我们的运动模型。我们的模型快速、紧凑，并且在具有稀疏输入视点的流行多视图动态场景数据集上实现了非常好的性能。我们模型的源代码可以在我们的项目页面上找到：https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html。]]></description>
      <guid>https://arxiv.org/abs/2404.11669</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>基于视频的气管插管技能深度学习评估</title>
      <link>https://arxiv.org/abs/2404.11727</link>
      <description><![CDATA[arXiv:2404.11727v1 公告类型：新
摘要：气管插管（ETI）是在民用和战斗伤员护理环境中进行的一种紧急程序，用于建立气道。 ETI 技能的客观和自动化评估对于医疗保健提供者的培训和认证至关重要。然而，目前的方法是基于专家的手动反馈，这是主观的、耗时和资源密集的，并且容易出现评估者间可靠性差和光环效应。这项工作提出了一个使用单视图和多视图视频评估 ETI 技能的框架。该框架由两个阶段组成。首先，二维卷积自动编码器 (AE) 和预训练的自监督网络从视频中提取特征。其次，使用交叉视图注意模块增强的一维卷积将 AE 的特征作为输入和输出预测以进行技能评估。 ETI 数据集分两个阶段收集。在第一阶段，ETI 由两个受试者群体执行：专家和新手。在第二阶段，新手受试者在时间压力下进行ETI，结果要么成功，要么不成功。还分析了针对专家和新手的单个头戴式摄像机的第三个视频数据集。该研究在初始阶段识别专家/新手试验的准确性达到了 100%。在第二阶段，该模型在成功/不成功程序分类方面显示出 85% 的准确度。仅使用头戴式摄像头，该模型在专家和新手分类方面的准确率达到 96%，同时在成功和不成功分类方面保持 85% 的准确率。此外，还提供了 GradCAM 来解释专家和新手行为以及成功和不成功试验之间的差异。该方法为自动评估 ETI 技能提供了可靠且客观的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.11727</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:59 GMT</pubDate>
    </item>
    </channel>
</rss>