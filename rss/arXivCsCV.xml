<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 26 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>深度 RAW 图像超分辨率。 NTIRE 2024 年挑战调查</title>
      <link>https://arxiv.org/abs/2404.16223</link>
      <description><![CDATA[arXiv:2404.16223v1 公告类型：新
摘要：本文回顾了 NTIRE 2024 RAW 图像超分辨率挑战赛，重点介绍了所提出的解决方案和结果。 RAW 超分辨率的新方法在现代图像信号处理 (ISP) 管道中可能至关重要，但是，这个问题并未像 RGB 领域那样得到探讨。本次挑战的目标是将 RAW Bayer 图像放大 2 倍，同时考虑到噪声和模糊等未知退化。本次挑战赛共有 230 名参赛者报名，并在挑战期间提交了 45 名参赛者的成绩。此处对前 5 名提交的性能进行了审查并提供，作为当前 RAW 图像超分辨率最先进水平的衡量标准。]]></description>
      <guid>https://arxiv.org/abs/2404.16223</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:18 GMT</pubDate>
    </item>
    <item>
      <title>用于实时语义分割的多目标优化基准测试套件</title>
      <link>https://arxiv.org/abs/2404.16266</link>
      <description><![CDATA[arXiv:2404.16266v1 公告类型：新
摘要：作为自动化机器学习中的新兴挑战之一，硬件感知神经架构搜索（HW-NAS）任务可以被视为黑盒多目标优化问题（MOP）。 HW-NAS的一个重要应用是实时语义分割，在自动驾驶场景中发挥着举足轻重的作用。用于实时语义分割的 HW-NAS 本质上需要平衡多个优化目标，包括模型精度、推理速度和特定于硬件的考虑因素。尽管其重要性，但尚未开发出基准来构建多目标优化这样一项具有挑战性的任务。为了弥补这一差距，我们引入了定制的流程，将 HW-NAS 的实时语义分割任务转换为标准 MOP。在简化的基础上，我们提出了一个基准测试套件 CitySeg/MOP，其中包含源自 Cityscapes 数据集的 15 个 MOP。 CitySeg/MOP 测试套件集成到 EvoXBench 平台中，提供与各种编程语言（例如 Python 和 MATLAB）的无缝接口，以进行即时健身评估。我们针对各种多目标进化算法全面评估了 CitySeg/MOP 测试套件，展示了其多功能性和实用性。源代码可在 https://github.com/EMI-Group/evoxbench 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.16266</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:18 GMT</pubDate>
    </item>
    <item>
      <title>使用纹理分析进行植物图像分类的空隙池层</title>
      <link>https://arxiv.org/abs/2404.16268</link>
      <description><![CDATA[arXiv:2404.16268v1 公告类型：新
摘要：池化层（例如，最大值和平均值）可能会忽略像素强度和/或特征值的空间排列中编码的重要信息。我们提出了一种新颖的空隙池层，旨在通过评估局部窗口内的变异性来捕获特征图的空间异质性。该层在多个尺度上运行，允许网络自适应地学习分层特征。空隙池层可以无缝集成到任何人工神经网络架构中。实验结果证明了该层在捕获复杂空间模式方面的有效性，从而提高了特征提取能力。所提出的方法在各个领域都有希望，特别是在农业图像分析任务中。这项工作通过引入一种新颖的池化层来丰富空间特征的表示，从而为人工神经网络架构的不断发展做出了贡献。我们的代码是公开的。]]></description>
      <guid>https://arxiv.org/abs/2404.16268</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:18 GMT</pubDate>
    </item>
    <item>
      <title>AIS 2024 用户生成内容视频质量评估挑战赛：方法和结果</title>
      <link>https://arxiv.org/abs/2404.16205</link>
      <description><![CDATA[arXiv:2404.16205v1 公告类型：新
摘要：本文回顾了 AIS 2024 视频质量评估 (VQA) 挑战赛，重点关注用户生成内容 (UGC)。本次挑战的目的是收集基于深度学习的方法，能够评估 UGC 视频的感知质量。 YouTube UGC 数据集中的用户生成视频包括多样化的内容（体育、游戏、歌词、动漫等）、质量和分辨率。所提出的方法必须在 1 秒内处理 30 个 FHD 帧。此次挑战赛共有 102 名参赛者报名，15 名参赛者提交了代码和模型。此处对前 5 名提交的性能进行了审查并提供，作为对用户生成内容的有效视频质量评估的各种深度模型的调查。]]></description>
      <guid>https://arxiv.org/abs/2404.16205</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:17 GMT</pubDate>
    </item>
    <item>
      <title>ActiveRIR：声学环境建模的主动视听探索</title>
      <link>https://arxiv.org/abs/2404.16216</link>
      <description><![CDATA[arXiv:2404.16216v1 公告类型：新
摘要：环境声学模型表示对于任何给定的声源/接收器位置，声音如何通过室内环境的物理特性进行转换。构建声学模型的传统方法涉及在空间中密集的空间位置收集大量声学数据，成本昂贵且耗时，或者依赖场景几何的特权知识来智能地选择声学数据采样位置。我们提出了主动声学采样，这是一项有效构建未映射环境的环境声学模型的新任务，其中配备视觉和声学传感器的移动代理联合构建环境声学模型和动态占用地图。我们引入了 ActiveRIR，这是一种强化学习 (RL) 策略，它利用来自视听传感器流的信息来指导代理导航并确定最佳声学数据采样位置，从而从最小的声学样本集生成高质量的环境声学模型。我们使用基于环境声学模型中的信息增益的新型强化学习奖励来训练我们的策略。 ActiveRIR 通过最先进的声学模拟平台评估各种不可见的室内环境，其性能优于一系列方法——包括基于空间新颖性和视觉探索的传统导航代理以及现有的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2404.16216</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:17 GMT</pubDate>
    </item>
    <item>
      <title>NeRF-XL：使用多个 GPU 扩展 NeRF</title>
      <link>https://arxiv.org/abs/2404.16221</link>
      <description><![CDATA[arXiv:2404.16221v1 公告类型：新
摘要：我们提出了 NeRF-XL，这是一种在多个 GPU 上分布神经辐射场 (NeRF) 的原理方法，从而能够以任意大容量训练和渲染 NeRF。我们首先重新审视现有的多 GPU 方法，这些方法将大场景分解为多个独立训练的 NeRF，并确定这些方法的几个基本问​​题，这些问题阻碍了重建质量的提高，因为在训练中使用了额外的计算资源 (GPU)。 NeRF-XL 解决了这些问题，只需使用更多硬件即可使用任意数量的参数来训练和渲染 NeRF。我们方法的核心在于一种新颖的分布式训练和渲染公式，它在数学上等同于经典的单 GPU 情况，并最大限度地减少了 GPU 之间的通信。通过解锁具有任意大参数数量的 NeRF，我们的方法是第一个揭示 NeRF 的多 GPU 缩放定律的方法，显示出通过更大的参数数量可以提高重建质量，并通过更多 GPU 来提高速度。我们展示了 NeRF-XL 在各种数据集上的有效性，包括迄今为止最大的开源数据集 MatrixCity，其中包含覆盖 25km^2 城市区域的 258K 图像。]]></description>
      <guid>https://arxiv.org/abs/2404.16221</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:17 GMT</pubDate>
    </item>
    <item>
      <title>教学视频中的步骤差异</title>
      <link>https://arxiv.org/abs/2404.16222</link>
      <description><![CDATA[arXiv:2404.16222v1 公告类型：新
摘要：将用户视频与参考操作视频进行比较是 AR/VR 技术提供针对用户进度量身定制的个性化帮助的关键要求。然而，当前基于语言的帮助方法只能回答有关单个视频的问题。我们提出了一种方法，首先利用现有的步骤注释和随附的旁白，自动生成涉及 HowTo100M 中的视频对的大量视觉指令调整数据，然后训练视频条件语言模型以在多个原始视频中联合推理。我们的模型在识别视频对之间的差异并根据这些差异的严重性对视频进行排名方面实现了最先进的性能，并显示出对多个视频执行一般推理的有希望的能力。]]></description>
      <guid>https://arxiv.org/abs/2404.16222</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:17 GMT</pubDate>
    </item>
    <item>
      <title>具有遮挡的 3D 人体姿势估计：介绍 BlendMimic3D 数据集和 GCN 细化</title>
      <link>https://arxiv.org/abs/2404.16136</link>
      <description><![CDATA[arXiv:2404.16136v1 公告类型：新
摘要：在 3D 人体姿势估计（HPE）领域，准确估计人体姿势，特别是在有遮挡的场景中，是一个重大挑战。这项工作确定并解决了 3D HPE 当前技术水平中关于数据稀缺和遮挡处理策略的差距。我们推出了新颖的 BlendMimic3D 数据集，旨在模拟现实世界中发生遮挡的情况，以便无缝集成到 3D HPE 算法中。此外，我们提出了一个 3D 姿态细化块，利用图卷积网络 (GCN) 通过图模型增强姿态表示。该 GCN 块充当即插即用解决方案，适用于各种 3D HPE 框架，无需重新训练。通过使用来自 BlendMimic3D 的遮挡数据训练 GCN，我们证明了在解决遮挡姿势方面的显着改进，并且与非遮挡姿势的结果相当。项目网页位于 https://blendmimic3d.github.io/BlendMimic3D/。]]></description>
      <guid>https://arxiv.org/abs/2404.16136</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:16 GMT</pubDate>
    </item>
    <item>
      <title>按现实世界挑战分类的协作感知中间融合方法调查</title>
      <link>https://arxiv.org/abs/2404.16139</link>
      <description><![CDATA[arXiv:2404.16139v1 公告类型：新
摘要：本调查分析了自动驾驶协作感知中的中间融合方法，并按现实世界的挑战进行分类。我们研究了各种方法，详细介绍了它们的特征以及它们采用的评估指标。重点是解决传输效率、定位错误、通信中断和异构性等挑战。此外，我们探索对抗对抗性攻击和防御的策略，以及适应域转移的方法。目的是概述中间融合方法如何有效应对这些不同的挑战，强调它们在推进自动驾驶协作感知领域的作用。]]></description>
      <guid>https://arxiv.org/abs/2404.16139</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:16 GMT</pubDate>
    </item>
    <item>
      <title>SAM 会梦见 EIG 吗？使用预期信息增益表征交互式分段器性能</title>
      <link>https://arxiv.org/abs/2404.16155</link>
      <description><![CDATA[arXiv:2404.16155v1 公告类型：新
摘要：我们介绍了交互式分割模型的评估程序。基于贝叶斯实验设计的概念，该过程测量模型对点提示的理解及其与所需分割掩模的对应关系。我们表明，Oracle Dice 指数测量在测量此属性时不敏感，甚至具有误导性。我们演示了所提出的程序在三个交互式分割模型和两个大型图像分割数据集的子集上的使用。]]></description>
      <guid>https://arxiv.org/abs/2404.16155</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:16 GMT</pubDate>
    </item>
    <item>
      <title>使用类共现概率改进多标签识别</title>
      <link>https://arxiv.org/abs/2404.16193</link>
      <description><![CDATA[arXiv:2404.16193v1 公告类型：新
摘要：多标签识别（MLR）涉及识别图像中的多个对象。为了解决这个问题的额外复杂性，最近的工作利用了在大型文本图像数据集上训练的视觉语言模型（VLM）的信息来完成任务。这些方法为每个对象（类）学习一个独立的分类器，忽略它们出现的相关性。可以从训练数据中捕获此类共现作为一对类之间的条件概率。我们提出了一个框架，通过合并对象对的共现信息来扩展独立分类器，以提高独立分类器的性能。我们使用图卷积网络 (GCN) 来强化类之间的条件概率，通过细化从使用 VLM 获得的图像和文本源得出的初始估计。我们在四个 MLR 数据集上验证了我们的方法，我们的方法优于所有最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.16193</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:16 GMT</pubDate>
    </item>
    <item>
      <title>关于用于视频生成、理解和流媒体的生成式人工智能和法学硕士的调查</title>
      <link>https://arxiv.org/abs/2404.16038</link>
      <description><![CDATA[arXiv:2404.16038v1 公告类型：新
摘要：本文深入探讨了当前最热门的人工智能技术，即生成人工智能（Generative AI）和大语言模型（LLM）如何重塑视频技术领域，包括视频生成、理解和流媒体。它强调了这些技术在制作高度逼真的视频方面的创新应用，这是弥合现实世界动态与数字创作之间差距的重大飞跃。该研究还深入研究了法学硕士在视频理解方面的先进能力，证明了他们从视觉内容中提取有意义的信息的有效性，从而增强了我们与视频的互动。在视频流领域，本文讨论了法学硕士如何为更高效和以用户为中心的流媒体体验做出贡献，并根据个人观众的喜好调整内容交付。这篇全面的综述探讨了当前的成就、持续的挑战以及将生成式人工智能和法学硕士应用于视频相关任务的未来可能性，强调了这些技术在推进与多媒体、网络和人工智能社区相关的视频技术领域所具有的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2404.16038</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:15 GMT</pubDate>
    </item>
    <item>
      <title>FairDeDup：检测和减轻语义数据集重复数据删除中的视觉语言公平性差异</title>
      <link>https://arxiv.org/abs/2404.16123</link>
      <description><![CDATA[arXiv:2404.16123v1 公告类型：新
摘要：最近的数据集重复数据删除技术已经证明，内容感知数据集修剪可以显着降低视觉语言预训练（VLP）模型的训练成本，与原始数据集上的训练相比，不会造成显着的性能损失。这些结果基于对从网络收集的常用图像标题数据集进行修剪，这些数据集已知含有有害的社会偏见，然后可以将其编入训练模型中。在这项工作中，我们评估了重复数据删除如何影响训练后的模型中这些偏差的普遍性，并对最近的 SemDeDup 算法引入了一种易于实现的修改，可以减少我们观察到的负面影响。在检查在 LAION-400M 的重复数据删除变体上训练的 CLIP 式模型时，我们发现我们提出的 FairDeDup 算法在 FairFace 和 FACET 数据集上始终比 SemDeDup 改进公平性指标，同时在 CLIP 基准上保持零样本性能。]]></description>
      <guid>https://arxiv.org/abs/2404.16123</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:15 GMT</pubDate>
    </item>
    <item>
      <title>翻译 OCTA 中视网膜特征的定量表征</title>
      <link>https://arxiv.org/abs/2404.16133</link>
      <description><![CDATA[arXiv:2404.16133v1 公告类型：新
摘要：目的：本研究探讨了使用生成机器学习 (ML) 将光学相干断层扫描 (OCT) 图像转换为光学相干断层扫描血管造影 (OCTA) 图像的可行性，从而可能绕过对专门 OCTA 硬件的需求。方法：该方法涉及实现生成对抗网络框架，其中包括 2D 血管分割模型和 2D OCTA 图像翻译模型。该研究利用包含 500 名患者的公共数据集（根据分辨率和疾病状态分为子集）来验证 TR-OCTA 图像的质量。验证采用多种质量和定量指标来将翻译图像与地面真实 OCTA (GT-OCTA) 进行比较。然后，我们使用 GT-OCTA 定量表征 TR-OCTA 中生成的血管特征，以评估使用 TR-OCTA 进行客观疾病诊断的可行性。结果：TR-OCTA 在 3 毫米和 6 毫米数据集中均显示出较高的图像质量（与 GT-OCTA 相比，具有高分辨率、适度的结构相似性和对比度质量）。血管指标存在轻微差异，尤其是患病患者。与受局部血管扭曲影响的密度特征相比，弯曲度和血管周长指数等血管特征表现出更好的趋势。结论：本研究通过使用 TR-OCTA 的血管特征进行疾病检测，为临床实践中采用 OCTA 的局限性提供了一种有前景的解决方案。翻译相关性：这项研究有可能通过更广泛地提供详细的血管成像并减少对昂贵的 OCTA 设备的依赖，从而显着增强视网膜疾病的诊断过程。]]></description>
      <guid>https://arxiv.org/abs/2404.16133</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:15 GMT</pubDate>
    </item>
    <item>
      <title>VN-Net：用于稀疏时空气象预报的视觉数值融合图卷积网络</title>
      <link>https://arxiv.org/abs/2404.16037</link>
      <description><![CDATA[arXiv:2404.16037v1 公告类型：新
摘要： 稀疏气象预报是细粒度天气预报不可或缺的部分，值得广泛关注。最近的研究强调了时空图卷积网络（ST-GCN）在预测地面气象站数值数据方面的潜力。然而，作为保真度最高、延迟最低的数据之一，卫星视觉数据在 ST-GCN 中的应用仍有待探索。很少有研究证明结合上述多模态数据进行稀疏气象预报的有效性。为了实现这一目标，我们引入了视觉数值融合图卷积网络（VN-Net），它主要利用：1）数值GCN（N-GCN）来自适应地建模时空数值数据的静态和动态模式； 2）Vision-LSTM网络（V-LSTM）从时间序列卫星图像中捕获多尺度联合通道和空间特征； 4) 基于 GCN 的解码器，用于生成指定气象因素的每小时预测。据我们所知，VN-Net是首次尝试引入GCN方法，利用多模态数据更好地处理稀疏时空气象预报。我们在 Weather2k 数据集上的实验表明，VN-Net 在温度、相对湿度和能见度预测的平均绝对误差 (MAE) 和均方根误差 (RMSE) 方面明显优于最先进的技术。此外，我们进行解释分析并设计定量评估指标，以评估纳入视觉数据的影响。]]></description>
      <guid>https://arxiv.org/abs/2404.16037</guid>
      <pubDate>Fri, 26 Apr 2024 09:14:14 GMT</pubDate>
    </item>
    </channel>
</rss>