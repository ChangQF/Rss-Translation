<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 11 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>StereoDiffusion：使用潜在扩散模型生成免训练立体图像</title>
      <link>https://arxiv.org/abs/2403.04965</link>
      <description><![CDATA[arXiv:2403.04965v1 公告类型：新
摘要：随着制造商推出更多 XR 设备，对立体图像的需求不断增加。为了满足这一需求，我们引入了 StereoDiffusion，这种方法与传统的修复管道不同，无需训练，使用起来非常简单，并且可以无缝集成到原始的稳定扩散模型中。我们的方法修改潜在变量，以提供端到端的轻量级功能，用于快速生成立体图像对，而不需要微调模型权重或任何图像后处理。使用原始输入生成左图像并估计其视差图，我们通过立体像素移位操作生成右图像的潜在向量，并辅以对称像素移位掩模去噪和自注意力层修改方法来对齐右图像侧图像与左侧图像。此外，我们提出的方法在整个立体生成过程中保持了高标准的图像质量，在各种定量评估中取得了最先进的分数。]]></description>
      <guid>https://arxiv.org/abs/2403.04965</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>ActFormer：通过主动查询的可扩展协作感知</title>
      <link>https://arxiv.org/abs/2403.04968</link>
      <description><![CDATA[arXiv:2403.04968v1 公告类型：新
摘要：协作感知利用多个机器人丰富的视觉观察，将单个机器人的感知能力扩展到其视野之外。许多先前的工作接收来自所有协作者的消息广播，导致在处理大量机器人和传感器时面临可扩展性挑战。在这项工作中，我们的目标是使用基于 Transformer 的架构来解决 \textit{可扩展的基于相机的协作感知}。我们的关键想法是使单个机器人能够根据学习的空间先验智能地识别协作者及其关联相机的相关性。这种对视觉特征相关性的主动理解不需要特征本身的传输，从而提高了通信和计算效率。具体来说，我们提出了 ActFormer，这是一个 Transformer，它通过使用预定义的 BEV 查询与多机器人多摄像头输入交互来学习鸟瞰图 (BEV) 表示。每个BEV查询可以根据姿态信息主动选择相关相机进行信息聚合，而不是不加区别地与所有相机交互。在V2X-Sim数据集上的实验表明，ActFormer将AP@0.7的检测性能从29.89%提高到45.15%，查询次数减少了约50%，展示了ActFormer在多智能体协作3D物体检测中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.04968</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>PIPsUS：超声波中的自监督密集点跟踪</title>
      <link>https://arxiv.org/abs/2403.04969</link>
      <description><![CDATA[arXiv:2403.04969v1 公告类型：新
摘要：寻找点级对应关系是超声 (US) 中的一个基本问题，因为它可以实现 US 地标跟踪，以便在不同手术（包括头颈手术）中进行术中图像引导。大多数现有的US跟踪方法，例如基于光流或特征匹配的方法，最初是针对RGB图像设计的，然后才应用于US。因此，域转移会影响它们的性能。培训可以通过真实的通信进行监督，但在美国获得这些通信的成本很高。为了解决这些问题，我们提出了一种称为 PIPsUS 的自监督像素级跟踪模型。我们的模型可以在一次前向传递中跟踪任意数量的点，并通过考虑多个帧（而不仅仅是连续帧）来利用时间信息。我们开发了一种新的自我监督训练策略，利用针对 RGB 图像训练的长期点跟踪模型作为教师，指导模型学习真实的运动，并使用数据增强来强制跟踪美国外观。我们在颈部和口腔超声以及超声心动图上评估了我们的方法，与快速归一化互相关和调谐光流相比，显示出更高的点跟踪精度。论文被接受后，代码将可用。]]></description>
      <guid>https://arxiv.org/abs/2403.04969</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>AFreeCA：所有人的无注释计数</title>
      <link>https://arxiv.org/abs/2403.04943</link>
      <description><![CDATA[arXiv:2403.04943v1 公告类型：新
摘要：对象计数方法通常依赖于手动注释的数据集。创建此类数据集的成本限制了这些网络对特定类别（例如人类或企鹅）的对象进行计数的多功能性，并且对不同类别的对象进行计数仍然是一个挑战。强大的文本到图像潜在扩散模型（LDM）的可用性提出了这些模型是否可用于生成计数数据集的问题。然而，LDM 很难仅根据文本提示来创建具有精确数量的对象的图像，但它们可以通过添加和删除图像中的对象来提供可靠的 \textit{sorting} 信号。利用这些数据，我们首先引入一种无监督排序方法来学习与对象相关的特征，随后使用 LDM 生成的计数数据对这些特征进行细化和锚定以用于计数目的。此外，我们提出了一种密度分类器引导的方法，用于将图像划分为包含可以可靠计数的对象的块。因此，我们可以为任何类型的对象生成计数数据，并以无监督的方式对它们进行计数。我们的方法优于其他无监督和少样本的替代方案，并且不限于可获得计数数据的特定对象类。代码在接受后发布。]]></description>
      <guid>https://arxiv.org/abs/2403.04943</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>通过对抗性攻击欺骗神经网络进行运动预测</title>
      <link>https://arxiv.org/abs/2403.04954</link>
      <description><![CDATA[arXiv:2403.04954v1 公告类型：新
摘要：人体运动预测仍然是一个悬而未决的问题，这对于自动驾驶和安全应用极其重要。尽管该领域取得了巨大进展，但广泛研究的对抗性攻击主题尚未应用于人体运动预测中的多重回归模型，例如 GCN 和基于 MLP 的架构。这项工作旨在通过在最先进的架构中进行广泛的定量和定性实验来缩小这一差距，类似于图像分类中对抗性攻击的初始阶段。结果表明，即使扰动水平较低，模型也容易受到攻击。我们还展示了影响模型性能的 3D 变换实验，特别是，我们表明大多数模型对不改变关节距离的简单旋转和平移敏感。我们的结论是，与早期的 CNN 模型类似，运动预测任务容易受到小扰动和简单 3D 变换的影响。]]></description>
      <guid>https://arxiv.org/abs/2403.04954</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>BAGS：通过多尺度内核建模模糊不可知高斯泼溅</title>
      <link>https://arxiv.org/abs/2403.04926</link>
      <description><![CDATA[arXiv:2403.04926v1 公告类型：新
摘要：最近在使用 3D 高斯进行场景重建和新颖视图合成方面的努力可以在策划的基准测试中取得令人印象深刻的结果；然而，现实生活中捕捉到的图像通常是模糊的。在这项工作中，我们分析了基于高斯分布的方法针对各种图像模糊的鲁棒性，例如运动模糊、散焦模糊、缩小模糊等。在这些退化情况下，基于高斯分布的方法往往会过度拟合，并产生比基于神经辐射场的方法更糟糕的结果。为了解决这个问题，我们提出了模糊不可知高斯泼溅（BAGS）。 BAGS 引入了额外的 2D 建模功能，即使存在图像模糊，也可以重建 3D 一致的高质量场景。具体来说，我们通过模糊提议网络（BPN）估计每像素卷积核来对模糊进行建模。 BPN 旨在考虑场景的空间、颜色和深度变化，以最大限度地提高建模能力。此外，BPN 还提出了一种质量评估掩模，用于指示发生模糊的区域。最后，我们介绍一种由粗到细的内核优化方案；这种优化方案速度快，并且避免了由于稀疏点云初始化而导致的次优解决方案，当我们在模糊图像上应用运动结构时，这种情况经常发生。我们证明 BAGS 在各种具有挑战性的模糊条件和成像几何条件下实现了逼真的渲染，同时显着改进了现有方法。]]></description>
      <guid>https://arxiv.org/abs/2403.04926</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>分而治之：通过内存高效平铺集成进行高分辨率工业异常检测</title>
      <link>https://arxiv.org/abs/2403.04932</link>
      <description><![CDATA[arXiv:2403.04932v1 公告类型：新
摘要：工业异常检测是计算机视觉领域的一项重要任务，具有广泛的实际用例。许多现实世界数据集中的异常区域尺寸较小，因此需要以高分辨率处理图像。这经常对模型训练和推理阶段的内存消耗带来重大挑战，使得一些现有方法无法广泛采用。为了克服这一挑战，我们提出了平铺集成方法，该方法通过将输入图像划分为平铺网格并为每个平铺位置训练专用模型来减少内存消耗。平铺集成与任何现有的异常检测模型兼容，无需对底层架构进行任何修改。通过引入重叠图块，我们利用了传统堆叠集成的优势，进一步提高了异常检测能力，超越了单纯的高分辨率。我们使用不同的底层架构（包括 Padim、PatchCore、FastFlow 和 Reverse Distillation）对两个标准异常检测数据集：MVTec 和 VisA 进行全面分析。我们的方法展示了跨设置的显着改进，同时保持在 GPU 内存限制内，仅消耗与单个模型处理单个图块所需的 GPU 内存一样多的内存。]]></description>
      <guid>https://arxiv.org/abs/2403.04932</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>用于动态视觉刺激生成的时空风格转移算法</title>
      <link>https://arxiv.org/abs/2403.04940</link>
      <description><![CDATA[arXiv:2403.04940v1 公告类型：新
摘要：了解视觉信息如何在生物和人工系统中编码通常需要视觉科学家产生适当的刺激来测试特定的假设。尽管深度神经网络模型通过图像风格迁移等方法彻底改变了图像生成领域，但可用的视频生成方法却很少。在这里，我们介绍时空风格转移（STST）算法，这是一种动态视觉刺激生成框架，可以对视频刺激进行强大的操作和合成，以进行视觉研究。它基于双流深度神经网络模型，该模型分解空间和时间特征以生成动态视觉刺激，其模型层激活与输入视频的激活相匹配。作为一个例子，我们展示了我们的算法能够生成模型同色异体，即动态刺激，其两流模型中的层激活与自然视频的层激活相匹配。我们表明，这些生成的刺激与其自然对应物的低级时空特征相匹配，但缺乏其高级语义特征，使其成为研究对象识别的强大范例。与早期层相比，深度视觉模型中的后期层激活表现出自然刺激和同色异谱刺激之间的相似性较低，这证实了生成的刺激中缺乏高级信息。最后，我们使用生成的刺激来探索预测编码深度网络的表示能力。这些结果展示了我们的算法作为视觉科学中动态刺激生成的多功能工具的潜在应用。]]></description>
      <guid>https://arxiv.org/abs/2403.04940</guid>
      <pubDate>Mon, 11 Mar 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应大型视觉语言模型以跨视觉模式连接边缘设备</title>
      <link>https://arxiv.org/abs/2403.04908</link>
      <description><![CDATA[arXiv:2403.04908v1 公告类型：新
摘要：视觉语言（VL）模型的最新进展激发了人们对其在边缘设备上的部署的兴趣，但处理不同视觉模式、手动注释和计算约束方面的挑战仍然存在。我们推出了 EdgeVL，这是一种新颖的框架，它通过无缝集成双模态知识蒸馏和量化感知对比学习来弥补这一差距。这种方法可以适应大型 VL 模型（例如 CLIP），以便在资源有限的设备上高效地使用 RGB 和非 RGB 图像，而无需手动注释。 EdgeVL 不仅将视觉语言对齐功能转移到紧凑模型，而且还保持量化后的特征质量，显着增强跨各种视觉模式的开放词汇分类性能。我们的工作代表了第一个系统性的努力，使大型 VL 模型适应边缘部署，在多个数据集上展示了高达 15.4% 的准确性改进，并且模型大小减少了高达 93 倍。]]></description>
      <guid>https://arxiv.org/abs/2403.04908</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>$\text{R}^2$-Bench：扰动下参考感知模型的鲁棒性基准测试</title>
      <link>https://arxiv.org/abs/2403.04924</link>
      <description><![CDATA[arXiv:2403.04924v1 公告类型：新
摘要：参考感知旨在通过多模式参考指导为视觉对象奠定基础，对于弥合提供指令的人类与智能系统感知的环境之间的差距至关重要。尽管该领域取得了进展，但参考感知模型（RPM）针对破坏性扰动的稳健性尚未得到很好的探索。这项工作彻底评估了 RPM 在一般和特定情况下对抗各种扰动的弹性。认识到参考感知任务的复杂性，我们提出了扰动的综合分类法，然后开发了一个通用工具箱来综合和评估复合扰动的影响。利用这个工具箱，我们构建了 $\text{R}^2$-Bench，这是一个基准，用于评估五个关键任务中噪声条件下引用感知模型的鲁棒性。此外，我们提出了 $\text{R}^2$-Agent，这是一种基于 LLM 的代理，可通过自然语言指令简化和自动化模型评估。我们的调查揭示了当前 RPM 对各种扰动的脆弱性，并提供了评估模型稳健性的工具，有可能促进智能系统安全、有弹性地集成到复杂的现实场景中。]]></description>
      <guid>https://arxiv.org/abs/2403.04924</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>使用条件可逆神经网络优化视网膜假体刺激</title>
      <link>https://arxiv.org/abs/2403.04884</link>
      <description><![CDATA[arXiv:2403.04884v1 公告类型：新
摘要：植入式视网膜假体通过绕过视网膜中受损的感光细胞并直接刺激剩余的功能性视网膜细胞，为恢复部分视力提供了一种有前景的解决方案。然而，相机和视网膜细胞之间的信息传输往往受到电极阵列分辨率低和缺乏不同神经节细胞类型特异性的限制，导致刺激效果不佳。在这项工作中，我们建议利用基于标准化流的条件可逆神经网络以无监督的方式优化视网膜植入刺激。这些网络的可逆性使我们能够将它们用作视觉系统计算模型的替代，同时还将输入相机信号编码为电极阵列上的优化电刺激。与其他方法（例如简单下采样、线性模型和前馈卷积神经网络）相比，基于流的可逆神经网络及其条件扩展可产生更好的视觉重建质量。使用经生理验证的模拟工具的各种指标。]]></description>
      <guid>https://arxiv.org/abs/2403.04884</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>走向场景图预期</title>
      <link>https://arxiv.org/abs/2403.04899</link>
      <description><![CDATA[arXiv:2403.04899v1 公告类型：新
摘要：时空场景图通过将场景分解为单个对象及其成对的时间关系来表示视频中的交互。对对象之间细粒度的成对关系的长期预期是一个具有挑战性的问题。为此，我们引入场景图预测（SGA）任务。我们采用最先进的场景图生成方法作为基线来预测对象之间未来的成对关系，并提出了一种新颖的方法 SceneSayer。在 SceneSayer 中，我们利用以对象为中心的关系表示来推理观察到的视频帧并模拟对象之间关系的演变。我们采用连续时间的视角，并分别使用 NeuralODE 和 NeuralSDE 的概念对对象交互演化的潜在动态进行建模。我们分别通过求解常微分方程和随机微分方程来推断未来关系的表示。对动作基因组数据集的广泛实验验证了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.04899</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>UniTable：通过自监督预训练建立表结构识别的统一框架</title>
      <link>https://arxiv.org/abs/2403.04822</link>
      <description><![CDATA[arXiv:2403.04822v1 公告类型：新
摘要：表格通过人类创建的隐式约定传达事实和定量数据，这些约定通常对机器解析来说具有挑战性。先前有关表结构识别 (TSR) 的工作主要集中在可用输入和工具的复杂的特定于任务的组合。我们提出了 UniTable，一个统一了 TSR 的培训范式和培训目标的培训框架。其训练范例将纯像素级输入的简单性与来自各种未注释表格图像的自监督预训练（SSP）所赋予的有效性和可扩展性结合起来。我们的框架将所有三个 TSR 任务的训练目标（提取表结构、单元格内容和单元格边界框 (bbox)）统一为一个与任务无关的统一训练目标：语言建模。广泛的定量和定性分析凸显了 UniTable 在四个最大的 TSR 数据集上的最先进 (SOTA) 性能。为了促进可重复研究、提高透明度和 SOTA 创新，我们在 https://github.com/poloclub/unitable 开源了我们的代码，并发布了第一个整个推理管道的 Jupyter Notebook，跨多个 TSR 数据集进行调整，支持所有三个 TSR 任务。]]></description>
      <guid>https://arxiv.org/abs/2403.04822</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>一个值得提示的项目：具有解开控制的多功能图像编辑</title>
      <link>https://arxiv.org/abs/2403.04880</link>
      <description><![CDATA[arXiv:2403.04880v1 公告类型：新
摘要：基于文本到图像扩散模型（DPM）的成功，图像编辑是实现人类与人工智能生成内容交互的重要应用。在各种编辑方法中，提示空间内的编辑因其控制语义的能力和简单性而受到更多关注。然而，由于扩散模型通常是在描述性文本标题上进行预训练的，直接编辑文本提示中的单词通常会导致生成完全不同的图像，违反了图像编辑的要求。另一方面，现有的编辑方法通常考虑引入空间掩模来保留未编辑区域的身份，这些区域通常被 DPM 忽略，从而导致不和谐的编辑结果。针对这两个挑战，在这项工作中，我们建议将全面的图像提示交互分解为多个项目提示交互，每个项目都链接到一个特殊的学习提示。由此产生的框架名为 D-Edit，基于预训练的扩散模型，具有解开的交叉注意层，并采用两步优化来构建项目提示关联。然后可以通过操作相应的提示将多功能图像编辑应用于特定项目。我们在四种类型的编辑操作中展示了最先进的结果，包括基于图像、基于文本、基于蒙版的编辑和项目删除，涵盖了大多数类型的编辑应用程序，所有这些都在一个统一的框架内。值得注意的是，D-Edit 是第一个能够（1）通过蒙版编辑实现项目编辑和（2）结合基于图像和文本的编辑的框架。我们通过定性和定量评估来展示各种图像集合的编辑结果的质量和多功能性。]]></description>
      <guid>https://arxiv.org/abs/2403.04880</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>端子条物体检测工业应用中综合训练数据影响的调查</title>
      <link>https://arxiv.org/abs/2403.04809</link>
      <description><![CDATA[arXiv:2403.04809v1 公告类型：新
摘要：在工业制造中，存在许多目视检查或检测特定物体的任务，这些任务目前是手动或通过经典图像处理方法执行的。因此，将最新的深度学习模型引入工业环境具有提高生产力和实现新应用的潜力。然而，收集和标记足够的数据通常很棘手，从而使此类项目的实施变得复杂。因此，图像合成方法通常用于从 3D 模型生成合成训练数据并自动对其进行注释，尽管这会导致模拟域与真实域之间的差距。在本文中，我们研究了标准物体检测器在端子排物体检测的复杂工业应用中的模拟到真实泛化性能。结合领域随机化和领域知识，我们创建了一个用于自动生成训练数据的图像合成管道。此外，我们还手动标注了 300 张端子排的真实图像以进行评估。结果表明，感兴趣的对象在任一域中具有相同比例的重要性。尽管如此，在优化的缩放条件下，RetinaNet 的模拟与真实性能的平均精度差异为 2.69%，Faster R-CNN 的模拟与真实性能差异为 0.98%，使该方法符合工业要求。]]></description>
      <guid>https://arxiv.org/abs/2403.04809</guid>
      <pubDate>Mon, 11 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    </channel>
</rss>