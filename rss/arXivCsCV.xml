<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Thu, 30 Nov 2023 03:14:21 GMT</lastBuildDate>
    <item>
      <title>用于抗锯齿渲染的多尺度 3D 高斯泼溅。 （arXiv：2311.17089v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17089</link>
      <description><![CDATA[3D 高斯最近作为一种高效的表示形式出现
3D 重建和渲染。尽管其渲染质量和速度很高
在高分辨率下，当以较低的分辨率渲染时，它们都会急剧恶化
分辨率或从远处的相机位置。低分辨率或远距离时
渲染时，图像的像素大小可能会低于奈奎斯特频率
与每个 splatted 3D Gaussian 的屏幕尺寸进行比较并导致锯齿
影响。连续的 alpha 渲染也大大减慢了速度
每个像素混合更多的散点高斯。为了解决这些问题，我们
提出一种多尺度 3D 高斯分布算法，该算法保持
不同尺度的高斯代表同一场景。更高分辨率
图像使用更小的高斯渲染，并且分辨率较低的图像
用较少的较大高斯函数渲染。在相似的训练时间下，我们的算法
可以实现 13\%-66\% PSNR 和 160\%-2400\% 渲染速度提升
与 Mip-NeRF360 数据集相比，4$\times$-128$\times$ 比例渲染
单尺度 3D 高斯分布。
]]></description>
      <guid>http://arxiv.org/abs/2311.17089</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>超越单一力量：广义视觉语言模型的定制集成。 （arXiv：2311.17091v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17091</link>
      <description><![CDATA[微调预训练的视觉语言模型 (VLM)，例如 CLIP，用于
开放世界泛化因其实用性而越来越受欢迎
价值。然而，仅依靠
单个模型的复杂算法设计，即使是表现出强大的模型
性能，例如 CLIP-ViT-B/16。本文首次探讨了
利用较弱的 VLM 来增强协作潜力
鲁棒单一模型的泛化。肯定的发现激励我们
从新的角度解决泛化问题，即集成
预先训练的 VLM。我们介绍三种定制的集成策略，每种策略
针对某一特定场景量身定制。首先我们介绍一下零样本
ensemble，根据不同模型的logits自动调​​整
当只有预训练的 VLM 可用时的置信度。此外，对于场景
通过额外的少量样本，我们提出了免训练和调整的集成，
根据计算资源的可用性提供灵活性。这
所提出的集成策略在零样本、基础到新和
跨数据集泛化，实现新的最先进的性能。
值得注意的是，这项工作代表了朝着增强
通过集成的 VLM 的泛化性能。该代码可在
https://github.com/zhiheLu/Ensemble_VLM.git。
]]></description>
      <guid>http://arxiv.org/abs/2311.17091</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>SEED-Bench-2：多模式大型语言模型的基准测试。 （arXiv：2311.17092v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17092</link>
      <description><![CDATA[多模态大语言模型 (MLLM)，建立在
强大的大语言模型（LLM），最近表现出了卓越的
不仅可以生成文本，还可以生成交错的图像
多模式输入（类似于 GPT-4V 和 DALL-E 3 的组合）。然而，
现有的 MLLM 基准仍然仅限于评估模型的理解力
单一图像文本输入的能力，未能跟上所取得的进步
在 MLLM 中。调查研究时必须有一个全面的基准
进展并揭示当前 MLLM 的局限性。在这项工作中，我们
将 MLLM 的功能分类为从 $L_0$ 到
$L_4$ 基于他们可以接受和生成的模式，并提出建议
SEED-Bench-2，一个评估
MLLM 的\textbf{分层}功能。具体来说，SEED-Bench-2
包含 24K 个多项选择题以及准确的人工注释，其中
跨越27个维度，包括文本和图像的评估
一代。带有真实选项的多项选择题源自
人工注释可以对模型进行客观有效的评估
性能，消除了在过程中人工或 GPT 干预的需要
评估。我们进一步评估 23 个著名开源软件的性能
MLLM 并总结有价值的观察结果。通过揭示其局限性
通过广泛评估现有的 MLLM，我们的目标是 SEED-Bench-2
提供见解，将推动未来研究实现一般目标
人工智能。数据集和评估代码可在以下位置获取
\href{https://github.com/AILab-CVC/SEED-Bench}
]]></description>
      <guid>http://arxiv.org/abs/2311.17092</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>PEA-Diffusion：非英语文本到图像生成中具有知识蒸馏的参数高效适配器。 （arXiv：2311.17086v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17086</link>
      <description><![CDATA[文本到图像的扩散模型以其生成
基于文字提示的逼真图像。然而现有的作品有
主要专注于英语，缺乏对非英语文本到图像的支持
楷模。最常用的翻译方法无法解决代
与语言文化相关的问题，同时从头开始进行特定的培训
语言数据集非常昂贵。在本文中，我们受到启发
提出一种简单的基于知识的即插即用语言迁移方法
蒸馏。我们需要做的就是训练一个类似 MLP 的轻量级
在teacher下只有6M参数的参数高效适配器（PEA）
知识蒸馏以及小型并行数据语料库。我们是
惊讶地发现冻结UNet的参数依然可以达到
在特定语言的即时评估集上表现出色，
证明PEA可以激发潜在的发电能力
原始的UNet。此外，它的性能非常接近
通用提示评估集上的英语文本到图像模型。此外，
我们的适配器可以用作插件来实现显着的结果
跨语言文本到图像生成的下游任务。代码将是
网址：https://github.com/OPPO-Mente-Lab/PEA-Diffusion
]]></description>
      <guid>http://arxiv.org/abs/2311.17086</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>重新思考混合以提高对抗性可转移性。 （arXiv：2311.17087v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17087</link>
      <description><![CDATA[混合增强已被广泛集成以生成对抗性
从一个国家迁移时具有出色的对抗性可转移性的例子
其他模型的替代模型。然而，影响的根本机制
混合对可转移性的影响仍未被探索。在这项工作中，我们
假设对抗性例子位于决策的收敛点
不同类别之间的边界表现出更好的可转移性
确定 Admix 倾向于将对抗性示例引导至此类
地区。然而，我们发现 Admix 中添加图像的约束削弱了其
能力，导致可转移性有限。为了解决这样的问题，我们
提出了一种新的基于输入变换的攻击，称为混合图像，但是
分离梯度（MIST）。具体来说，MIST 随机混合输入
具有随机移位图像的图像并分离每个损失的梯度
每个混合图像的项目。为了抵消不精确的梯度，MIST
计算每个输入样本的多个混合图像的梯度。
ImageNet 数据集上的大量实验结果表明，MIST
优于现有的基于 SOTA 输入转换的攻击，具有明显的
卷积神经网络 (CNN) 和视觉转换器的余量
(ViTs) w/wo 防御机制，支持 MIST 的高效性和
概论。
]]></description>
      <guid>http://arxiv.org/abs/2311.17087</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>使用模内和跨模态不一致的无监督多模态 Deepfake 检测。 （arXiv：2311.17088v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17088</link>
      <description><![CDATA[Deepfake 视频对社会构成了越来越大的威胁
对刑事司法、民主和人身安全的负面影响
隐私。与此同时，大规模检测深度伪造品仍然是一个非常具有挑战性的任务
通常需要来自现有 Deepfake 的标记训练数据的任务
生成方法。此外，即使是最准确的监督学习，
深度伪造检测方法不能推广到使用新生成的深度伪造
生成方法。在本文中，我们介绍了一种新颖的无监督方法
用于通过测量模内和跨模态来检测深度伪造视频
多模态特征之间的一致性；特别是视觉、音频和身份
特征。所提出的检测方法背后的基本假设是
自从 Deepfake 一代试图转移一个人的面部运动以来
身份到另一个，这些方法最终都会遇到一个权衡
运动和身份之间的矛盾令人羡慕地导致了可察觉的不一致。
我们通过广泛的实验验证了我们的方法，证明了
Deepfake 中存在显着的模内和跨模态不一致
视频，可以有效地利用它们来高精度地检测它们。
我们提出的方法是可扩展的，因为它不需要原始样本
推理，可概括，因为它仅在真实数据上进行训练，并且
可以解释，因为它可以查明模态的确切位置
不一致的情况可以由人类专家进行验证。
]]></description>
      <guid>http://arxiv.org/abs/2311.17088</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>CLIC：上下文中的概念学习。 （arXiv：2311.17083v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17083</link>
      <description><![CDATA[本文解决了学习局部视觉模式的挑战
从一张图像中生成对象，并用该图像生成描绘对象的图像
图案。学习本地化概念并将其放置在目标中的物体上
图像是一项不平凡的任务，因为对象可能具有不同的方向和
形状。我们的方法建立在视觉概念的最新进展之上
学习。它涉及从物体获取视觉概念（例如装饰品）
源图像，然后将其应用于对象（例如椅子）
目标图像。我们的关键思想是进行上下文中的概念学习，获取
所属对象的更广泛背景下的局部视觉概念
到。为了本地化概念学习，我们使用包含以下内容的软掩模：
掩模内和周围图像区域的概念。我们展示我们的
通过图像内的对象生成方法，展示合理的
嵌入上下文中学习的概念。我们还介绍了方法
将获得的概念引导到目标图像中的特定位置，
采用交叉注意机制，并建立之间的对应关系
源对象和目标对象。我们的方法的有效性得到了证明
通过定量和定性实验以及比较
反对基线技术。
]]></description>
      <guid>http://arxiv.org/abs/2311.17083</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:18 GMT</pubDate>
    </item>
    <item>
      <title>DepthSSC：用于单目 3D 语义场景完成的深度空间对齐和动态体素分辨率。 （arXiv：2311.17084v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17084</link>
      <description><![CDATA[利用单目相机完成 3D 语义场景的任务正在取得进展
自动驾驶领域日益受到关注。其目标是
从部分图像预测 3D 场景中每个体素的占用状态
输入。尽管存在多种方法，但其中许多都忽略了
空间和深度信息之间的精确对齐问题。讲话
为此，我们提出了 DepthSSC，一种语义场景完成的高级方法
完全基于单目相机。 DepthSSC 结合了 ST-GF（空间
Transformation Graph Fusion）模块具有几何感知体素化，使
动态调整体素分辨率并考虑几何复杂性
3D 空间，以确保空间和深度信息之间的精确对齐。
这种方法成功地缓解了空间错位和扭曲问题
在先前的方法中观察到。通过对SemanticKITTI数据集的评估，
DepthSSC 不仅展示了其捕捉复杂 3D 图像的有效性
结构细节，同时也实现了最先进的性能。我们相信
DepthSSC 为基于单目相机的 3D 语义提供了全新的视角
场景完成研究并预计它将激发进一步的相关
学习。
]]></description>
      <guid>http://arxiv.org/abs/2311.17084</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:18 GMT</pubDate>
    </item>
    <item>
      <title>超越视觉线索：同步探索以目标为中心的视觉语言跟踪语义。 （arXiv：2311.17085v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17085</link>
      <description><![CDATA[单目标跟踪旨在定位视频序列中的一个特定目标，
给定其初始状态。经典追踪器仅依赖视觉提示，
限制他们应对外观变化等挑战的能力，
歧义和干扰。因此，视觉语言（VL）跟踪出现了
作为一种有前途的方法，将语言描述直接结合到
提供高级语义并增强跟踪性能。然而，目前
VL 跟踪器尚未充分利用 VL 学习的力量，因为它们受到了影响
免受诸如严重依赖现成主干网等功能的限制
提取、无效的 VL 融合设计以及不存在 VL 相关损失
功能。因此，我们提出了一种新颖的跟踪器，逐步探索
用于 VL 跟踪的以目标为中心的语义。具体来说，我们建议第一个
用于 VL 跟踪的同步学习骨干网 (SLB)，由两个组成
新颖的模块：目标增强模块（TEM）和语义感知模块
（萨姆）。这些模块使跟踪器能够感知与目标相关的语义
并同时理解视觉和文本模式的背景
速度，促进不同语义下的 VL 特征提取和融合
水平。此外，我们设计了密集匹配损失来进一步加强
多模态表示学习。 VL 跟踪的广泛实验
数据集证明了我们方法的优越性和有效性。
]]></description>
      <guid>http://arxiv.org/abs/2311.17085</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:18 GMT</pubDate>
    </item>
    <item>
      <title>对抗人工智能艺术中的“同质性”：对交互式人工智能装置《击剑幻觉》的思考。 （arXiv：2311.17080v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17080</link>
      <description><![CDATA[文章总结了人工中的三类“相同性”问题
智能（AI）艺术，每一种都发生在人工智能发展的不同阶段
图像创建工具。通过击剑幻觉项目，文章
反思AI艺术制作的设计在缓解
均匀性，保持人工智能图像合成器图像的唯一性，
并增强艺术品与观众之间的联系。这张纸
致力于通过讲述独特的人工智能艺术来激发创作
来自击剑幻觉项目的努力和见解，所有
致力于解决“同一性”问题。
]]></description>
      <guid>http://arxiv.org/abs/2311.17080</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:17 GMT</pubDate>
    </item>
    <item>
      <title>I-MedSAM：使用 Segment Anything 进行隐式医学图像分割。 （arXiv：2311.17081v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17081</link>
      <description><![CDATA[随着深度神经网络（DNN）的发展，人们做出了许多努力
用于处理医学图像分割。传统方法，例如 nnUNet
在各个数据集上训练特定的分割模型。最近的很多
已经提出了一些方法来适应基本的分段任意模型
（SAM）到医学图像分割。然而，他们仍然专注于离散
生成像素级预测的表示，这些预测在空间上
不灵活并且很难扩展到更高分辨率。相反，隐式方法
学习分割的连续表示，这对于医学至关重要
图像分割。在本文中，我们提出了 I-MedSAM，它利用
连续表示和 SAM 的优点，以获得更好的
跨领域能力和准确的边界划分。由于医学影像
分割需要预测详细的分割边界，我们设计了一个
新颖的适配器可通过高频信息增强 SAM 功能
在参数高效微调 (PEFT) 期间。转换 SAM 特征和
坐标转换为连续分割输出，我们利用隐式神经网络
用于学习隐式分段解码器的表示（INR）。我们还建议
用于有效学习 INR 的不确定性引导采样策略。
对二维医学图像分割任务的广泛评估表明
我们提出的方法只有 1.6M 可训练参数，优于现有方法
方法包括离散方法和连续方法。代码将被发布。
]]></description>
      <guid>http://arxiv.org/abs/2311.17081</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:17 GMT</pubDate>
    </item>
    <item>
      <title>DreamPropeller：通过并行采样增强文本到 3D 的生成。 （arXiv：2311.17082v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17082</link>
      <description><![CDATA[最近的方法，例如分数蒸馏采样 (SDS) 和变分法
使用 2D 扩散模型进行文本到 3D 生成的分数蒸馏 (VSD)
表现出了令人印象深刻的发电质量。然而，漫长的一代
此类算法的时间显着降低了用户体验。解决
针对这个问题，我们提出了 DreamPropeller，一种嵌入式加速算法
可以围绕任何现有的基于文本到 3D 的生成管道
分数蒸馏。我们的框架概括了皮卡德迭代，这是一种经典的迭代
用于并行采样 ODE 路径的算法，并且可以考虑非 ODE 路径
例如基于动量的梯度更新和尺寸变化
优化过程与 3D 生成的许多情况一样。我们表明我们的
算法用并行计算换取挂钟时间，并凭经验实现
所有测试的速度提升高达 4.7 倍，生成质量的下降可以忽略不计
构架。
]]></description>
      <guid>http://arxiv.org/abs/2311.17082</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:17 GMT</pubDate>
    </item>
    <item>
      <title>IG Captioner：信息增益字幕器是强大的零样本分类器。 （arXiv：2311.17072v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17072</link>
      <description><![CDATA[生成训练已被证明对于构建非常强大
视觉语言模型。然而，在零样本判别基准上，有
使用生成和训练模型训练的模型之间仍然存在性能差距
歧视性目标。在本文中，我们的目标是通过以下方式缩小这一差距：
提高分类任务生成训练的效率，无需
任何微调过程或附加模块。

具体来说，我们专注于缩小生成字幕之间的差距
和 CLIP 分类器。我们首先分析
标题生成器和分类器，并观察标题生成继承了
使用纯文本模态训练的语言模型的分布偏差，
使其不太依赖于视觉信号。为了解决这个问题，我们
重新设计字幕员的评分目标，以减轻
分布偏差并专注于衡量所带来的信息增益
视觉输入。我们进一步设计一个生成训练目标来匹配
评价目标。我们将训练和评估的模型命名为
作为信息增益（IG）字幕器的新颖程序。我们对模型进行预训练
公共 Laion-5B 数据集并执行一系列判别性评估。
对于 ImageNet 上的零样本分类，IG Captioner 实现了 $&gt; 18\%$
对标准字幕机的改进，实现可比的性能
与 CLIP 分类器。 IG Captioner 也展现了强劲的表现
MSCOCO 和 Flickr30K 上的零样本图像文本检索任务。我们希望这个
论文激发了进一步研究统一生成性和判别性
视觉语言模型的训练程序。
]]></description>
      <guid>http://arxiv.org/abs/2311.17072</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:16 GMT</pubDate>
    </item>
    <item>
      <title>用于人员重新识别的基于整体和组件的语义表示的自监督学习。 （arXiv：2311.17074v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17074</link>
      <description><![CDATA[交互式细分模型 (ISM)，例如细分任意模型 (Segment Anything Model)
显着改善了各种计算机视觉任务，但它们的应用
人员重新识别 (ReID) 仍然有限。另一方面，现有
ReID 的语义预训练模型通常具有预定义等限制
解析范围或粗略语义。此外，ReID 和换衣
ReID（CC-ReID）由于领域不同，通常单独对待。
本文研究了是否利用精确的以人为中心的语义
表示可以提高 ReID 性能并提高泛化能力
各种 ReID 任务之间。我们提出了 SemReID，一种自我监督的 ReID 模型，
利用 ISM 进行基于自适应部分的语义提取，有助于
提高 ReID 性能。 SemReID 还进一步完善了其语义
通过图像遮蔽和 KoLeo 等技术进行表示
正则化。评估三种类型的 ReID 数据集——标准
ReID、CC-ReID 和无约束 ReID——展示了卓越的性能
与最先进的方法相比。此外，认识到资源的稀缺性
具有细粒度语义的大型人物数据集，我们引入了新颖的
LUPerson-Part 数据集协助 ReID 方法获取细粒度零件
语义以实现稳健的性能。
]]></description>
      <guid>http://arxiv.org/abs/2311.17074</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:16 GMT</pubDate>
    </item>
    <item>
      <title>大型多模态模型的组合思想链提示。 （arXiv：2311.17076v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17076</link>
      <description><![CDATA[强大的视觉支柱和大型语言模型（LLM）的结合
推理导致大型多模态模型（LMM）成为当前的
适用于各种视觉和语言 (VL) 任务的标准。然而，最近
研究表明，即使是最先进的 LMM 仍然难以捕获
组合视觉推理的各个方面，例如属性和关系
物体之间。一种解决方案是利用场景图（SG）——一种形式化
对象及其关系和属性，已被广泛用作
视觉和文本领域之间的桥梁。然而，场景图数据需要
场景图注释，收集起来很昂贵，因此不容易
可扩展。此外，基于 SG 数据微调 LMM 可能会导致灾难性的结果
忘记预训练目标。为了克服这个问题，受到启发
思想链方法，我们提出组合思想链（CCoT），
利用SG的新型零样本思想链提示方法
表示以便从 LMM 中提取组合知识。
具体来说，我们首先使用 LMM 生成 SG，然后在中使用该 SG
产生响应的提示。通过大量的实验，我们发现
所提出的 CCoT 方法不仅提高了多个视觉上的 LMM 性能
和语言 VL 组合基准，而且还提高了性能
通用多模式基准上的几个流行的 LMM，无需
微调或带注释的真实 SG。
]]></description>
      <guid>http://arxiv.org/abs/2311.17076</guid>
      <pubDate>Thu, 30 Nov 2023 03:14:16 GMT</pubDate>
    </item>
    </channel>
</rss>