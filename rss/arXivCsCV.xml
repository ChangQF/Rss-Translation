<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://arxiv.org/rss/</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 31 Jan 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于面部变形攻击的最佳地标引导图像混合</title>
      <link>https://arxiv.org/abs/2401.16722</link>
      <description><![CDATA[在本文中，我们提出了一种进行面部变形攻击的新方法，该方法利用最佳地标引导图像混合。当前的面部变形攻击可以分为基于地标的方法和基于生成的方法。基于地标的方法使用几何变换根据平均地标来扭曲面部区域，但通常会产生视觉质量较差的变形图像。基于生成的方法采用生成模型来混合多个人脸图像，可以实现更好的视觉质量，但通常无法成功生成可以有效躲避最先进的人脸识别系统（FRS）的变形图像。我们提出的方法通过优化变形地标并使用图卷积网络（GCN）结合地标和外观特征来克服以前方法的局限性。我们将面部标志建模为完全连接的二部图中的节点，并利用 GCN 来模拟它们的空间和结构关系。其目的是捕捉面部形状的变化，并在变形过程中准确操纵面部外观特征，从而产生高度逼真和视觉忠实的变形面部图像。在两个公共数据集上的实验证明，我们的方法继承了之前基于地标和基于生成的方法的优点，并生成了更高质量的变形图像，对最先进的 FRS 构成了更重大的威胁。]]></description>
      <guid>https://arxiv.org/abs/2401.16722</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:16 GMT</pubDate>
    </item>
    <item>
      <title>MESA：通过分段来匹配所有内容</title>
      <link>https://arxiv.org/abs/2401.16741</link>
      <description><![CDATA[特征匹配是计算机视觉领域的一项关键任务，涉及寻找图像之间的对应关系。先前的研究使用基于学习的特征比较取得了显着的性能。然而，图像之间普遍存在的匹配冗余导致这些方法中出现不必要且容易出错的计算，从而限制了其准确性。为了解决这个问题，我们提出了 MESA，这是一种建立精确区域（或区域）匹配以有效减少匹配冗余的新方法。 MESA首先利用SAM（一种最先进的图像分割基础模型）的高级图像理解能力来获取具有隐式语义的图像区域。然后，提出了一个多关系图来对这些区域的空间结构进行建模并构建它们的尺度层次结构。基于从图导出的图模型，区域匹配被重新表述为能量最小化任务并得到有效解决。大量实验表明，MESA 可以显着提高室内和室外下游任务（例如，多点匹配器）的精度。 DKM 在室内姿态估计中+13.61%。]]></description>
      <guid>https://arxiv.org/abs/2401.16741</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:16 GMT</pubDate>
    </item>
    <item>
      <title>利用多视角时空关系变换器实现精确的 3D 人体姿势估计</title>
      <link>https://arxiv.org/abs/2401.16700</link>
      <description><![CDATA[3D人体姿态估计捕获三维空间中的人体关节点，同时保留深度信息和物理结构。这对于需要精确姿态信息的应用至关重要，例如人机交互、场景理解和康复训练。由于数据采集的挑战，3D人体姿态估计的主流数据集主要由实验室环境中采集的多视点视频数据组成，这些数据除了图像帧内容之外还包含丰富的时空相关信息。鉴于 Transformer 卓越的自注意力机制，能够从多视图视频数据集中捕获时空相关性，我们提出了一种用于 3D 序列到序列 (seq2seq) 人体姿势检测的多阶段框架。首先，空间模块通过图像内内容表示人体姿态特征，而帧图像关系模块提取多视角图像之间的时间关系和3D空间位置关系特征。其次，采用自注意力机制，消除非人体部位的干扰，减少计算资源。我们的方法在 Human3.6M（一种流行的 3D 人体姿势检测数据集）上进行评估。实验结果表明，我们的方法在此数据集上实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2401.16700</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:15 GMT</pubDate>
    </item>
    <item>
      <title>从长期噪声视频中进行多粒度对应学习</title>
      <link>https://arxiv.org/abs/2401.16702</link>
      <description><![CDATA[现有的视频语言研究主要集中在学习短视频剪辑，由于建模长视频的计算成本过高，很少探索长期时间依赖性。为了解决这个问题，一种可行的解决方案是学习视频剪辑和字幕之间的对应关系，但这不可避免地会遇到多粒度噪声对应（MNC）问题。具体来说，MNC 指的是剪辑-标题错位（粗粒度）和帧-字错位（细粒度），阻碍了时间学习和视频理解。在本文中，我们提出了噪声鲁棒时间最优传输（Norton），它在统一的最优传输（OT）框架中解决了 MNC 问题。简而言之，诺顿利用视频段落和剪辑字幕对比损失来捕获基于 OT 的长期依赖性。为了解决视频段落对比度中的粗粒度错位问题，诺顿通过可对齐的提示桶过滤掉不相关的剪辑和字幕，并根据传输距离重新对齐异步剪辑字幕对。为了解决细粒度的错位问题，诺顿采用了软极大值运算符来识别关键单词和关键帧。此外，Norton 通过 OT 分配纠正对齐目标，利用剪辑字幕对比度中潜在的错误负样本，以确保精确的时间建模。视频检索、视频质量保证和动作分割的大量实验验证了我们方法的有效性。代码可从 https://lin-yijie.github.io/projects/Norton 获取。]]></description>
      <guid>https://arxiv.org/abs/2401.16702</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:15 GMT</pubDate>
    </item>
    <item>
      <title>LF Tracy：用于光场相机中显着物体检测的统一单管道方法</title>
      <link>https://arxiv.org/abs/2401.16712</link>
      <description><![CDATA[利用从光场 (LF) 相机提取的丰富信息有助于执行密集的预测任务。然而，采用光场数据来增强显着目标检测 (SOD) 仍然遵循传统的 RGB 方法，并且在社区中仍未得到充分探索。以前的方法主要采用定制的双流设计来发现光场相机内隐式的角度特征，从而导致不同的 LF 表示之间存在显着的信息隔离。在本研究中，我们提出了一种有效的范式（LF Tracy）来解决这一限制。我们避开了双流主干的传统专用融合和解码器架构，转而采用统一的单管道方法。这首先包括一个简单而有效的数据增强策略，称为 MixLD，以桥接不同 LF 表示下的空间、深度和隐式角度信息的连接。然后引入高效的信息聚合（IA）模块来促进非对称特征信息融合。由于这种创新方法，我们的模型超越了现有的最先进的方法，特别是在最新的大规模 PKU 数据集上比之前的结果提高了 23%。通过仅利用 2890 万个参数，与使用 RGB 图像的主干相比，该模型在增加 300 万个参数的情况下，准确率提高了 10%，而使用 LF 图像的主干则提高了 86%。源代码将在 https://github.com/FeiBryantkit/LF-Tracy 公开发布。]]></description>
      <guid>https://arxiv.org/abs/2401.16712</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:15 GMT</pubDate>
    </item>
    <item>
      <title>IEEE BigData 2023 击键验证挑战赛 (KVC)</title>
      <link>https://arxiv.org/abs/2401.16559</link>
      <description><![CDATA[本文介绍了 IEEE BigData 2023 击键验证挑战赛 (KVC) 的结果，该挑战考虑了击键动力学 (KD) 的生物特征验证性能，捕获了来自超过 185,000 名受试者的可变转录文本的长推文序列。数据来自 KD 迄今为止最大的两个公共数据库：阿尔托桌面数据库和移动击键数据库，保证每个受试者、年龄和性别注释的数据量最少，不存在损坏的数据，并避免受试者分布过度不平衡关于所考虑的人口统计属性。参与者提出了多种神经架构，使得最好的团队在桌面和移动场景中分别实现了低至 3.33% 和 3.61% 的全局等错误率 (EER)，超越了当前最先进的生物特征验证性能KD。 KVC 将在 CodaLab 上托管，并将继续成为研究界在相同实验条件下比较不同方法并加深该领域知识的有用工具。]]></description>
      <guid>https://arxiv.org/abs/2401.16559</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:14 GMT</pubDate>
    </item>
    <item>
      <title>深入医学图像中的任何事物：比较研究</title>
      <link>https://arxiv.org/abs/2401.16600</link>
      <description><![CDATA[单目深度估计 (MDE) 是许多医疗跟踪和绘图算法的关键组成部分，尤其是内窥镜或腹腔镜视频中的算法。然而，由于无法从真实患者数据中获取真实深度图，因此监督学习并不是预测医疗场景深度图的可行方法。尽管 MDE 的自我监督学习最近引起了人们的关注，但其输出很难可靠地评估，并且每个 MDE 对其他患者和解剖结构的普遍性是有限的。这项工作评估了新发布的 Depth Anything Model 在医疗内窥镜和腹腔镜场景上的零样本性能。我们将 Depth Anything 与在一般场景上训练的其他 MDE 模型以及在内窥镜数据上训练的域内模型进行了准确性和推理速度的比较。我们的研究结果表明，虽然 Depth Anything 的零样本能力相当令人印象深刻，但它在速度和性能方面并不一定比其他模型更好。我们希望这项研究能够激发在医疗场景中使用 MDE 基础模型的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2401.16600</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么、何时以及如何在大数据驱动的 3D 物体检测中使用主动学习来实现安全自动驾驶：实证探索</title>
      <link>https://arxiv.org/abs/2401.16634</link>
      <description><![CDATA[自动驾驶数据集中 3D 物体检测的主动学习策略可能有助于解决数据不平衡、冗余和高维数据的挑战。我们证明了熵查询选择信息样本的有效性，旨在降低注释成本并提高模型性能。我们在 nuScenes 数据集上使用 BEVFusion 模型进行 3D 对象检测进行实验，将主动学习与随机采样进行比较，并证明熵查询在大多数情况下表现优异。该方法对于缩小多数班级和少数班级之间的表现差距特别有效。特定于类的分析揭示了有限数据预算的注释资源的有效分配，强调了为模型训练选择多样化和信息丰富的数据的重要性。我们的研究结果表明，熵查询是一种有前途的数据选择策略，可以增强资源受限环境中的模型学习。]]></description>
      <guid>https://arxiv.org/abs/2401.16634</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:14 GMT</pubDate>
    </item>
    <item>
      <title>使用模板匹配和 CNN 通过连接和终端检测来表征磁性迷宫结构</title>
      <link>https://arxiv.org/abs/2401.16688</link>
      <description><![CDATA[在材料科学中，表征周期性结构中的断层对于理解材料特性至关重要。为了表征磁性迷宫图案，必须准确识别连接点和端子，每张图像通常具有一千多个紧密排列的缺陷。这项研究引入了一种名为 TM-CNN（模板匹配 - 卷积神经网络）的新技术，旨在检测图像中的大量小物体，例如磁性迷宫图案中的缺陷。使用TM-CNN识别了444张实验图像中的这些结构，并探索结果以加深对磁性材料的理解。它采用两阶段检测方法，将初始检测中使用的模板匹配与用于消除错误识别的卷积神经网络相结合。为了训练 CNN 分类器，需要创建大量训练图像。这一困难阻碍了 CNN 在许多实际应用中的使用。 TM-CNN 通过自动进行大部分注释并仅将少量修正留给人工审阅者，显着减少了创建训练图像的手动工作量。在测试中，TM-CNN 取得了令人印象深刻的 F1 分数 0.988，远远优于传统的模板匹配和基于 CNN 的目标检测算法。]]></description>
      <guid>https://arxiv.org/abs/2401.16688</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:14 GMT</pubDate>
    </item>
    <item>
      <title>按照人类指令进行高质量图像恢复</title>
      <link>https://arxiv.org/abs/2401.16468</link>
      <description><![CDATA[图像恢复是一个基本问题，涉及从退化的观察中恢复高质量的干净图像。多合一图像恢复模型可以使用特定于退化的信息作为指导恢复模型的提示，有效地从各种类型和级别的退化中恢复图像。在这项工作中，我们提出了第一种使用人工编写的指令来指导图像恢复模型的方法。考虑到多种退化类型，在自然语言提示的情况下，我们的模型可以从退化的对应图像中恢复高质量图像。我们的方法 InstructIR 在多项恢复任务上取得了最先进的结果，包括图像去噪、去雨、去模糊、去雾和（低光）图像增强。 InstructIR 比以前的一体化恢复方法提高了 +1dB。此外，我们的数据集和结果代表了文本引导图像恢复和增强新研究的新基准。我们的代码、数据集和模型可在以下网址获取：https://github.com/mv-lab/InstructIR]]></description>
      <guid>https://arxiv.org/abs/2401.16468</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:13 GMT</pubDate>
    </item>
    <item>
      <title>用新的基元扩展快速运动的运动学理论</title>
      <link>https://arxiv.org/abs/2401.16519</link>
      <description><![CDATA[快速运动的运动学理论及其相关的西格玛对数正态模型二维时空轨迹。它主要被构造为虚拟目标点之间曲线的时间重叠。具体来说，它使用弧和对数正态分别作为表示轨迹和速度的基元。本文建议开发这个模型，即我们所说的运动学理论变换，它建立了一个允许使用更多基元的数学框架。我们主要评估欧拉曲线以链接虚拟目标点和高斯、Beta、Gamma、双界对数正态和广义极值函数来对钟形速度剖面进行建模。使用这些基元，我们报告由人类、动物和拟人机器人执行的时空轨迹的重建结果。]]></description>
      <guid>https://arxiv.org/abs/2401.16519</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:13 GMT</pubDate>
    </item>
    <item>
      <title>用于 HSI 场景上波段选择的 Dropout Concrete Autoencoder</title>
      <link>https://arxiv.org/abs/2401.16522</link>
      <description><![CDATA[基于深度学习的高光谱图像（HSI）信息波段选择方法最近受到强烈关注，以消除光谱相关性和冗余。然而，由于离散变量无法参数化选择过程，现有的基于深度学习的方法要么需要额外的后处理策略来选择描述性波段，要么间接优化模型。为了克服这些限制，这项工作提出了一种新颖的端到端网络，用于信息丰富的频段选择。所提出的网络受到具体自动编码器（CAE）和丢失特征排序策略的进步的启发。与传统的基于深度学习的方法不同，所提出的网络直接在给定所需的频带子集的情况下进行训练，无需进一步的后处理。四个 HSI 场景的实验结果表明，所提出的 dropout CAE 实现了实质性且有效的性能水平，优于竞争方法。]]></description>
      <guid>https://arxiv.org/abs/2401.16522</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:13 GMT</pubDate>
    </item>
    <item>
      <title>SHViT：具有内存高效宏设计的单头视觉变压器</title>
      <link>https://arxiv.org/abs/2401.16456</link>
      <description><![CDATA[最近，高效的 Vision Transformer 在资源受限的设备上表现出了出色的性能和低延迟。传统上，他们在宏观层面使用 4x4 补丁嵌入和 4 级结构，同时在微观层面利用多头配置的复杂注意力。本文旨在以内存高效的方式解决所有设计级别的计算冗余问题。我们发现，使用较大步长的 patchify 茎不仅可以降低内存访问成本，还可以通过利用早期阶段减少空间冗余的令牌表示来实现有竞争力的性能。此外，我们的初步分析表明，早期阶段的注意力层可以用卷积代替，而后期阶段的几个注意力头在计算上是多余的。为了解决这个问题，我们引入了一个单头注意力模块，该模块本质上可以防止头冗余，同时通过并行组合全局和局部信息来提高准确性。在我们的解决方案的基础上，我们推出了 SHViT，这是一种单头视觉转换器，可以实现最先进的速度与精度权衡。例如，在 ImageNet-1k 上，我们的 SHViT-S4 在 GPU、CPU 和 iPhone12 移动设备上分别比 MobileViTv2 x1.0 快 3.3 倍、8.1 倍和 2.4 倍，同时准确度提高 1.3%。对于使用 Mask-RCNN 头在 MS COCO 上进行对象检测和实例分割，我们的模型实现了与 FastViT-SA12 相当的性能，同时在 GPU 和移动设备上的主干延迟分别降低了 3.8 倍和 2.0 倍。]]></description>
      <guid>https://arxiv.org/abs/2401.16456</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:12 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散先验桥接生成模型和判别模型以实现统一视觉感知</title>
      <link>https://arxiv.org/abs/2401.16459</link>
      <description><![CDATA[扩散模型在图像生成方面的卓越能力促使人们努力将其应用扩展到生成任务之外。然而，缺乏一种统一的方法来将扩散模型应用于具有不同语义粒度要求的视觉感知任务，这是一个持续存在的挑战。我们的目的是建立一个统一的视觉感知框架，利用生成模型和判别模型之间的潜在协同作用。在本文中，我们提出了 Vermouth，一个简单而有效的框架，包括一个包含丰富生成先验的预训练稳定扩散（SD）模型、一个能够集成分层表示的统一头（U-head）以及一个提供判别先验的适应专家。全面的研究揭示了苦艾酒的潜在特征，例如隐藏在不同时间步长和不同 U 网阶段的潜在变量中的不同感知粒度。我们强调，没有必要合并重量级或复杂的解码器来将扩散模型转换为有效的表示学习器。针对定制判别模型的广泛比较评估展示了我们的方法在基于零样本草图的图像检索（ZS-SBIR）、少样本分类和开放词汇语义分割任务上的有效性。这些有希望的结果证明了扩散模型作为强大学习者的潜力，确立了它们在提供信息丰富和强大的视觉表示方面的重要性。]]></description>
      <guid>https://arxiv.org/abs/2401.16459</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:12 GMT</pubDate>
    </item>
    <item>
      <title>DressCode：根据文本指导自动缝纫和生成服装</title>
      <link>https://arxiv.org/abs/2401.16465</link>
      <description><![CDATA[服装在人类外表中的重要作用凸显了服装数字化对于数字人类创造的重要性。 3D 内容创作的最新进展对于数字人类创作至关重要。尽管如此，通过文本指导生成服装仍处于萌芽阶段。我们引入了一种文本驱动的 3D 服装生成框架 DressCode，其旨在使新手设计民主化，并在时装设计、虚拟试穿和数字人类创作方面提供巨大潜力。对于我们的框架，我们首先引入 SewingGPT，这是一种基于 GPT 的架构，它将交叉注意力与文本条件嵌入相结合，以生成带有文本指导的缝纫图案。我们还为高质量、基于图块的 PBR 纹理生成定制了预训练的稳定扩散。通过利用大型语言模型，我们的框架通过自然语言交互生成 CG 友好的服装。我们的方法还促进了图案完成和纹理编辑，通过用户友好的交互简化了设计师的流程。通过全面评估以及与其他最先进方法的比较，我们的方法展示了最佳的质量并与输入提示保持一致。用户研究进一步验证了我们的高质量渲染结果，突出了其在生产环境中的实用性和潜力。]]></description>
      <guid>https://arxiv.org/abs/2401.16465</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:12 GMT</pubDate>
    </item>
    </channel>
</rss>