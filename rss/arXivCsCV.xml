<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 15 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通用血管周围空间识别网络 (PINGU)</title>
      <link>https://arxiv.org/abs/2405.08337</link>
      <description><![CDATA[arXiv:2405.08337v1 公告类型：新
摘要：血管周围空间（PVS）是大脑废物清除系统（类淋巴系统）的核心组成部分。这些结构在 MRI 图像上可见，它们的形态与衰老和神经系统疾病有关。 PVS 的手动量化既耗时又主观。已经开发了许多用于 PVS 分割的深度学习方法，但大多数方法都是在同质数据集和高分辨率扫描上开发和评估的，这可能限制了它们在临床和研究中获取的广泛图像质量的适用性。在这项工作中，我们在来自 6 个不同数据集的一系列不同质量和分辨率的手动分割 MRI 图像的异构训练样本上训练 nnUNet，这是一种性能最佳的生物医学图像分割算法。这些方法与用于 PVS 3D 分割的公开深度学习方法进行了比较。由此产生的模型 PINGU（通用血管周围空间识别 Nnunet）在白质 (WM) 中实现了 0.50(SD=0.15)、0.63(0.17) 和 0.54(0.11)、0.66(0.66) 的体素和簇级别骰子分数。 0.17）在基底神经节（BG）。 PINGU（0.20-0.38（WM，体素）、0.29-0.58（WM，簇）、0.22-0.36（BG，体素）、0.46-0.60（BG，簇））和公开可用的算法（0.18-0.30（WM，体素），0.29-0.38（WM簇），0.10-0.20（BG，体素），0.15-0.37（BG，簇）），但PINGU远远优于公开可用的算法，尤其是在BG。最后，从具有同质扫描属性的单个站点进行手动分割训练 PINGU 在内部交叉验证方面的性能稍低，但在某些情况下在外部验证方面提供了较高的性能。 PINGU 作为广泛使用的 PVS 分割工具脱颖而出，在 BG（与血管疾病和病理学相关的 PVS 领域）中具有特殊优势。]]></description>
      <guid>https://arxiv.org/abs/2405.08337</guid>
      <pubDate>Wed, 15 May 2024 06:18:57 GMT</pubDate>
    </item>
    <item>
      <title>StraightPCF：直点云过滤</title>
      <link>https://arxiv.org/abs/2405.08322</link>
      <description><![CDATA[arXiv:2405.08322v1 公告类型：新
摘要：点云过滤是一项基本的 3D 视觉任务，旨在消除噪声，同时恢复底层的干净表面。最先进的方法通过沿着随机轨迹将噪声点移动到干净的表面来消除噪声。这些方法通常需要在训练目标和/或后处理过程中进行正则化，以确保保真度。在本文中，我们介绍了 StraightPCF，一种新的基于深度学习的点云过滤方法。它的工作原理是沿着直线路径移动噪声点，从而减少离散化误差，同时确保更快地收敛到干净的表面。我们将噪声斑块建模为高噪声斑块变体与其干净对应物之间的中间状态，并设计 VelocityModule 来推断从前者到后者的恒定流速。这种恒定的流量导致直线过滤轨迹。此外，我们引入了一个距离模块，它使用估计的距离标量来缩放直线轨迹，以在干净表面附近实现收敛。我们的网络是轻量级的，只有 $\sim530K$ 参数，是 IterativePFN（最新的点云过滤网络）的 17%。对合成数据和真实数据的大量实验表明，我们的方法取得了最先进的结果。我们的方法还展示了过滤点的良好分布，而无需正则化。实现代码可以在：https://github.com/ddsediri/StraightPCF 找到。]]></description>
      <guid>https://arxiv.org/abs/2405.08322</guid>
      <pubDate>Wed, 15 May 2024 06:18:56 GMT</pubDate>
    </item>
    <item>
      <title>视网膜病变分割的跨数据集概括</title>
      <link>https://arxiv.org/abs/2405.08329</link>
      <description><![CDATA[arXiv:2405.08329v1 公告类型：新
摘要：识别眼底图像中的病变是自动化和可解释的视网膜疾病诊断的重要里程碑。为了支持这个方向的研究，已经发布了多个数据集，提出了不同病变的真实图。然而，注释之间存在重要差异，并提出了跨数据集的泛化问题。这项研究描述了几个已知数据集的特征，并比较了为增强模型泛化性能而提出的不同技术，例如随机权重平均、模型汤和集成。我们的结果提供了关于如何将粗略标记的数据与细粒度数据集相结合以改进病变分割的见解。]]></description>
      <guid>https://arxiv.org/abs/2405.08329</guid>
      <pubDate>Wed, 15 May 2024 06:18:56 GMT</pubDate>
    </item>
    <item>
      <title>VS-Assistant：满足外科医生需求的多功能手术助手</title>
      <link>https://arxiv.org/abs/2405.08272</link>
      <description><![CDATA[arXiv:2405.08272v1 公告类型：新
摘要：外科手术对于患者的医疗保健至关重要，许多研究已经开发出先进的算法来为外科医生提供理解和决策帮助。尽管取得了很大的进步，但这些算法是针对单一特定任务和场景开发的，在实践中需要手动组合不同的功能，从而限制了适用性。因此，智能且多功能的手术助手有望准确理解外科医生的意图，并相应地执行特定任务以支持手术过程。在这项工作中，通过利用先进的多模态大语言模型（MLLM），我们提出了一种多功能手术助手（VS-Assistant），它可以准确理解外科医生的意图并完成一系列手术理解任务，例如手术场景分析、手术器械检测和按需分割。具体来说，为了实现卓越的手术多模态理解，我们设计了混合投影仪 (MOP) 模块来调整 VS-Assistant 中的手术 MLLM，以平衡自然知识和手术知识。此外，我们还设计了一种手术功能调用调优策略，使 VS-Assistant 能够理解手术意图，从而按需进行一系列手术功能调用，以满足外科医生的需求。对神经外科数据的大量实验证实，我们的 VS-Assistant 可以比现有的 MLLM 更准确地理解外科医生的意图，从而在文本分析和视觉任务中具有压倒性的性能。源代码和模型将公开。]]></description>
      <guid>https://arxiv.org/abs/2405.08272</guid>
      <pubDate>Wed, 15 May 2024 06:18:55 GMT</pubDate>
    </item>
    <item>
      <title>基于事件的光流的矢量符号架构</title>
      <link>https://arxiv.org/abs/2405.08300</link>
      <description><![CDATA[arXiv:2405.08300v1 公告类型：新
摘要：从特征匹配的角度来看，事件摄像机的光流估计涉及通过比较伴随事件帧之间的特征相似性来识别事件对应关系。在这项工作中，我们利用矢量符号架构（VSA）为事件框架引入了一种有效且鲁棒的高维（HD）特征描述符。 VSA 中相邻变量之间的拓扑相似性有助于增强流匹配点的特征描述符的表示相似性，而其结构化符号表示能力则有助于事件极性和多个空间尺度的特征融合。基于这个高清特征描述符，我们提出了一种基于事件的光流的新颖特征匹配框架，包括基于模型（VSA-Flow）和自监督学习（VSA-SM）方法。在VSA-Flow中，准确的光流估计验证了HD特征描述符的有效性。在VSA-SM中，提出了一种基于HD特征描述符的新型相似性最大化方法，以自监督的方式仅从事件中学习光流，从而消除了对辅助灰度图像的需要。评估结果表明，与基于模型的学习方法和自监督学习方法相比，我们基于 VSA 的方法在 DSEC 基准上具有更高的准确性，同时在 MVSEC 基准上与这两种方法相比仍然具有竞争力。这一贡献标志着特征匹配方法中基于事件的光流的重大进步。]]></description>
      <guid>https://arxiv.org/abs/2405.08300</guid>
      <pubDate>Wed, 15 May 2024 06:18:55 GMT</pubDate>
    </item>
    <item>
      <title>用于密集、遮挡和大规模事件中地理空间车辆检测的多模态协作网络</title>
      <link>https://arxiv.org/abs/2405.08251</link>
      <description><![CDATA[arXiv:2405.08251v1 公告类型：新
摘要： 在大规模灾害事件中，最佳救援路线的规划取决于灾害现场的物体检测能力，其中主要挑战之一是物体密集和遮挡的存在。现有的方法通常基于 RGB 模态，很难在拥挤的环境中区分具有相似颜色和纹理的目标，并且无法识别模糊的对象。为此，我们首先利用 RGB 和高度图模式构建两个用于大规模事件的多模态密集和遮挡车辆检测数据集。基于这些数据集，我们提出了一种用于密集和遮挡车辆检测的多模态协作网络，简称 MuDet。 MuDet 分层增强了模态内部和跨模态的可辨别信息的完整性，并区分简单和复杂的样本。 MuDet 包括三个主要模块：单模态特征层次增强（Uni-Enh）、多模态交叉学习（Mul-Lea）和困难-简单判别（He-Dis）模式。 Uni-Enh 和 Mul-Lea 增强了每种模态的特征，并促进两种异构模态特征的交叉集成。 He-Dis通过对置信度值进行定义和阈值处理，有效分离出类内差异显着、类间差异最小的密集遮挡车辆目标，从而抑制复杂背景。两个重新标记的多模态基准数据集、4K-SAI-LCS 数据集和 ISPRS Potsdam 数据集的实验结果证明了 MuDet 的鲁棒性和泛化性。这项工作的代码可以在 \url{https://github.com/Shank2358/MuDet} 上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2405.08251</guid>
      <pubDate>Wed, 15 May 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>图像之间基于调色板的颜色传输</title>
      <link>https://arxiv.org/abs/2405.08263</link>
      <description><![CDATA[arXiv:2405.08263v1 公告类型：新
摘要：作为图像增强的一个重要子主题，颜色迁移旨在根据参考图像增强源图像的配色方案，同时保留语义上下文。为了实现颜色传输，提出了基于调色板的颜色映射框架。 \textcolor{black}{它是一种经典的解决方案，不依赖于复杂的语义分析来生成新的配色方案。然而，该框架通常需要手动设置，降低了其实用性。}传统调色板生成的质量取决于分色的程度。在本文中，我们提出了一种新的基于调色板的颜色传输方法，可以自动生成新的配色方案。通过重新设计的基于调色板的聚类方法，可以根据颜色分布将像素分为不同的部分，具有更好的适用性。 {通过结合基于深度学习的图像分割和新的颜色映射策略，可以在前景和背景部分独立地实现颜色迁移，同时保持语义一致性。}实验结果表明，我们的方法在自然方面比同行方法表现出显着的优势真实性、色彩一致性、通用性和鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2405.08263</guid>
      <pubDate>Wed, 15 May 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>迈向临床医生首选的分割：利用人在环进行医学图像分割中的测试时间适应</title>
      <link>https://arxiv.org/abs/2405.08270</link>
      <description><![CDATA[arXiv:2405.08270v1 公告类型：新
摘要：基于深度学习的医学图像分割模型在跨多个医疗中心部署时经常面临性能下降，这很大程度上是由于数据分布的差异。测试时间适应（TTA）方法使预先训练的模型适应测试数据，已被用来减轻这种差异。然而，现有的 TTA 方法主要侧重于操作批量归一化（BN）层或采用即时和对抗性学习，这可能无法有效纠正因数据分布不同而引起的不一致。在本文中，我们提出了一种新颖的人机循环 TTA (HiTTA) 框架，该框架在两个重要方面脱颖而出。首先，它利用了临床医生校正预测的大部分被忽视的潜力，将这些校正集成到 TTA 过程中，以引导模型做出与临床注释偏好更一致的预测。其次，我们的框架设想了一种发散损失，专门设计用于通过仔细校准 BN 参数来减少由域差异引起的预测发散。我们的 HiTTA 以其双重能力而著称，能够适应测试数据的分布，同时确保模型的预测符合临床预期，从而增强其在医疗环境中的相关性。在公共数据集上进行的大量实验强调了我们的 HiTTA 相对于现有 TTA 方法的优越性，强调了整合人类反馈的优势以及我们的分歧损失在增强模型性能和跨不同医疗中心的适应性方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2405.08270</guid>
      <pubDate>Wed, 15 May 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>基于多感受野策略的弱光及缺陷条件下壁画图像渐进增强与修复</title>
      <link>https://arxiv.org/abs/2405.08245</link>
      <description><![CDATA[arXiv:2405.08245v1 公告类型：新
摘要：古代壁画是宝贵的文化遗产，具有巨大的考古价值。它们通过内容提供对古代宗教、仪式、民间传说等的深入了解。然而，由于长期氧化和保护不充分，古代壁画不断遭到损坏，包括剥落、发霉等。此外，由于古代壁画通常是在室内绘制的，数码设备拍摄的图像的光强度往往较低。能见度差阻碍了受损地区的进一步恢复。为了解决古代壁画日益严重的损坏问题并促进考古遗址的批量修复，我们针对受损并在弱光下拍摄的古代壁画提出了一种两阶段修复模型，称为MER（壁画增强和修复网）。我们的两阶段模型不仅提高了恢复图像的视觉质量，而且与其他竞争对手相比，在相关指标评估中取得了值得称赞的结果。此外，我们还推出了一个专门利用所提出的模型修复古代壁画的网站。代码可在 https://gitee.com/bbfan2024/MER.git 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.08245</guid>
      <pubDate>Wed, 15 May 2024 06:18:53 GMT</pubDate>
    </item>
    <item>
      <title>具有密集 Blob 表示的组合文本到图像生成</title>
      <link>https://arxiv.org/abs/2405.08246</link>
      <description><![CDATA[arXiv:2405.08246v1 公告类型：新
摘要：现有的文本到图像模型很难遵循复杂的文本提示，因此需要额外的接地输入以实现更好的可控性。在这项工作中，我们建议将场景分解为视觉基元（表示为密集的斑点表示），其中包含场景的细粒度细节，同时是模块化的、人类可解释的且易于构建。基于斑点表示，我们开发了一个基于斑点的文本到图像扩散模型，称为 BlobGEN，用于组合生成。特别是，我们引入了一个新的屏蔽交叉注意模块来解开斑点表示和视觉特征之间的融合。为了利用大型语言模型（LLM）的组合性，我们引入了一种新的上下文学习方法来根据文本提示生成 blob 表示。我们的大量实验表明，BlobGEN 在 MS-COCO 上实现了卓越的零样本生成质量和更好的布局引导可控性。当通过法学硕士增强时，我们的方法在合成图像生成基准上表现出卓越的数值和空间正确性。项目页面：https://blobgen-2d.github.io。]]></description>
      <guid>https://arxiv.org/abs/2405.08246</guid>
      <pubDate>Wed, 15 May 2024 06:18:53 GMT</pubDate>
    </item>
    <item>
      <title>用于动作检测的语义和运动感知时空变换器网络</title>
      <link>https://arxiv.org/abs/2405.08204</link>
      <description><![CDATA[arXiv:2405.08204v1 公告类型：新
摘要：本文提出了一种新颖的时空变换器网络，该网络引入了几个原始组件来检测未修剪视频中的动作。首先，多特征选择性语义注意模型计算空间特征和运动特征之间的相关性，以正确地模拟不同动作语义之间的时空交互。其次，运动感知网络利用运动感知二维位置编码算法对视频帧中动作语义的位置进行编码。这种运动感知机制可以记住当前方法无法利用的动作帧中的动态时空变化。第三，基于序列的时间注意力模型捕获动作帧中的异构时间依赖性。与自然语言处理中使用的标准时间注意力主要旨在寻找语言单词之间的相似性相比，所提出的基于序列的时间注意力旨在确定共同定义动作含义的视频帧之间的差异和相似性。所提出的方法在四个时空动作数据集上的性能优于最先进的解决方案：AVA 2.2、AVA 2.1、UCF101-24 和 EPIC-Kitchens。]]></description>
      <guid>https://arxiv.org/abs/2405.08204</guid>
      <pubDate>Wed, 15 May 2024 06:18:52 GMT</pubDate>
    </item>
    <item>
      <title>无限纹理：文本引导的高分辨率扩散纹理合成</title>
      <link>https://arxiv.org/abs/2405.08210</link>
      <description><![CDATA[arXiv:2405.08210v1 公告类型：新
摘要：我们提出了无限纹理，一种根据文本提示生成任意大纹理图像的方法。我们的方法在单个纹理上微调扩散模型，并学习将该统计分布嵌入到模型的输出域中。我们使用样本纹理补丁来为这个微调过程提供种子，该纹理补丁可以选择从文本到图像模型（如 DALL-E 2）生成。在生成时，我们的微调扩散模型通过分数聚合策略来使用在单个 GPU 上生成任意分辨率的输出纹理图像。我们将我们的方法合成的纹理与基于补丁和深度学习纹理合成方法的现有工作进行比较。我们还展示了我们生成的纹理在 3D 渲染和纹理传输中的两个应用。]]></description>
      <guid>https://arxiv.org/abs/2405.08210</guid>
      <pubDate>Wed, 15 May 2024 06:18:52 GMT</pubDate>
    </item>
    <item>
      <title>RATLIP：基于循环仿射变换的生成对抗性 CLIP 文本到图像合成</title>
      <link>https://arxiv.org/abs/2405.08114</link>
      <description><![CDATA[arXiv:2405.08114v1 公告类型：新
摘要：以文本描述为条件合成高质量的真实感图像非常具有挑战性。生成对抗网络（GAN）是该任务的经典模型，经常遇到图像和文本描述之间的一致性低以及合成图像不够丰富的问题​​。最近，条件仿射变换（CAT），例如条件批量归一化和实例归一化，已被应用于 GAN 的不同层来控制图像中的内容合成。 CAT 是一个多层感知器，它基于相邻层之间的批量统计数据独立预测数据，全局文本信息对其他层不可用。为了解决这个问题，我们首先对 CAT 和递归神经网络 (RAT) 进行建模，以确保不同层可以访问全局信息。然后，我们在 RAT 之间引入洗牌注意力，以减轻循环神经网络中信息遗忘的特征。此外，我们的生成器和鉴别器都利用强大的预训练模型 Clip，该模型已广泛用于通过学习潜在空间中的多模态表示来建立文本和图像之间的关联。鉴别器利用 CLIP 理解复杂场景的能力来准确评估生成图像的质量。在 CUB、Oxford 和 CelebA-tiny 数据集上进行了大量实验，以证明所提出的模型相对于当前最先进模型的优越性。代码是 https://github.com/OxygenLu/RATLIP。]]></description>
      <guid>https://arxiv.org/abs/2405.08114</guid>
      <pubDate>Wed, 15 May 2024 06:18:51 GMT</pubDate>
    </item>
    <item>
      <title>IHC 很重要：将 IHC 分析纳入 H&E 全玻片图像分析，通过两阶段多模态双线性池融合改进癌症分级</title>
      <link>https://arxiv.org/abs/2405.08197</link>
      <description><![CDATA[arXiv:2405.08197v1 公告类型：新
摘要：免疫组织化学（IHC）在病理学中发挥着至关重要的作用，因为它可以检测组织样本中蛋白质的过度表达。然而，关于 IHC 对准确癌症分级影响的机器学习模型研究仍然较少。我们发现 IHC 和 H\&amp;E 具有明显的优缺点，同时又具有一定的互补性。基于这一观察，我们开发了一个带有特征池模块的两阶段多模态双线性模型。该模型旨在最大限度地发挥 IHC 和 HE 特征表示的潜力，从而与单独使用相比提高性能。我们的实验表明，将 IHC 数据与 H\&amp;E 染色图像结合到机器学习模型中，可以为癌症分级带来卓越的预测结果。所提出的框架在公共数据集 BCI 上实现了高达 0.953 的令人印象深刻的 ACC。]]></description>
      <guid>https://arxiv.org/abs/2405.08197</guid>
      <pubDate>Wed, 15 May 2024 06:18:51 GMT</pubDate>
    </item>
    <item>
      <title>DiffTF++：用于大词汇量 3D 生成的 3D 感知扩散变压器</title>
      <link>https://arxiv.org/abs/2405.08055</link>
      <description><![CDATA[arXiv:2405.08055v1 公告类型：新
摘要：自动生成多样化且高质量的 3D 资产是 3D 计算机视觉中一项基本但具有挑战性的任务。尽管在 3D 生成方面做出了广泛的努力，但现有的基于优化的方法仍难以有效地生成大规模 3D 资产。同时，前馈方法通常只专注于生成单个类别或几个类别，限制了它们的通用性。因此，我们引入了基于扩散的前馈框架，以通过单一模型应对这些挑战。为了有效地处理跨类别的几何和纹理的巨大多样性和复杂性，我们1）采用改进的三平面来保证效率； 2) 引入 3D 感知转换器，将通用 3D 知识与专用 3D 特征聚合起来； 3) 设计 3D 感知编码器/解码器以增强广义 3D 知识。基于 TransFormer 的 3D 感知扩散模型 DiffTF，我们提出了一个更强的 3D 生成版本，即 DiffTF++。它可以归结为两部分：多视图重建损失和三平面细化。具体来说，我们利用多视图重建损失来微调扩散模型和三平面解码器，从而避免重建误差带来的负面影响并改善纹理合成。通过消除两个阶段之间的不匹配，生成性能得到增强，尤其是在纹理方面。此外，还引入了 3D 感知细化过程来过滤掉伪影并细化三平面，从而生成更复杂和合理的细节。在 ShapeNet 和 OmniObject3D 上进行的大量实验令人信服地证明了我们提出的模块的有效性以及具有多样性、丰富语义和高质量的最先进的 3D 对象生成性能。]]></description>
      <guid>https://arxiv.org/abs/2405.08055</guid>
      <pubDate>Wed, 15 May 2024 06:18:50 GMT</pubDate>
    </item>
    </channel>
</rss>