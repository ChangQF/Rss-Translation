<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 08 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>学习视觉变压器的相关结构</title>
      <link>https://arxiv.org/abs/2404.03924</link>
      <description><![CDATA[arXiv:2404.03924v1 公告类型：新
摘要：我们引入了一种新的注意力机制，称为结构自注意力（StructSA），它利用注意力的关键查询交互中自然出现的丰富相关模式。 StructSA 通过卷积识别关键查询相关性的时空结构来生成注意力图，并使用它们动态聚合值特征的局部上下文。这有效地利用了图像和视频中丰富的结构模式，例如场景布局、对象运动和对象间关系。使用 StructSA 作为主要构建块，我们开发了结构视觉变换器 (StructViT) 并评估其在图像和视频分类任务上的有效性，在 ImageNet-1K、Kinetics-400、Something-Something 上取得了最先进的结果V1 和 V2、Diving-48 和 FineGym。]]></description>
      <guid>https://arxiv.org/abs/2404.03924</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>LightOctree：轻量级 3D 空间相干室内照明估计</title>
      <link>https://arxiv.org/abs/2404.03925</link>
      <description><![CDATA[arXiv:2404.03925v1 公告类型：新
摘要：我们提出了一种轻量级解决方案，用于从单个 RGB 图像估计空间相干的室内照明。以前使用体积表示估计照明的方法忽略了光源在空间中的稀疏分布，需要大量的内存和计算资源才能获得高质量的结果。我们引入了一个统一的、基于体素八叉树的照明估计框架来产生 3D 空间相干照明。此外，提出了可微分体素八叉树锥跟踪渲染层，以消除整个过程中的常规体积表示，并确保跨不同频域的特征保留。这种减少显着减少了空间使用和所需的浮点运算，而不会显着影响精度。实验结果表明，与以前的方法相比，我们的方法以最小的成本实现了高质量的相干估计。]]></description>
      <guid>https://arxiv.org/abs/2404.03925</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>增强乳房 X 光检查中的乳腺癌诊断：卷积神经网络和可解释人工智能的评估和集成</title>
      <link>https://arxiv.org/abs/2404.03892</link>
      <description><![CDATA[arXiv:2404.03892v1 公告类型：新
摘要：该研究引入了一种结合卷积神经网络（CNN）和可解释人工智能（XAI）的集成框架，用于使用 CBIS-DDSM 数据集增强乳腺癌诊断。利用经过微调的 ResNet50 架构，我们的研究不仅可以有效区分乳腺 X 线摄影图像的良性和恶性类别，还可以通过采用 XAI 方法（即 Grad-CAM、LIME、和 SHAP，为医疗保健专业人员解释 CNN 的决策流程。我们的方法包括精心设计的数据预处理流程和先进的数据增强技术，以抵消数据集的限制，并采用使用预先训练的网络（例如 VGG-16、DenseNet 和 ResNet）进行迁移学习。我们研究的重点是评估 XAI 在解释模型预测方面的有效性，重点是利用 Hausdorff 度量来定量评估 AI 生成的解释与专家注释之间的一致性。这种方法对于 XAI 在促进人工智能辅助诊断的可信度和道德公平方面发挥着关键作用。我们的研究结果表明 CNN 和 XAI 在推进乳腺癌诊断方法方面的有效合作，从而促进先进人工智能技术在临床环境中更加无缝的集成。通过增强人工智能驱动决策的可解释性，这项工作为改善人工智能系统和医疗从业者之间的协作奠定了基础，最终丰富了患者护理。此外，我们研究的意义远远超出了当前的方法论，提倡对多模式数据的整合和人工智能解释的完善进行后续调查，以满足临床实践的需求。]]></description>
      <guid>https://arxiv.org/abs/2404.03892</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>VoltaVision：电子元件分类的迁移学习模型</title>
      <link>https://arxiv.org/abs/2404.03898</link>
      <description><![CDATA[arXiv:2404.03898v1 公告类型：新
摘要：在本文中，我们分析了迁移学习在电子元件分类方面的有效性。迁移学习重用预先训练的模型，可以节省构建强大分类器的时间和资源，而不是从头开始学习。我们的工作介绍了一种轻量级 CNN（称为 VoltaVision），并将其性能与更复杂的模型进行了比较。我们测试了这样一个假设：将知识从类似任务转移到我们的目标领域会比在通用数据集上训练的最先进模型产生更好的结果。我们这项工作的数据集和代码可在 https://github.com/AnasIshfaque/VoltaVision 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.03898</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>Concept Weaver：在文本到图像模型中实现多概念融合</title>
      <link>https://arxiv.org/abs/2404.03913</link>
      <description><![CDATA[arXiv:2404.03913v1 公告类型：新
摘要：虽然在定制文本到图像生成模型方面取得了重大进展，但生成结合多个个性化概念的图像仍然具有挑战性。在这项工作中，我们介绍了 Concept Weaver，一种在推理时构建定制文本到图像扩散模型的方法。具体来说，该方法将过程分为两个步骤：创建与输入提示的语义一致的模板图像，然后使用概念融合策略对模板进行个性化。融合策略将目标概念的外观合并到模板图像中，同时保留其结构细节。结果表明，与其他方法相比，我们的方法可以生成具有更高身份保真度的多个自定义概念。此外，该方法被证明可以无缝处理两个以上的概念，并紧密遵循输入提示的语义，而无需混合不同主题的外观。]]></description>
      <guid>https://arxiv.org/abs/2404.03913</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>SleepVST：使用预先训练的变压器根据近红外视频信号进行睡眠分级</title>
      <link>https://arxiv.org/abs/2404.03831</link>
      <description><![CDATA[arXiv:2404.03831v1 公告类型：新
摘要：基于摄像头的生理监测技术的进步使得呼吸和心搏的稳健、非接触式测量成为可能，众所周知，这可以指示睡眠阶段。这引发了对基于摄像头的睡眠监测的研究，作为“黄金标准”多导睡眠图的有前途的替代品，“金标准”多导睡眠图操作繁琐，管理成本昂贵，因此不适合长期临床研究。在本文中，我们介绍了 SleepVST，这是一种 Transformer 模型，可在基于摄像头的睡眠阶段分类（睡眠分期）中实现最先进的性能。在对接触传感器数据进行预训练后，SleepVST 在 SHHS 和 MESA 数据集上优于现有的心肺睡眠分期方法，科恩 kappa 总分分别为 0.75 和 0.77。然后，我们证明 SleepVST 可以成功地转换为从视频中提取的心肺波形，从而实现完全无接触的睡眠分期。使用 50 个晚上的视频数据集，我们在基于四类视频的睡眠分期中实现了 78.8% 的总准确率和 0.71 的 Cohen&#39;s $\kappa$，在该领域树立了新的最先进水平。]]></description>
      <guid>https://arxiv.org/abs/2404.03831</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>PARIS3D：使用大型多模态模型进行基于推理的 3D 零件分割</title>
      <link>https://arxiv.org/abs/2404.03836</link>
      <description><![CDATA[arXiv:2404.03836v1 公告类型：新
摘要：3D 感知系统的最新进展显着提高了其执行分割等视觉识别任务的能力。然而，这些系统仍然严重依赖明确的人类指令来识别目标对象或类别，缺乏主动推理和理解隐含用户意图的能力。我们引入了一种新颖的分割任务，称为 3D 对象推理部分分割，旨在基于有关 3D 对象特定部分的复杂和隐式文本查询输出分割掩码。为了促进评估和基准测试，我们提供了一个大型 3D 数据集，其中包含超过 60k 条指令，并配有专门为基于推理的 3D 部分分割而设计的相应真实部分分割注释。我们提出了一种模型，能够基于隐式文本查询分割 3D 对象的各个部分，并生成与 3D 对象分割请求相对应的自然语言解释。实验表明，我们的方法实现了与使用显式查询的模型竞争的性能，并具有识别部分概念、推理它们并用世界知识补充它们的附加能力。我们的源代码、数据集和训练模型可在 https://github.com/AmrinKareem/PARIS3D 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.03836</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>提高面部识别的分布外数据分类的公平性</title>
      <link>https://arxiv.org/abs/2404.03876</link>
      <description><![CDATA[arXiv:2404.03876v1 公告类型：新
摘要：标准分类理论假设测试集中的图像分布和训练集中的图像分布相同。不幸的是，现实生活场景通常具有看不见的数据（“分布外数据”），这与训练分布中的数据（“分布内”）不同。这个问题在社会正义问题中最为普遍，其中来自代表性不足群体的数据可能出现在测试数据中，而不代表同等比例的训练数据。这可能会导致模型自信地返回错误的决策和预测。我们对以下问题感兴趣：当神经网络在分布内数据的多个数据集上同时训练时，神经网络的性能能否提高分布外数据的面部图像？我们通过合并离群值曝光模型来解决这个问题，并研究在实施其他面部图像数据集时模型的性能如何变化。我们观察到，通过应用离群值曝光、结合可训练的权重参数来增加机器对离群值图像的重视，以及通过重新加权不同类标签的重要性，可以提高模型的准确性和其他指标。我们还尝试了对图像进行排序并通过图像特征确定异常值是否比按平均像素值排序对指标有更大的影响。我们的目标是通过扫描更广泛的图像范围，使模型不仅更准确，而且更公平。我们还以相反的顺序测试了数据集，看看具有平衡特征的更公平的数据集是否会对模型的准确性产生影响。]]></description>
      <guid>https://arxiv.org/abs/2404.03876</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>使用变分贝叶斯混合量化运动预测的不确定性</title>
      <link>https://arxiv.org/abs/2404.03789</link>
      <description><![CDATA[arXiv:2404.03789v1 公告类型：新
摘要：安全性和鲁棒性是开发值得信赖的自动驾驶汽车的关键因素。解决这些因素的一个重要方面是让车辆能够预测周围所有移动物体的未来轨迹并量化预测的不确定性。在本文中，我们提出了顺序神经变分代理（SeNeVA），这是一种描述单个移动物体未来轨迹分布的生成模型。与 Argoverse 2 和 INTERACTION 数据集上最先进的方法相比，我们的方法可以区分分布外数据，同时量化不确定性并实现有竞争力的性能。具体而言，在交互测试集上实现了 0.446 米的最小最终位移误差、0.203 米的最小平均位移误差和 5.35% 的缺失率。还提供了广泛的定性和定量分析来评估所提出的模型。我们的开源代码可在 https://github.com/PurdueDigitalTwin/seneva 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.03789</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>语言引导实例感知域自适应全景分割</title>
      <link>https://arxiv.org/abs/2404.03799</link>
      <description><![CDATA[arXiv:2404.03799v1 公告类型：新
摘要：全景分割的日益相关性与自动驾驶和 AR/VR 应用的进步密切相关。然而，由于密集数据注释的昂贵性质，此类模型的部署受到限制，从而引发了无监督域适应（UDA）。全景 UDA 的一个关键挑战是减少标记源和未标记目标域之间的域差距，同时协调语义和实例分割的子任务以限制灾难性干扰。虽然已经取得了相当大的进展，但现有的方法主要集中在语义分割的适应上。在这项工作中，我们专注于通过新颖的实例感知跨域混合策略 IMix 合并实例级自适应。 IMix 通过提高实例分割性能显着提高全景质量。具体来说，我们建议将来自目标域的高置信度预测实例插入到源图像上，保留所得伪标签的详尽性，同时减少注入的确认偏差。然而，这种增强是以语义性能下降为代价的，这归因于灾难性遗忘。为了缓解这个问题，我们通过采用基于 CLIP 的域对齐 (CDA) 来规范我们的语义分支，利用自然语言提示的域鲁棒性。最后，我们提出了一个结合这两种机制（称为 LIDAPS）的端到端模型，在所有流行的全景 UDA 基准测试上取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2404.03799</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Transformer 中的位置偏差查询选择和对比查询表示在 CT 扫描中有效检测淋巴结</title>
      <link>https://arxiv.org/abs/2404.03819</link>
      <description><![CDATA[arXiv:2404.03819v1 公告类型：新
摘要：淋巴结（LN）评估是放射学和肿瘤学常规临床工作流程中一项关键、不可或缺但极具挑战性的任务。准确的淋巴结分析对于癌症诊断、分期和治疗计划至关重要。即使对于经验丰富的医生来说，在观察者间差异较大的情况下，在 3D CT 中找到分散分布、低对比度的临床相关淋巴结也很困难。由于相邻解剖结构具有相似的图像强度、形状或纹理（血管、肌肉、食道等），以前的自动 LN 检测工作通常会产生有限的召回率和较高的误报 (FP)。在这项工作中，我们提出了一种新的 LN DEtection TRansformer，名为 LN-DETR，以实现更准确的性能。更重要的是，通过使用多尺度 2.5D 特征融合增强 2D 主干以显式地合并 3D 上下文，我们为提高 LN 查询的表示质量做出了两个主要贡献。 1）考虑到LN边界通常不明确，提出了IoU预测头和位置去偏查询选择，以选择具有较高定位精度的LN查询作为解码器查询的初始化。 2）为了减少 FP，采用查询对比学习来显式强化 LN 查询，使其针对最匹配的真实查询而不是不匹配的查询预测。通过结合来自不同身体部位（颈部、胸部和腹部）和病理/癌症的七个 LN 数据集，对 1067 名患者（具有 10,000 多个标记的 LN）的 3D CT 扫描进行了训练和测试，我们的方法显着提高了以前领先方法的性能：在内部和外部测试中，相同 FP 率下的平均召回率 &gt; 4-5%。我们使用 NIH DeepLesion 基准进一步评估通用病变检测任务，与其他领先的报告结果相比，我们的方法在每张图像 0.5 至 4 FP 上实现了 88.46% 平均召回率的最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2404.03819</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>工业异常分割的测试时间培训</title>
      <link>https://arxiv.org/abs/2404.03743</link>
      <description><![CDATA[arXiv:2404.03743v1 公告类型：新
摘要：异常检测和分割（AD＆S）对于工业质量控制至关重要。虽然现有方法擅长为每个像素生成异常分数，但实际应用需要生成二进制分割来识别异常。由于许多实际场景中缺乏标记的异常，标准实践基于仅包含标称样本的验证集得出的一些统计数据对这些图进行二值化，从而导致分割性能较差。本文通过提出一种测试时训练策略来提高分割性能来解决这个问题。事实上，在测试时，我们可以直接从异常样本中提取丰富的特征来训练能够有效区分缺陷的分类器。我们的通用方法可以在任何提供异常分数图作为输出的 AD&amp;S 方法的下游工作，即使在多模式设置中也是如此。我们通过对 MVTec AD 和 MVTec 3D-AD 进行广泛的实验和评估，证明了我们的方法相对于基线的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.03743</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>扁平化父偏见：庞加莱球中的分层语义分割</title>
      <link>https://arxiv.org/abs/2404.03778</link>
      <description><![CDATA[arXiv:2404.03778v1 公告类型：新
摘要：层次结构是语义分类法的自然表示，包括图像分割中常规使用的分类法。事实上，最近关于语义分割的工作报告提高了利用分层标签结构的监督训练的准确性。受到这些结果的鼓舞，我们重新审视这项工作背后的基本假设。我们假设并凭经验验证观察到的分割准确性提高的原因可能与语义层次结构的使用完全无关。为了证明这一点，我们使用代表性的分层方法设计了一系列跨域实验。我们发现，在新的测试领域，一个平面（非分层）分割网络（其中从孩子推断出父母）比全面的分层方法具有更高的分割准确性。作为对这些发现的补充，并受到双曲空间内在属性的启发，我们研究了一种使用庞加莱球模型进行层次分割的更有原则的方法。双曲表示在很大程度上也优于以前的（欧几里德）分层方法，并且在分割精度方面与我们的平坦欧几里德基线相当。然而，它还表现出语义层次结构中父节点的令人惊讶的强大校准质量，特别是在更具挑战性的领域。我们的综合分析表明，分层分割的既定实践可能仅限于域内设置，而平面分类器的泛化效果要好得多，特别是如果它们在双曲空间中建模的话。]]></description>
      <guid>https://arxiv.org/abs/2404.03778</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>一致性模型的强化学习：更快的奖励引导文本到图像生成</title>
      <link>https://arxiv.org/abs/2404.03673</link>
      <description><![CDATA[arXiv:2404.03673v1 公告类型：新
摘要：强化学习（RL）通过直接优化捕捉图像质量、美观和指令遵循能力的奖励，改进了扩散模型的引导图像生成。然而，由此产生的生成策略继承了导致生成缓慢的扩散模型的相同迭代采样过程。为了克服这一限制，一致性模型提出学习一类新的生成模型，将噪声直接映射到数据，从而产生一个可以在短短一次采样迭代中生成图像的模型。在这项工作中，为了优化特定任务奖励的文本到图像生成模型并实现快速训练和推理，我们提出了一个通过 RL 微调一致性模型的框架。我们的框架称为一致性模型强化学习 (RLCM)，将一致性模型的迭代推理过程构建为 RL 过程。 RLCM 在文本到图像生成功能上改进了 RL 微调扩散模型，并在推理时间内以计算量换取样本质量。通过实验，我们表明 RLCM 可以使文本到图像的一致性模型适应难以通过提示表达的目标，例如图像可压缩性，以及源自人类反馈的目标，例如美学质量。与 RL 微调扩散模型相比，RLCM 训练速度明显更快，提高了奖励目标下测量的生成质量，并通过仅需两个推理步骤生成高质量图像来加快推理过程。我们的代码可在 https://rlcm.owenoertell.com 获取]]></description>
      <guid>https://arxiv.org/abs/2404.03673</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>SC4D：稀疏控制视频到 4D 生成和运动传输</title>
      <link>https://arxiv.org/abs/2404.03736</link>
      <description><![CDATA[arXiv:2404.03736v1 公告类型：新
摘要：2D/3D 生成模型的最新进展使得能够从单视图视频生成动态 3D 对象。现有方法利用分数蒸馏采样将动态场景形成为动态 NeRF 或密集 3D 高斯。然而，由于 NeRF 的隐含性质或复杂的密集高斯运动预测，这些方法很难在单视图条件下的参考视图对齐、时空一致性和运动保真度之间取得平衡。为了解决这些问题，本文提出了一种名为 SC4D 的高效、稀疏控制的视频转 4D 框架，该框架将运动和外观解耦，以实现卓越的视频转 4D 生成。此外，我们引入自适应高斯（AG）初始化和高斯对齐（GA）损失来减轻形状退化问题，确保学习的运动和形状的保真度。综合实验结果表明，我们的方法在质量和效率上都超过了现有方法。此外，在 SC4D 运动和外观的分离建模的推动下，我们设计了一种新颖的应用程序，可以根据文本描述将学习到的运动无缝转移到各种 4D 实体阵列上。]]></description>
      <guid>https://arxiv.org/abs/2404.03736</guid>
      <pubDate>Mon, 08 Apr 2024 06:18:02 GMT</pubDate>
    </item>
    </channel>
</rss>