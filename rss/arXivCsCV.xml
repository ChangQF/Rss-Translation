<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 08 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>SAM-PD：SAM 在通过提示去噪跟踪和分割视频中的任何内容方面可以帮助我们走多远</title>
      <link>https://arxiv.org/abs/2403.04194</link>
      <description><![CDATA[arXiv:2403.04194v1 公告类型：新
摘要：最近，快速分割模型，例如分段任意模型（SAM），已经在静态图像上展示了强大的零样本泛化能力。这些提示模型对不精确的提示输入（例如不精确的边界框）表现出去噪能力。在本文中，我们探讨了应用 SAM 跟踪和分割视频中的对象的潜力，其中我们将跟踪任务视为即时去噪任务。具体来说，我们迭代地传播前一帧中每个对象蒙版的边界框作为下一帧的提示。此外，为了增强 SAM 针对位置和尺寸变化的去噪能力，我们提出了一种多提示策略，为每个对象提供多个抖动和缩放框提示，并保留与模板掩模具有最高语义相似度的掩模预测。我们还引入了基于点的细化阶段来处理遮挡并减少累积错误。在不涉及跟踪模块的情况下，我们的方法在三个数据集（DAVIS2017、YouTubeVOS2018 和 UVO）上展示了视频对象/实例分割任务的可比性能，作为简洁的基线，并赋予基于 SAM 的下游应用程序跟踪功能。]]></description>
      <guid>https://arxiv.org/abs/2403.04194</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>CN-RMA：结合网络与光线行进聚合进行多视图图像的 3D 室内物体检测</title>
      <link>https://arxiv.org/abs/2403.04198</link>
      <description><![CDATA[arXiv:2403.04198v1 公告类型：新
摘要：本文介绍了 CN-RMA，一种从多视图图像中检测 3D 室内物体的新方法。我们观察到关键挑战是图像和 3D 对应的模糊性，没有明确的几何结构来提供遮挡信息。为了解决这个问题，CN-RMA利用3D重建网络和3D对象检测网络的协同作用，其中重建网络提供粗略的截断符号距离函数（TSDF），并引导图像特征在端到端中正确投票到3D空间。 -结束方式。具体来说，我们通过光线行进将权重与每条光线的采样点关联起来，表示图像中的像素对相应 3D 位置的贡献。这些权重由预测的有符号距离确定，以便图像特征仅投票给重建表面附近的区域。我们的方法在多视图图像的 3D 对象检测中实现了最先进的性能，根据 ScanNet 和 ARKitScenes 数据集上的 mAP@0.25 和 mAP@0.5 进行测量。代码和模型发布于 https://github.com/SerCharles/CN-RMA。]]></description>
      <guid>https://arxiv.org/abs/2403.04198</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>ACC-ViT：Atrous Convolution 在 Vision Transformers 中回归</title>
      <link>https://arxiv.org/abs/2403.04200</link>
      <description><![CDATA[arXiv:2403.04200v1 公告类型：新
摘要：变形金刚通过受视觉感知启发的注意力机制创新，已经提升到最先进的视觉架构。目前，视觉变换器中盛行两类注意力：区域注意力和稀疏注意力。前者限制了区域内的像素交互；后者将它们分布在稀疏的网格上。它们的对立性质导致了要么保持等级关系，要么获得全球背景的困境。在这项工作中，受空洞卷积的启发，我们引入了空洞注意力（Atrous Attention），它是区域注意力和稀疏注意力的融合，它可以自适应地巩固局部和全局信息，同时保持层次关系。作为对空洞卷积的进一步致敬，我们用空洞卷积重新设计了普遍存在的逆残差卷积块。最后，我们提出了一种通用的混合视觉变压器主干，名为 ACC-ViT，遵循标准视觉任务的传统做法。我们的微型版本模型在 ImageNet-1K 上实现了 $\sim 84 \%$ 精度，参数少于 $28.5$ 万美元，比最先进的 MaxViT 提高了 $\sim 84 \%$ 参数，同时参数减少了 $8.4\%$ 。此外，我们还研究了 ACC-ViT 主干在不同评估设置下的功效，例如微调、线性探测和零样本学习，涉及医学图像分析、对象检测和语言图像对比学习等任务。因此，ACC-ViT 是一个强大的视觉主干，在移动规模版本中也具有竞争力，非常适合具有小数据集的利基应用程序。]]></description>
      <guid>https://arxiv.org/abs/2403.04200</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>SDPL：无人机视图地理定位的平移密集分区学习</title>
      <link>https://arxiv.org/abs/2403.04172</link>
      <description><![CDATA[arXiv:2403.04172v1 公告类型：新
摘要：跨视图地理定位旨在匹配来自不同平台（例如无人机和卫星）的同一目标的图像。这是一项具有挑战性的任务，因为从不同的角度来看目标的外观和环境内容都会发生变化。现有方法主要侧重于通过特征图分割挖掘更全面的信息，但不可避免地破坏了图像结构，并且对查询中目标的移动和尺度敏感。为了解决上述问题，我们引入了一种简单而有效的基于部分的表示学习，称为移位密集分区学习（SDPL）。具体来说，我们提出了密集分区策略（DPS），它将图像分为多个部分以探索上下文信息，同时显式地维护全局结构。为了处理非中心目标的场景，我们进一步提出了移位融合策略，该策略基于不同的分割中心并行生成多组零件，然后自适应地融合所有特征以选择最佳分割。大量实验表明，我们的 SDPL 对位置移动和尺度变化具有鲁棒性，并且在两个流行基准（即 University-1652 和 SUES-200）上实现了具有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.04172</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 Segment Anything 进行边缘信息学习的机器图像编码</title>
      <link>https://arxiv.org/abs/2403.04173</link>
      <description><![CDATA[arXiv:2403.04173v1 公告类型：新
摘要：机器图像编码（ICM）是一种用于图像识别的图像压缩技术。
  由于图像识别人工智能的需求不断增长，这项技术至关重要。
  在本文中，我们提出了一种仅对图像中对象部分的边缘信息进行编码和解码的 ICM 方法，我们将其称为 SA-ICM。
  这是一个学习图像压缩 (LIC) 模型，使用 Segment Anything 创建的边缘信息进行训练。
  我们的方法可用于具有各种任务的图像识别模型。
  SA-ICM 对输入数据的变化也具有鲁棒性，使其适用于各种用例。
  此外，我们的方法从隐私角度来看也有好处，因为它删除了编码器一侧的人脸信息，从而保护了个人隐私。
  此外，这种LIC模型训练方法可用于训练视频神经表示（NeRV），这是一种视频压缩模型。
  通过使用 Segment Anything 创建的边缘信息来训练 NeRV，可以创建对图像识别有效的 NeRV（SA-NeRV）。
  实验结果证实了SA-ICM的优势，在图像识别方面呈现出最佳的图像压缩性能。
  我们还表明 SA-NeRV 在机器视频压缩方面优于普通 NeRV。]]></description>
      <guid>https://arxiv.org/abs/2403.04173</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>YYDS：具有粗略描述的可见红外人员重新识别</title>
      <link>https://arxiv.org/abs/2403.04183</link>
      <description><![CDATA[arXiv:2403.04183v1 公告类型：新
摘要：由于存在相当大的跨模态差异，可见光-红外行人重新识别（VI-ReID）具有挑战性。现有的工作主要集中于学习模态不变的特征，同时抑制模态特定的特征。然而，由于缺乏颜色信息，仅依靠红外样本检索可见图像是一个极端的问题。为此，我们提出了 Refer-VI-ReID 设置，旨在匹配来自红外图像和粗略语言描述（例如“红上衣黑裤子的男人”）的目标可见图像，以补充缺失的颜色信息。为了解决这个任务，我们设计了一种 Y-Y 形状的分解结构，称为 YYDS，来分解和聚合目标的纹理和颜色特征。具体来说，首先提出文本-IoU正则化策略以促进分解训练，然后提出联合关系模块来推断聚合。此外，研究了k-倒数重排序算法的跨模态版本，称为CMKR，其中探索了三种邻居搜索策略和一种局部查询扩展方法来缓解近邻的模态偏差问题。我们使用手动注释的描述在 SYSU-MM01、RegDB 和 LLCM 数据集上进行实验。 YYDS 和 CMKR 在所有三个数据集上都比 SOTA 方法取得了显着的改进。代码可在 https://github.com/dyhBUPT/YYDS 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.04183</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>MAP：无源模型知识产权保护的掩模修剪</title>
      <link>https://arxiv.org/abs/2403.04149</link>
      <description><![CDATA[arXiv:2403.04149v1 公告类型：新
摘要：深度学习在各种应用中取得了显着的进步，凸显了保护训练有素的模型的知识产权（IP）的重要性。它不仅需要授权使用，还需要确保模型在授权数据域中的部署，即使模型专用于某些目标域。以前的方法在执行 IP 保护时需要同时访问源训练数据并针对未经授权的数据，这使得它们对于去中心化的私有数据来说存在风险且效率低下。在本文中，我们针对只有训练有素的源模型可用的实际环境，并研究如何实现知识产权保护。为了实现这一目标，我们提出了一种新颖的掩码修剪（MAP）框架。 MAP源于一个直观的假设，即训练有素的模型中存在与目标相关的参数，定位和剪枝它们是IP保护的关键。从技术上讲，MAP 冻结源模型并学习特定于目标的二进制掩码，以防止未经授权的数据使用，同时最大限度地减少授权数据的性能下降。此外，我们引入了一个新的指标，旨在在源性能下降和目标性能下降之间实现更好的平衡。为了验证其有效性和多功能性，我们在各种场景下评估了 MAP，包括普通源可用、实用无源和具有挑战性的无数据。大量实验表明 MAP 具有最先进的新性能。]]></description>
      <guid>https://arxiv.org/abs/2403.04149</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>用于小样本异常检测的双路径鉴频器</title>
      <link>https://arxiv.org/abs/2403.04151</link>
      <description><![CDATA[arXiv:2403.04151v1 公告类型：新
摘要：少样本异常检测（FSAD）在工业制造中至关重要。然而，现有的 FSAD 方法难以有效利用有限数量的正常样本，并且可能无法检测和定位空间域中不明显的异常。我们进一步发现这些细微的异常在频域中会更加明显。在本文中，我们从频率角度提出了双路径鉴频器（DFD）网络来解决这些问题。具体来说，我们在图像级别和特征级别上生成异常。多频信息构建模块提取差分频率分量，并将其提供给细粒度特征构建模块以提供适应的特征。我们将异常检测视为判别分类问题，因此采用双路径特征判别模块来检测和定位特征空间中的图像级和特征级异常。鉴别器的目标是学习潜在空间中异常特征和正常特征的联合表示。在 MVTec AD 和 VisA 基准上进行的大量实验表明，我们的 DFD 超越了当前最先进的方法。源代码将可用。]]></description>
      <guid>https://arxiv.org/abs/2403.04151</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>ProMISe：使用 SAM 进行快速医学图像分割</title>
      <link>https://arxiv.org/abs/2403.04164</link>
      <description><![CDATA[arXiv:2403.04164v1 公告类型：新
摘要：随着分段任意模型（SAM）的提出，用于医学图像分割（MIS）的微调 SAM 已变得流行。然而，由于 SAM 模型规模较大以及自然图像和医学图像之间存在显着的域差距，基于微调的策略成本高昂，并且存在不稳定、特征损坏和灾难性遗忘的潜在风险。此外，一些通过微调策略将SAM转移到特定领域MIS的方法禁用了模型的提示能力，严重限制了其使用场景。在本文中，我们提出了一种自动提示模块（APM），它提供基于 SAM 的基础模型，并在目标域中提供欧几里德自适应提示。我们的实验表明，这种自适应提示显着提高了 SAM 在 MIS 中的非微调性能。此外，我们提出了一种称为增量模式转换（IPS）的新型非侵入性方法，以使 SAM 适应特定的医疗领域。实验结果表明，IPS 使 SAM 无需进行微调即可在 MIS 中实现最先进的或有竞争力的性能。通过耦合这两种方法，我们提出了 ProMISe，一种用于即时医学图像分割的端到端非微调框架。我们的实验表明，单独使用我们的方法或组合使用我们的方法都可以在低成本模式转换中实现令人满意的性能，并且所有 SAM 参数都被冻结。]]></description>
      <guid>https://arxiv.org/abs/2403.04164</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>可扩展且鲁棒的 Transformer 解码器，用于使用基础模型进行可解释的图像分类</title>
      <link>https://arxiv.org/abs/2403.04125</link>
      <description><![CDATA[arXiv:2403.04125v1 公告类型：新
摘要：可解释的计算机视觉模型可以产生透明的预测，其中将图像的特征与训练数据集中的原型进行比较，并且它们之间的相似性构成分类的基础。然而，这些方法的训练计算成本很高，引入了额外的复杂性，并且可能需要领域知识来使超参数适应新的数据集。受对象检测、分割和大规模自监督基础视觉模型发展的启发，我们引入了组件特征（ComFe），这是一种新颖的可解释设计图像分类方法，使用变压器解码器头和分层混合建模。仅通过全局图像标签，没有分割或部分注释，ComFe 就可以识别一致的图像组件，例如鸟的头部、身体、翅膀和尾巴以及图像背景，并确定这些特征中哪些特征可以为预测提供信息。我们证明，与之前的可解释模型相比，ComFe 在一系列细粒度视觉基准中获得了更高的准确性，而无需单独调整每个数据集的超参数。我们还表明，ComFe 在包括 ImageNet 在内的一系列数据集上优于不可解释的线性头，并提高了泛化和鲁棒性基准的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.04125</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>医疗人工智能的可解释人工智能框架</title>
      <link>https://arxiv.org/abs/2403.04130</link>
      <description><![CDATA[arXiv:2403.04130v1 公告类型：新
摘要：医疗保健行业因医疗物联网人工智能 (AIoMT) 的融合而发生了革命性的变化，使得先进的数据驱动解决方案能够改善医疗保健系统。随着人工智能 (AI) 模型日益复杂，对可解释人工智能 (XAI) 技术的需求变得至关重要，特别是在医疗领域，透明且可解释的决策变得至关重要。因此，在这项工作中，我们利用自定义 XAI 框架，结合了局部可解释模型无关解释 (LIME)、SHApley 附加解释 (SHAP) 和梯度加权类激活映射 (Grad-Cam) 等技术，专门为AIoT领域。拟议的框架提高了战略医疗方法的有效性，旨在灌输对人工智能驱动的医疗应用的信任和促进理解。此外，我们利用多数投票技术来聚合来自多个卷积神经网络 (CNN) 的预测，并利用它们的集体智慧在医疗保健系统中做出稳健而准确的决策。在此决策过程的基础上，我们将 XAI 框架应用于脑肿瘤检测，作为展示准确和透明诊断的用例。评估结果凸显了XAI框架的卓越性能，实现了高精度、召回率和F1分数，训练准确率高达99%，验证准确率高达98%。将先进的 XAI 技术与基于集成的深度学习 (DL) 方法相结合，作为 AIoMT 的应用，可以实现精确可靠的脑肿瘤诊断。]]></description>
      <guid>https://arxiv.org/abs/2403.04130</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>迈向基于学习的规划：现实世界自动驾驶的 nuPlan 基准</title>
      <link>https://arxiv.org/abs/2403.04133</link>
      <description><![CDATA[arXiv:2403.04133v1 公告类型：新
摘要：机器学习（ML）已经取代了自动驾驶汽车中感知和预测的传统手工方法。然而，对于同样重要的规划任务，基于机器学习的技术的采用进展缓慢。我们推出 nuPlan，世界上第一个真实世界的自动驾驶数据集和基准。该基准旨在测试基于机器学习的规划人员处理不同驾驶情况并做出安全高效决策的能力。为此，我们引入了一个新的大规模数据集，其中包含来自 4 个城市（拉斯维加斯、波士顿、匹兹堡和新加坡）的 1282 小时的不同驾驶场景，并包括高质量的自动标记物体轨迹和交通灯数据。我们详尽地挖掘和分类了评估过程中使用的常见和罕见的驾驶场景，以获得对规划器的性能和特征的细粒度洞察。除了数据集之外，我们还提供了一个模拟和评估框架，使规划者的行为能够在闭环中进行模拟，以考虑与其他交通参与者的交互。我们对大量基线进行了详细分析，并调查了基于机器学习的方法与传统方法之间的差距。在 nuplan.org 上查找 nuPlan 数据集和代码。]]></description>
      <guid>https://arxiv.org/abs/2403.04133</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>使用专家组合进行视频关系检测</title>
      <link>https://arxiv.org/abs/2403.03994</link>
      <description><![CDATA[arXiv:2403.03994v1 公告类型：新
摘要：通过神经网络机器理解图像和视频中的视觉信息面临两个主要挑战。首先，在连接视觉和语言方面存在计算和推理差距，使得很难准确确定给定代理作用于哪个对象并通过语言表示它。其次，由单个整体神经网络训练的分类器通常缺乏稳定性和泛化性。为了克服这些挑战，我们引入了 MoE-VRD，这是一种利用专家组合进行视觉关系检测的新颖方法。 MoE-VRD 以&lt;主语、谓语、宾语&gt;元组的形式识别语言三元组，以从视觉处理中提取关系。利用视觉关系检测方面的最新进展，MoE-VRD 满足了在建立主体（行动）和客体（被行动）之间的关系时动作识别的要求。与单个整体网络相比，MoE-VRD 采用多个小型模型作为专家，其输出被聚合。 MoE-VRD 的每位专家都专门从事视觉关系学习和对象标记。通过利用专家的稀疏门控混合，MoE-VRD 可以实现条件计算，并在不增加计算复杂性的情况下显着增强神经网络容量。我们的实验结果表明，与最先进的方法相比，混合专家方法的条件计算能力和可扩展性在视觉关系检测方面具有卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.03994</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>LoDisc：学习全局-局部判别特征以实现自监督细粒度视觉识别</title>
      <link>https://arxiv.org/abs/2403.04066</link>
      <description><![CDATA[arXiv:2403.04066v1 公告类型：新
摘要：自监督对比学习策略因其在表征学习方面的卓越能力而受到广泛关注。然而，当前的对比学习倾向于学习图像的全局粗粒度表示，这有利于通用对象识别，而这种粗粒度特征不足以进行细粒度视觉识别。在本文中，我们提出通过纯粹的自监督全局-局部细粒度对比学习框架，将微妙的局部细粒度特征学习融入全局自监督对比学习中。具体来说，提出了一种称为局部判别（LoDisc）的新颖借口任务，以明确监督自监督模型对局部关键区域的关注，这些区域是通过简单但有效的位置掩模采样策略捕获的。我们表明，局部判别借口任务可以有效增强重要局部区域的细粒度线索，并且全局局部框架进一步细化图像的细粒度特征表示。对不同细粒度目标识别任务的大量实验结果表明，所提出的方法可以在不同的评估设置中取得不错的改进。同时，所提出的方法在一般目标识别任务中也有效。]]></description>
      <guid>https://arxiv.org/abs/2403.04066</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>一种以数据为中心的方法来解决图像数据增强中特定类别的偏差</title>
      <link>https://arxiv.org/abs/2403.04120</link>
      <description><![CDATA[arXiv:2403.04120v1 公告类型：新
摘要：数据增强（DA）增强了计算机视觉中的模型泛化，但可能会引入偏差，从而不均匀地影响类别准确性。我们的研究扩展了这一调查，通过随机裁剪检查 DA 在各种数据集（包括与 ImageNet 不同的数据集）中的特定类别偏差。我们使用 ResNet50、EfficientNetV2S 和 SWIN ViT 评估了这种现象，发现虽然残差模型表现出类似的偏差效应，但 Vision Transformers 表现出更大的鲁棒性或改变的动态。这表明了一种细致入微的模型选择方法，强调减轻偏差。我们还改进了“数据增强鲁棒性探索”方法，以更有效地管理 DA 引起的偏差，显着减少计算需求（训练 112 个模型而不是 1860 个；减少因子 16.2），同时仍然捕获基本偏差趋势。]]></description>
      <guid>https://arxiv.org/abs/2403.04120</guid>
      <pubDate>Fri, 08 Mar 2024 06:16:36 GMT</pubDate>
    </item>
    </channel>
</rss>