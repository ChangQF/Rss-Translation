<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 21 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>视频二值化的神经形态协同</title>
      <link>https://arxiv.org/abs/2402.12644</link>
      <description><![CDATA[arXiv:2402.12644v1 公告类型：新
摘要：双峰对象，例如用于相机校准的棋盘图案、用于对象跟踪的标记以及路标上的文本等，在我们的日常生活中普遍存在，并且作为一种视觉形式来嵌入可以轻松获取的信息。被视觉系统识别。虽然强度图像的二值化对于提取双峰对象中的嵌入信息至关重要，但之前很少有工作考虑由于视觉传感器和环境之间的相对运动而导致的模糊图像的二值化任务。模糊的图像可能会导致二值化质量下降，从而降低视觉系统运行的下游应用的性能。最近，神经形态相机提供了减轻运动模糊的新功能，但首先对图像进行去模糊然后以实时方式二值化并非易事。在这项工作中，我们提出了一种基于事件的二值重建方法，该方法利用双峰目标属性的先验知识在事件空间和图像空间中独立地执行推理，并将两个域的结果合并以生成清晰的二值图像。我们还开发了一种有效的集成方法来将该二进制图像传播到高帧率二进制视频。最后，我们开发了一种新方法来自然地融合事件和图像以进行无监督阈值识别。所提出的方法在公开可用的和我们收集的数据序列中进行了评估，并表明所提出的方法可以优于 SOTA 方法，可以在纯 CPU 设备上实时生成高帧率二进制视频。]]></description>
      <guid>https://arxiv.org/abs/2402.12644</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>DiffusionNOCS：管理 Sim2Real 多模态类别级姿势估计中的对称性和不确定性</title>
      <link>https://arxiv.org/abs/2402.12647</link>
      <description><![CDATA[arXiv:2402.12647v1 公告类型：新
摘要：本文解决了类别级姿态估计的挑战性问题。当前用于此任务的最先进方法在处理对称对象以及尝试仅通过合成数据训练推广到新环境时面临挑战。在这项工作中，我们通过提出一种概率模型来解决这些挑战，该模型依赖于扩散来估计对于恢复部分对象形状以及建立姿态估计所必需的对应关系至关重要的密集规范图。此外，我们引入了关键组件，通过利用具有多模态输入表示的扩散模型的强度来提高性能。我们通过在一系列真实数据集上进行测试来证明我们的方法的有效性。尽管仅根据我们生成的合成数据进行训练，但我们的方法实现了最先进的性能和前所未有的泛化质量，超越了基线，甚至超越了那些在目标领域专门训练过的基线。]]></description>
      <guid>https://arxiv.org/abs/2402.12647</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>在 GPS 失效的战场环境中使用立体视觉和深度学习进行基于地标的定位</title>
      <link>https://arxiv.org/abs/2402.12551</link>
      <description><![CDATA[arXiv:2402.12551v1 公告类型：新
摘要：战场环境中的定位越来越具有挑战性，因为 GPS 连接经常被拒绝或不可靠，并且在敌对战场地形中跨无线网络物理部署用于定位的锚节点可能很困难。现有的无距离定位方法依赖于基于无线电的锚点及其平均跳距，这在动态和稀疏无线网络拓扑中受到准确性和稳定性的影响。 SLAM 和视觉里程计等基于视觉的方法使用昂贵的传感器融合技术来生成地图和姿态估计。本文提出了一种仅使用无源相机传感器并考虑自然存在或人工地标作为锚点的非 GPS 战场环境中的定位新颖框架。所提出的方法利用定制校准的立体视觉相机进行距离估计和 YOLOv8s 模型，该模型使用我们用于地标识别的真实数据集进行训练和微调。使用高效的立体匹配算法生成深度图像，并通过利用地标识别模型预测的边界框提取地标深度特征来确定到地标的距离。然后使用有效的最小二乘算法获得未知节点的位置，然后使用L-BFGS-B（用于边界约束优化的有限内存拟牛顿代码）方法进行优化。实验结果表明，我们提出的框架比现有的基于锚的 DV-Hop 算法表现更好，并且在定位误差（RMSE）方面与最有效的基于视觉的算法竞争。]]></description>
      <guid>https://arxiv.org/abs/2402.12551</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>用于持续目标检测的高效参数挖掘和冻结</title>
      <link>https://arxiv.org/abs/2402.12624</link>
      <description><![CDATA[arXiv:2402.12624v1 公告类型：新
摘要：持续对象检测对于智能代理在现实环境中主动与人类交互至关重要。虽然参数隔离策略在持续学习分类的背景下得到了广泛的探索，但它们尚未完全用于增量对象检测场景。从先前专注于挖掘单个神经元响应的研究中汲取灵感，并整合神经剪枝最新发展的见解，我们提出了有效的方法来确定哪些层对于网络在连续更新中维持检测器的性能最重要。所提出的研究结果强调了层级参数隔离在促进对象检测模型中的增量学习方面的巨大优势，为未来在现实场景中的研究和应用提供了有希望的途径。]]></description>
      <guid>https://arxiv.org/abs/2402.12624</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>YOLO-Ant：采用深度可分离卷积和大内核设计的轻量级检测器，用于天线干扰源检测</title>
      <link>https://arxiv.org/abs/2402.12641</link>
      <description><![CDATA[arXiv:2402.12641v1 公告类型：新
摘要：在5G通信时代，去除影响通信的干扰源是一项资源密集型任务。计算机视觉的快速发展使得无人机能够执行各种高空探测任务。由于天线干扰源的物体检测领域尚未得到充分探索，该行业缺乏专门针对该特定任务的学习样本和检测模型。本文创建了一个天线数据集来解决重要的天线干扰源检测问题，并作为后续研究的基础。我们推出 YOLO-Ant，这是一种轻量级 CNN 和变压器混合检测器，专为天线干扰源检测而设计。具体来说，我们最初对网络深度和宽度进行了轻量级设计，确保后续研究在轻量级框架内进行。然后，我们提出了基于深度可分离卷积和大卷积核的DSLK-Block模块来增强网络的特征提取能力，有效提高小目标检测。为了解决天线检测中复杂背景和较大类间差异等挑战，我们构建了 DSLKVit-Block，这是一个结合了 DSLK-Block 和 Transformer 结构的强大特征提取模块。考虑到其轻量级设计和准确性，我们的方法不仅在天线数据集上实现了最佳性能，而且还在公共数据集上产生了有竞争力的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.12641</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>为多尺度计算机视觉设计高性能网络</title>
      <link>https://arxiv.org/abs/2402.12536</link>
      <description><![CDATA[arXiv:2402.12536v1 公告类型：新
摘要：自从深度学习出现以来，计算机视觉领域蓬勃发展，模型在越来越复杂的任务上快速改进。我们区分了改进计算机视觉模型的三种主要方法：（1）通过例如在大型、更多样化的数据集上进行训练来改进数据方面，（2）通过例如设计更好的优化器来改进训练方面，以及（3）改进网络架构（或简称网络）。在本论文中，我们选择改进后者，即改进计算机视觉模型的网络设计。更具体地说，我们研究了用于多尺度计算机视觉任务的新网络设计，这些任务需要对不同尺度的概念进行预测。这些新网络设计的目标是超越文献中现有的基线设计。我们特别注意确保比较是公平的，确保不同的网络设计使用相同的设置进行训练和评估。代码可在 https://github.com/CedricPicron/DetSeg 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2402.12536</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>专家的多线性混合：通过分解可扩展的专家专业化</title>
      <link>https://arxiv.org/abs/2402.12550</link>
      <description><![CDATA[arXiv:2402.12550v1 公告类型：新
摘要：专家混合（MoE）范式提供了一种强大的方法，可以将难以理解的密集层分解为更小的模块化计算，这些计算通常更适合人类解释、调试和可编辑性。然而，一个主要问题在于扩大专家数量以实现足够细粒度的专业化所需的计算成本。在本文中，我们提出了多线性专家混合（MMoE）层来解决这个问题，重点关注视觉模型。 MMoE 层完全以因式分解的形式对过大的权重张量执行隐式计算。因此，MMoE 既 (1) 避免了流行的“稀疏”MoE 模型中离散专家路由所带来的问题，又 (2) 不会产生“软”MoE 替代方案的限制性高推理时间成本。我们提供了定性和定量证据（分别通过可视化和反事实干预），表明在微调视觉任务的基础模型时缩放 MMoE 层会导致班级级别更专业的专家，同时与参数匹配的线性层的性能保持竞争力同行。最后，我们表明，学习到的专家专业知识进一步促进了 CelebA 属性分类中人口统计偏差的手动校正。我们的 MMoE 模型代码可在 https://github.com/james-oldfield/MMoE 上获取。]]></description>
      <guid>https://arxiv.org/abs/2402.12550</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>改进多对一图像到图像翻译的深度生成模型</title>
      <link>https://arxiv.org/abs/2402.12531</link>
      <description><![CDATA[arXiv:2402.12531v1 公告类型：新
摘要：深度生成模型已应用于图像到图像翻译的多种应用。生成对抗网络和扩散模型已经呈现出令人印象深刻的结果，为这些任务设定了新的最先进的结果。大多数方法在数据集中的不同域之间具有对称设置。这些方法假设所有领域都具有多种模态或只有一种模态。然而，有许多数据集在两个域之间具有多对一的关系。在这项工作中，我们首先引入了彩色 MNIST 数据集和颜色召回分数，它可以为评估多对一翻译模型提供简单的基准。然后，我们引入一种新的非对称框架来改进现有的多对一图像到图像转换的深度生成模型。我们将此框架应用于 StarGAN V2，并表明在无监督和半监督设置中，该新模型的性能在多对一图像到图像转换方面都有所提高。]]></description>
      <guid>https://arxiv.org/abs/2402.12531</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>可扩展的人机点云压缩</title>
      <link>https://arxiv.org/abs/2402.12532</link>
      <description><![CDATA[arXiv:2402.12532v1 公告类型：新
摘要：由于边缘设备的计算能力有限，深度学习推理可能相当昂贵。一种补救措施是通过网络压缩和传输点云数据以供服务器端处理。不幸的是，这种方法可能对网络因素敏感，包括可用的比特率。幸运的是，通过使用机器任务专用编解码器，可以在不牺牲推理精度的情况下降低比特率要求。在本文中，我们提出了一种可扩展的点云数据编解码器，专门用于机器分类任务，同时还提供了人类查看的机制。在所提出的可扩展编解码器中，“基本”比特流支持机器任务，并且“增强”比特流可用于为人类观看提供更好的输入重建性能。我们的架构基于 PointNet++，并在 ModelNet40 数据集上测试其功效。我们展示了比以前的非专用编解码器的显着改进。]]></description>
      <guid>https://arxiv.org/abs/2402.12532</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>神经系统的系统识别：超越图像到动力学建模</title>
      <link>https://arxiv.org/abs/2402.12519</link>
      <description><![CDATA[arXiv:2402.12519v1 公告类型：新
摘要：大量文献将大脑中生物神经元的记录与深度神经网络进行了比较。最终目标是解释深度网络或更好地理解和编码生物神经系统。最近，关于系统识别是否可能以及它能告诉我们多少关于大脑计算的争论一直存在。系统识别可以识别一种模型是否比另一种模型更有效地代表大脑计算。尽管如此，以前的工作并没有考虑时间方面以及深度网络中的视频和动态（例如运动）建模如何在大规模比较中与这些生物神经系统相关。为此，我们提出了一项系统识别研究，重点是在视觉皮层记录方面比较单个图像与视频理解模型。我们的研究包括两组实验；真实环境设置和模拟环境设置。该研究还涵盖 30 多个模型，与之前的工作不同，我们重点关注卷积与基于变压器、单流与双流以及完全与自监督视频理解模型。目标是捕获更多种类的动态建模架构。因此，这标志着首次从神经科学的角度对视频理解模型进行大规模研究。我们的模拟实验结果表明，系统识别在区分图像与视频理解模型方面可以达到一定水平。此外，我们还提供了关于视频理解模型如何预测视觉皮层反应的关键见解；显示视频理解比图像理解模型更好，卷积模型在早中期区域比基于变压器的模型更好，除了多尺度变压器仍然可以很好地预测这些区域，并且双流模型比单流模型更好。]]></description>
      <guid>https://arxiv.org/abs/2402.12519</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的立体密集匹配数据集从航空图像和大规模立体数据集转移的评估</title>
      <link>https://arxiv.org/abs/2402.12522</link>
      <description><![CDATA[arXiv:2402.12522v1 公告类型：新
摘要：密集匹配对于 3D 场景重建至关重要，因为它能够从图像采集中恢复场景 3D 几何形状。基于深度学习 (DL) 的方法在计算机视觉领域的对极立体视差估计的特殊情况下已显示出有效性。基于深度学习的方法在很大程度上取决于训练数据集的质量和数量。然而，在摄影测量界，为真实场景生成真实视差图仍然是一项具有挑战性的任务。为了应对这一挑战，我们提出了一种直接从光探测和测距（LiDAR）和图像生成地面实况视差图的方法，为四个不同区域和两个具有不同分辨率图像的区域的六个航空数据集生成大型且多样化的数据集。我们还在框架中引入了激光雷达到图像的联合配准改进，该框架对遮挡采取了特殊的预防措施，并避免视差插值以避免精度损失。跨具有不同场景类型、图像分辨率和几何配置的数据集评估 11 种密集匹配方法，这些方法在数据集移位中进行了深入研究，GANet 在相同的训练和测试数据下表现最佳，而 PSMNet 在不同数据集上表现出鲁棒性，我们提出了最好的方法使用限制数据集进行训练的策略。我们还将提供数据集和训练模型；更多信息请访问 https://github.com/whuwuteng/Aerial_Stereo_Dataset。]]></description>
      <guid>https://arxiv.org/abs/2402.12522</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>LangXAI：集成大型视觉模型来生成文本解释，以增强视觉感知任务的可解释性</title>
      <link>https://arxiv.org/abs/2402.12525</link>
      <description><![CDATA[arXiv:2402.12525v1 公告类型：新
摘要：LangXAI 是一个将可解释人工智能（XAI）与高级视觉模型集成在一起的框架，可为视觉识别任务生成文本解释。尽管 XAI 取得了进步，但对于人工智能和计算机视觉领域知识有限的最终用户来说，理解差距仍然存在。 LangXAI 通过向最终用户提供基于文本的分类、对象检测和语义分割模型输出解释来解决这个问题。初步结果表明，LangXAI 的可信度得到了增强，跨任务的 BERTScore 很高，为最终用户的视觉任务打造了一个更加透明和可靠的人工智能框架。]]></description>
      <guid>https://arxiv.org/abs/2402.12525</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉导航的封建网络</title>
      <link>https://arxiv.org/abs/2402.12498</link>
      <description><![CDATA[arXiv:2402.12498v1 公告类型：新
摘要：视觉导航遵循人类无需详细地图即可导航的直觉。一种常见的方法是交互式探索，同时构建带有可用于规划的节点图像的拓扑图。最近的变体从被动视频中学习，并且可以使用复杂的社交和语义线索进行导航。然而，需要大量的训练视频，使用大图，并且由于使用里程计而无法看到场景。我们引入了一种使用封建学习的视觉导航新方法，该方法采用由工人代理、中层经理和高层经理组成的层次结构。封建学习范式的关键是，每个级别的代理看到任务的不同方面，并在不同的空间和时间尺度上运行。该框架中开发了两个独特的模块。对于高层管理人员，我们以自我监督的方式学习记忆代理图，以在学习的潜在空间中记录先前的观察结果，并避免使用图表和里程计。对于中层管理人员，我们开发了一个航路点网络，该网络可输出模拟本地导航期间人类航路点选择的中间子目标。该路点网络是使用我们公开提供的一组新的远程操作视频进行预训练的，训练环境与测试环境不同。由此产生的封建导航网络实现了接近 SOTA 的性能，同时为图像目标导航任务提供了一种新颖的无强化学习、无图、无里程计、无度量地图方法。]]></description>
      <guid>https://arxiv.org/abs/2402.12498</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>将 kNN 与基础模型集成以实现适应性强且具有隐私意识的图像分类</title>
      <link>https://arxiv.org/abs/2402.12500</link>
      <description><![CDATA[arXiv:2402.12500v1 公告类型：新
摘要：传统的深度学习模型隐式编码知识，限制了其透明度和适应数据变化的能力。然而，这种适应性对于解决用户数据隐私问题至关重要。我们通过独立于模型权重存储底层训练数据的嵌入来解决这一限制，从而无需重新训练即可实现动态数据修改。具体来说，我们的方法将 $k$-最近邻 ($k$-NN) 分类器与基于视觉的基础模型集成，在自然图像上进行预训练自我监督，增强可解释性和适应性。我们分享以前未发布的基线方法的开源实现以及我们对性能改进的贡献。定量实验证实了跨已建立的基准数据集的改进分类以及该方法对不同医学图像分类任务的适用性。此外，我们还评估了该方法在持续学习和数据删除场景中的稳健性。该方法在弥合基础模型性能和数据隐私相关挑战之间的差距方面展现出了巨大的前景。源代码位于 https://github.com/TobArc/privacy-aware-image-classification-with-kNN。]]></description>
      <guid>https://arxiv.org/abs/2402.12500</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>多模态大语言模型的 (R) 演变：调查</title>
      <link>https://arxiv.org/abs/2402.12451</link>
      <description><![CDATA[arXiv:2402.12451v1 公告类型：新
摘要：连接文本和视觉模式在生成智能中起着至关重要的作用。因此，受大型语言模型成功的启发，大量研究工作致力于多模态大型语言模型（MLLM）的开发。这些模型可以无缝集成视觉和文本模式作为输入和输出，同时提供基于对话的界面和指令跟踪功能。在本文中，我们对最近基于视觉的 MLLM 进行了全面回顾，分析了它们的架构选择、多模态对齐策略和训练技术。我们还对这些模型进行了广泛的任务的详细分析，包括视觉基础、图像生成和编辑、视觉理解和特定领域的应用。此外，我们还编译和描述训练数据集和评估基准，在性能和计算要求方面对现有模型进行比较。总的来说，这项调查全面概述了当前的技术水平，为未来的 MLLM 奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2402.12451</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:04 GMT</pubDate>
    </item>
    </channel>
</rss>