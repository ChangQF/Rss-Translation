<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>JIGMARK：一种增强图像水印以防止扩散模型编辑的黑盒方法</title>
      <link>https://arxiv.org/abs/2406.03720</link>
      <description><![CDATA[arXiv:2406.03720v1 公告类型：新
摘要：在本研究中，我们研究了图像水印对基于扩散模型的图像编辑的脆弱性，这一挑战因访问梯度信息的计算成本和许多扩散模型的闭源性质而加剧。为了解决这个问题，我们引入了 JIGMARK。这种首创的水印技术通过对比学习扩散模型处理和未处理的图像对来增强鲁棒性，而无需直接反向传播扩散过程。我们的评估表明，JIGMARK 在对扩散模型编辑的弹性方面明显优于现有的水印解决方案，在 1% 的误报率下，其真实阳性率是领先基线的三倍多，同时保持了图像质量。同时，它不断提高对其他常规扰动（如 JPEG、模糊等）和恶意水印攻击的鲁棒性，通常比最先进的技术高出很多。此外，我们提出了人类对齐变异 (HAV) 分数，这是一种新的指标，它在量化图像编辑产生的图像衍生物的数量方面超越了传统的相似性测量。]]></description>
      <guid>https://arxiv.org/abs/2406.03720</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>用于文本属性人物搜索的属性感知隐式模态对齐</title>
      <link>https://arxiv.org/abs/2406.03721</link>
      <description><![CDATA[arXiv:2406.03721v1 公告类型：新
摘要：文本属性行人搜索旨在通过给定的文本属性找到特定的行人，这在通过目击者描述搜索指定行人的场景中非常有意义。关键挑战是文本属性和图像之间存在显著的模态差距。以前的方法侧重于通过单模态预训练模型实现显式表示和对齐。然而，这些模型中缺乏模态间对应关系可能会导致模态内局部信息的扭曲。此外，这些方法只考虑了模态间的对齐，而忽略了不同属性类别之间的差异。为了缓解上述问题，我们提出了一个属性感知的隐式模态对齐 (AIMA) 框架来学习文本属性和图像之间局部表示的对应关系，并结合全局表示匹配来缩小模态差距。首先，我们引入 CLIP 模型作为主干，并设计提示模板将属性组合转换为结构化句子。这有助于模型更好地理解和匹配图像细节。接下来，我们设计了一个 Masked Attribute Prediction (MAP) 模块，通过多模态交互预测图像和掩码文本属性特征交互后的掩码属性，从而实现隐式局部关系对齐。最后，我们提出了一种 Attribute-IoU Guided Intra-Modal Contrastive (A-IoU IMC) 损失，将不同文本属性在嵌入空间中的分布与它们的 IoU 分布对齐，实现更好的语义排列。在 Market-1501 Attribute、PETA 和 PA100K 数据集上进行的大量实验表明，我们提出的方法的性能明显超越了目前最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.03721</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>超点高斯分层用于实时高保真动态场景重建</title>
      <link>https://arxiv.org/abs/2406.03697</link>
      <description><![CDATA[arXiv:2406.03697v1 公告类型：新
摘要：在动态场景中渲染新视图图像是一项至关重要但具有挑战性的任务。当前的方法主要使用基于 NeRF 的方法来表示静态场景，并使用额外的时变 MLP 来模拟场景变形，导致渲染质量相对较低且推理速度较慢。为了应对这些挑战，我们提出了一种名为超点高斯溅射（SP-GS）的新框架。具体而言，我们的框架首先使用显式 3D 高斯来重建场景，然后将具有相似属性（例如旋转、平移和位置）的高斯聚类为超点。在这些超点的支持下，我们的方法成功地将 3D 高斯溅射扩展到动态场景，而计算成本仅略有增加。除了在高分辨率下实现最先进的视觉质量和实时渲染外，超点表示还提供了更强大的操作能力。大量实验证明了我们的方法在合成数据集和真实数据集上的实用性和有效性。请参阅我们的项目页面：https://dnvtmf.github.io/SP_GS.github.io。]]></description>
      <guid>https://arxiv.org/abs/2406.03697</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>DSNet：在语义分割中使用空洞卷积的新方法</title>
      <link>https://arxiv.org/abs/2406.03702</link>
      <description><![CDATA[arXiv:2406.03702v1 公告类型：新
摘要：在语义分割任务中，空洞卷积被用作增加感受野的方法。然而，在之前的语义分割工作中，它很少用于模型的浅层。我们重新审视了现代卷积神经网络 (CNN) 中空洞卷积的设计，并证明了使用大核应用空洞卷积的概念可能是一个更强大的范例。我们提出了三个更有效地应用空洞卷积的指导方针。遵循这些指导方针，我们提出了双分支 CNN 架构 DSNet，它在模型架构的浅层中结合了空洞卷积，并在 ImageNet 上对几乎整个编码器进行预训练以实现更好的性能。为了证明我们方法的有效性，我们的模型在 ADE20K、Cityscapes 和 BDD 数据集上实现了准确度和速度之间的新平衡。具体来说，DSNet 在 ADE20K 上实现了 40.0% mIOU，推理速度为 179.2 FPS，在 Cityscapes 上实现了 80.4% mIOU，速度为 81.9 FPS。源代码和模型可在 Github 上找到：https://github.com/takaniwa/DSNet。]]></description>
      <guid>https://arxiv.org/abs/2406.03702</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>设计强大的远程人脸反欺骗系统的原则</title>
      <link>https://arxiv.org/abs/2406.03684</link>
      <description><![CDATA[arXiv:2406.03684v1 公告类型：新
摘要：保护人脸数字身份免受各种攻击媒介的侵害至关重要，而人脸反欺骗在这一努力中起着至关重要的作用。当前的方法主要侧重于检测单个帧内的欺骗尝试以检测演示攻击。然而，能够实时操作的超现实生成模型的出现增加了数字生成攻击的风险。鉴于这些不断演变的威胁，本文旨在解决两个关键方面。首先，它揭示了最先进的人脸反欺骗方法在抵御数字攻击方面的脆弱性。其次，它介绍了人脸反欺骗系统中遇到的常见威胁的全面分类。通过一系列实验，我们展示了当前人脸反欺骗检测技术的局限性及其无法推广到新的数字攻击场景。值得注意的是，现有模型难以应对数字注入攻击，包括对抗性噪声、逼真的深度伪造攻击和数字重放攻击。为了帮助设计和实施能够抵御这些新出现的漏洞的强大的人脸反欺骗系统，本文提出了从模型准确性和稳健性到管道稳健性甚至平台稳健性的关键设计原则。特别是，我们建议使用主动传感器实施主动人脸反欺骗系统，以显著降低看不见的攻击媒介的风险并改善用户体验。]]></description>
      <guid>https://arxiv.org/abs/2406.03684</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>用于快照压缩成像的未训练神经网络：理论与算法</title>
      <link>https://arxiv.org/abs/2406.03694</link>
      <description><![CDATA[arXiv:2406.03694v1 公告类型：新
摘要：快照压缩成像 (SCI) 从单个 2D 测量中恢复高维 (3D) 数据立方体，使视频和高光谱成像等各种应用在采集速度和效率方面超越标准技术。在本文中，我们重点介绍使用未经训练的神经网络 (UNN)（例如深度图像先验 (DIP)）来建模源结构的 SCI 恢复算法。这种基于 UNN 的方法很有吸引力，因为它们有可能避免不同源模型和不同测量场景所需的计算密集型再训练。我们首先开发一个理论框架来表征这种基于 UNN 的方法的性能。该理论框架一方面使我们能够优化数据调制掩模的参数，另一方面，为可以从单次测量中恢复的数据帧数量与未经训练的 NN 的参数之间提供了基本联系。我们还采用了最近提出的 bagged-deep-image-prior (bagged-DIP) 思想来开发 SCI Bagged Deep Video Prior (SCI-BDVP) 算法，以解决标准 UNN 解决方案面临的常见挑战。我们的实验结果表明，在视频 SCI 中，我们提出的解决方案在 UNN 方法中达到了最佳水平，并且在噪声测量的情况下，它甚至优于监督解决方案。]]></description>
      <guid>https://arxiv.org/abs/2406.03694</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>基于冰图的海冰分类局部标签学习与焦点损失</title>
      <link>https://arxiv.org/abs/2406.03645</link>
      <description><![CDATA[arXiv:2406.03645v1 公告类型：新
摘要：海冰对北极和地球气候至关重要，需要持续监测和高分辨率测绘。然而，手动测绘海冰既费时又主观，这促使人们需要基于深度学习的自动分类方法。然而，训练这些算法具有挑战性，因为通常用作训练数据的专家生成的冰图不会映射单一冰类型，而是映射具有多种冰类型的多边形。此外，这些图表中各种冰类型的分布经常不平衡，导致性能偏向主导类别。在本文中，我们提出了一种新颖的 GeoAI 方法来训练海冰分类，将其形式化为具有明确置信度分数的部分标签学习任务，以解决多个标签和类别不平衡问题。我们将多边形级标签视为候选部分标签，将相应的冰浓度作为置信度分数分配给每个候选标签，并将它们与焦点损失相结合以训练卷积神经网络 (CNN)。我们提出的方法提高了 Sentinel-1 双极化 SAR 图像中海冰分类的性能，与使用独热编码标签和分类交叉熵损失的传统训练方法相比，提高了分类准确率（从 87% 提高到 92%）和加权平均 F-1 分数（从 90% 提高到 93%）。它还提高了 6 个海冰类别中的 4 个的 F-1 分数。]]></description>
      <guid>https://arxiv.org/abs/2406.03645</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:56 GMT</pubDate>
    </item>
    <item>
      <title>CVPR 2024 PVUW 研讨会 MOSE 赛道第三名解决方案：复杂视频对象分割</title>
      <link>https://arxiv.org/abs/2406.03668</link>
      <description><![CDATA[arXiv:2406.03668v1 公告类型：新
摘要：视频对象分割 (VOS) 是计算机视觉中一项重要的任务，重点是区分视频帧中的前景对象和背景。我们的工作从 Cutie 模型中汲取灵感，研究了对象记忆、记忆帧总数和输入分辨率对分割性能的影响。本报告验证了我们的推理方法在复杂视频对象分割 (MOSE) 数据集上的有效性，该数据集具有复杂的遮挡。我们的实验结果表明，我们的方法在测试集上获得了 0.8139 的 J\&amp;F 分数，在最终排名中名列第三。这些发现凸显了我们的方法在处理具有挑战性的 VOS 场景时的稳健性和准确性。]]></description>
      <guid>https://arxiv.org/abs/2406.03668</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:56 GMT</pubDate>
    </item>
    <item>
      <title>Hi5：无需人体注释的 2D 手势估计</title>
      <link>https://arxiv.org/abs/2406.03599</link>
      <description><![CDATA[arXiv:2406.03599v1 公告类型：新
摘要：我们提出了一个新的大型合成手势估计数据集 Hi5，以及一种新颖的廉价方法来收集高质量合成数据，无需人工注释或验证。利用计算机图形学的最新进展、具有不同性别和肤色的高保真 3D 手部模型以及动态环境和相机运动，我们的数据合成管道可以精确控制数据多样性和表示，确保模型训练稳健且公平。我们使用一台消费者 PC 生成一个包含 583,000 张图像的数据集，并带有准确的姿势注释，该数据集与现实世界的变化非常接近。使用 Hi5 训练的姿势估计模型在真实手部基准上表现出色，同时在遮挡和扰动测试中超越了使用真实数据训练的模型。我们的实验表明，合成数据是解决真实数据集中数据表示问题的可行解决方案，效果令人鼓舞。总体而言，本文提供了一种有前途的合成数据创建和注释新方法，可以降低成本并提高手势估计数据的多样性和质量。]]></description>
      <guid>https://arxiv.org/abs/2406.03599</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>自由度很重要：从点轨迹推断动力学</title>
      <link>https://arxiv.org/abs/2406.03625</link>
      <description><![CDATA[arXiv:2406.03625v1 公告类型：新
摘要：理解通用 3D 场景的动态在计算机视觉中具有根本挑战性，对于增强与场景重建、运动跟踪和化身创建相关的应用至关重要。在这项工作中，我们将任务作为推断 3D 点的密集、长距离运动的问题。通过观察一组点轨迹，我们旨在学习由神经网络参数化的隐式运动场，以预测同一域内新点的运动，而不依赖于任何数据驱动或场景特定的先验。为了实现这一点，我们的方法建立在最近引入的动态点场模型之上，该模型学习规范帧和单个观察帧之间的平滑变形场。然而，连续帧之间的时间一致性被忽略，并且由于每帧建模，所需参数的数量随序列长度线性增加。为了解决这些缺点，我们利用 SIREN 提供的内在正则化，并修改输入层以产生时空平滑的运动场。此外，我们分析了运动场雅可比矩阵，发现点周围无穷小区域中的运动自由度 (DOF) 和网络隐藏变量具有不同的行为来影响模型的表示能力。这使我们能够在保持模型紧凑性的同时提高模型表示能力。此外，为了降低过度拟合的风险，我们引入了一个基于分段运动平滑假设的正则化项。我们的实验评估了该模型在预测未见点轨迹方面的性能及其在引导下的时间网格对齐中的应用。结果证明了其优越性和有效性。该项目的代码和数据已公开：\url{https://yz-cnsdqz.github.io/eigenmotion/DOMA/}]]></description>
      <guid>https://arxiv.org/abs/2406.03625</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>通过食物理解扩散概念代数的局限性</title>
      <link>https://arxiv.org/abs/2406.03582</link>
      <description><![CDATA[arXiv:2406.03582v1 公告类型：新
摘要：近年来，图像生成技术，尤其是潜在扩散模型，人气飙升。已经开发了许多技术来操纵和澄清这些大型模型学习的语义概念，为偏见和概念关系提供了关键见解。然而，这些技术通常只在人类或动物面孔和艺术风格转变的传统领域得到验证。食品领域通过复杂的构图和区域偏见带来了独特的挑战，这可以揭示现有方法的局限性和机遇。通过食物图像的视角，我们分析了概念遍历技术中的定性和定量模式。我们揭示了模型捕捉和表现烹饪多样性细微差别的能力的可衡量见解，同时还确定了模型偏见和局限性出现的领域。]]></description>
      <guid>https://arxiv.org/abs/2406.03582</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:54 GMT</pubDate>
    </item>
    <item>
      <title>CountCLIP -- [重新] 教 CLIP 数到十</title>
      <link>https://arxiv.org/abs/2406.03586</link>
      <description><![CDATA[arXiv:2406.03586v1 公告类型：新
摘要：大型视觉语言模型 (VLM) 被证明可以学习丰富的联合图像文本表示，从而能够在相关的下游任务中实现高性能。然而，它们未能展示对物体的定量理解，并且缺乏良好的计数感知表示。本文对“教 CLIP 数到十”（Paiss 等人，2023 年）进行了可重复性研究，该研究提出了一种微调 CLIP 模型（Radford 等人，2021 年）的方法，通过引入计数对比损失项来提高图像中的零样本计数准确度，同时保持零样本分类的性能。我们用较低的计算资源提高了模型在较小训练数据子集上的性能。我们通过使用我们自己的代码重现他们的研究来验证这些说法。实现可以在 https://github.com/SforAiDl/CountCLIP 找到。]]></description>
      <guid>https://arxiv.org/abs/2406.03586</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:54 GMT</pubDate>
    </item>
    <item>
      <title>VideoPhy：评估视频生成的物理常识</title>
      <link>https://arxiv.org/abs/2406.03520</link>
      <description><![CDATA[arXiv:2406.03520v1 公告类型：新
摘要：互联网规模视频数据预训练的最新进展促成了文本到视频生成模型的发展，该模型可以在广泛的视觉概念和风格中创建高质量的视频。由于它们能够合成逼真的动作并渲染复杂的物体，这些生成模型有可能成为物理世界的通用模拟器。然而，目前尚不清楚我们距离现有的文本到视频生成模型这一目标还有多远。为此，我们提出了 VideoPhy，这是一个基准，旨在评估生成的视频是否遵循现实世界活动的物理常识（例如，当将弹珠放在倾斜的表面上时，它会滚下来）。具体来说，我们整理了一份包含 688 个字幕的列表，这些字幕涉及物理世界中各种材料类型之间的相互作用（例如，固体-固体、固体-流体、流体-流体）。然后，我们根据这些字幕生成视频，这些字幕来自各种最先进的文本转视频生成模型，包括开放模型（例如 VideoCrafter2）和封闭模型（例如 Google 的 Lumiere、Pika）。此外，我们的人工评估表明，现有模型严重缺乏生成符合给定文本提示的视频的能力，同时也缺乏物理常识。具体来说，性能最佳的模型 Pika 生成的视频仅在 19.7% 的情况下符合字幕和物理定律。因此，VideoPhy 强调视频生成模型远不能准确模拟物理世界。最后，我们还用自动评估器 VideoCon-Physics 补充了数据集，以大规模评估语义遵循和物理常识。]]></description>
      <guid>https://arxiv.org/abs/2406.03520</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:53 GMT</pubDate>
    </item>
    <item>
      <title>Npix2Cpix：基于 GAN 的图像到图像转换网络，具有检索-分类集成功能，可用于从历史文档图像中检索水印</title>
      <link>https://arxiv.org/abs/2406.03556</link>
      <description><![CDATA[arXiv:2406.03556v1 公告类型：新
摘要：古代水印的识别和恢复长期以来一直是古抄本学和历史学的主要课题。由于水印的多样性、拥挤和嘈杂的样本、多种表示模式以及类间和类内变化的细微区别，基于水印对历史文献进行分类可能很困难。本文提出了一种基于 U-net 的条件生成对抗网络 (GAN)，将嘈杂的原始历史水印图像转换为仅带有水印的干净、无手写图像。考虑到它能够将退化（嘈杂）像素转换为干净像素，所提出的网络被称为 Npix2Cpix。所提出的网络没有使用直接退化的带水印图像，而是使用对抗性学习进行图像到图像的转换，首次创建杂乱无章且无手写图像，以恢复和分类水印。为了学习从输入噪声图像到输出干净图像的映射，使用两个单独的损失函数训练了基于 U-net 的 GAN 的生成器和鉴别器，每个损失函数都基于图像之间的距离。在使用所提出的 GAN 对噪声水印图像进行预处理后，使用基于 Siamese 的一次性学习对水印进行分类。根据在大规模历史水印数据集上的实验结果，从污染图像中提取水印可以产生较高的一次性分类准确率。对检索到的水印的定性和定量评估说明了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.03556</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:53 GMT</pubDate>
    </item>
    <item>
      <title>通过定制数据增强增强交通标志识别：解决类别不平衡和实例稀缺问题</title>
      <link>https://arxiv.org/abs/2406.03576</link>
      <description><![CDATA[arXiv:2406.03576v1 公告类型：新
摘要：本文解决了交通标志识别（TSR）中的关键挑战，这对于道路安全至关重要——具体来说，就是数据集中的类别不平衡和实例稀缺性。我们引入了定制的数据增强技术，包括合成图像生成、几何变换和一种新颖的基于障碍物的增强方法，以提高数据集质量，从而提高模型的鲁棒性和准确性。我们的方法结合了多种增强过程，以准确模拟现实世界的情况，从而扩大了训练数据的多样性和代表性。我们的研究结果表明 TSR 模型性能有了显着提高，对交通标志识别系统具有重要意义。这项研究不仅解决了 TSR 中的数据集限制，还提出了一种针对不同地区和应用的类似挑战的模型，标志着计算机视觉和交通标志识别系统领域的进步。]]></description>
      <guid>https://arxiv.org/abs/2406.03576</guid>
      <pubDate>Fri, 07 Jun 2024 06:20:53 GMT</pubDate>
    </item>
    </channel>
</rss>