<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Fri, 08 Dec 2023 03:14:49 GMT</lastBuildDate>
    <item>
      <title>使用事件摄像机进行低功耗、连续远程行为定位。 （arXiv：2312.03799v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03799</link>
      <description><![CDATA[自然科学研究人员需要可靠的方法来量化动物
行为。最近，出现了许多计算机视觉方法来自动化
过程。然而，在偏远地区观察野生物种仍然是一个难题
由于照明条件困难和电力限制，任务具有挑战性
供应和数据存储。事件摄像机具有独特的优势
由于其低功耗和高可靠性，依赖电池的远程监控
动态范围能力。我们使用这种新颖的传感器来量化行为
帽带企鹅称为欣喜若狂的展示。我们将问题表述为
时间动作检测任务，确定动作的开始和结束时间
行为。为此，我们记录了一群正在繁殖的企鹅
南极洲几周的时间，并标记了 16 个巢穴的事件数据。这
开发的方法由候选时间间隔生成器组成
（提案）以及其中动作的分类器。实验表明
事件摄像机对运动的自然响应对于连续
行为监控和检测，达到平均精度（mAP）
58%（天气好的情况下会增加到 63%）。结果还
展示对包含在各种照明条件下的鲁棒性
具有挑战性的数据集。事件相机的低功耗功能允许
记录时间比传统相机长三倍。这项工作开创了先河
使用事件摄像机进行远程野生动物观察，开辟了新的领域
跨学科的机会。 https://tub-rip.github.io/eventpenguins/
]]></description>
      <guid>http://arxiv.org/abs/2312.03799</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:49 GMT</pubDate>
    </item>
    <item>
      <title>AnimatableDreamer：文本引导的非刚性 3D 模型生成和重建以及规范分数蒸馏。 （arXiv：2312.03795v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03795</link>
      <description><![CDATA[文本到 3D 模型的改编具有先进的静态 3D 模型质量，但是
顺序 3D 模型生成，特别是对于具有较大尺寸的可动画对象
动议，仍然稀缺。我们的工作提出了 AnimatableDreamer，一种文本转 4D
生成框架能够生成不同类别的非刚性
物体，同时遵循从单眼视频中提取的物体运动。
AnimatableDreamer 的核心配备了我们新颖的优化设计
称为规范分数蒸馏（CSD），它简化了生成
通过在时变中的不同帧上进行去噪，将维度从 4D 变为 3D
相机空间，同时以独特的规范进行蒸馏过程
每个视频共享的空间。具体来说，CSD 确保对梯度进行评分
通过可微的扭曲反向传播到规范空间，因此
保证生成时间一致并保持形态
不同姿势的合理性。通过将 3D 生成器提升到 4D
扭曲函数，AnimatableDreamer 提供了非刚性 3D 的新颖视角
模型生成和重建。此外，通过归纳知识
多视图一致扩散模型，CSD 正则化重建
新颖的观点，从而循环增强生成过程。广泛的
实验证明了我们的方法在生成
来自单目视频的高灵活性文本引导 3D 模型，同时还
与典型的非刚性相比，显示出改进的重建性能
重建方法。项目页面 https://AnimatableDreamer.github.io。
]]></description>
      <guid>http://arxiv.org/abs/2312.03795</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:48 GMT</pubDate>
    </item>
    <item>
      <title>利用反射强度先验知识去除单幅图像反射。 （arXiv：2312.03798v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03798</link>
      <description><![CDATA[现实世界图像中的单图像反射消除 (SIRR) 是一项具有挑战性的任务
由于玻璃表面上发生各种图像退化而导致的任务
光的透射和反射。许多现有方法依赖于特定的先验
解决问题的假设。在本文中，我们提出了一个通用的
反射强度先验，捕获反射强度
现象并证明其有效性。了解反射强度
之前，我们介绍了反射先验提取网络（RPEN）。经过
将图像分割成区域块，RPEN 学习非均匀反射
图像中的先验。我们提出基于先验的反射去除网络（PRRN）
使用简单的 Transformer U-Net 架构，适应反射先验馈送
来自 RPEN。现实世界基准的实验结果表明
我们的方法在 SIRR 中实现最先进的准确性的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2312.03798</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:48 GMT</pubDate>
    </item>
    <item>
      <title>通过半径分布正交成本量实现内存高效光流。 （arXiv：2312.03790v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03790</link>
      <description><![CDATA[循环全对场变换 (RAFT) 中的完整 4D 成本量或
Transformer 的全局匹配实现了令人印象深刻的光流性能
估计。然而，它们的内存消耗随着
输入分辨率，使得它们对于高分辨率图像不切实际。在
在本文中，我们提出了 MeFlow，一种新颖的内存高效方法
高分辨率光流估计。 MeFlow的关键是循环本地
正交成本体积表示，分解二维搜索空间
动态地进入两个一维正交空间，使我们的方法能够扩展
有效地处理非常高分辨率的输入。保存重要信息
在正交空间中，我们利用自注意力来传播特征
信息从二维空间到正交空间。我们进一步提出一个
半径分布多尺度查找策略来建模对应关系
以可忽略不计的成本实现大位移。我们验证效率并
我们的方法在具有挑战性的 Sintel 和 KITTI 基准测试中的有效性，以及
真实世界的 4K ($2160\!\times\!3840$) 图像。我们的方法具有竞争力
Sintel 和 KITTI 基准测试中的性能，同时保持最高
高分辨率输入的内存效率。
]]></description>
      <guid>http://arxiv.org/abs/2312.03790</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:47 GMT</pubDate>
    </item>
    <item>
      <title>AnimateZero：视频扩散模型是零镜头图像动画师。 （arXiv：2312.03793v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03793</link>
      <description><![CDATA[大规模文本到视频（T2V）扩散模型在
近年来在视觉质量、运动和时间一致性方面取得了进步。
然而，生成过程仍然是一个黑匣子，其中所有属性
（例如，外观、运动）是联合学习和生成的，没有精确的
除了粗略的文字描述之外的控制能力。受到图像动画的启发
它将视频作为一种特定的外观与相应的外观解耦
运动，我们建议 AnimateZero 推出预训练的文本到视频
扩散模型，即 AnimateDiff，并提供更精确的外观和
它的运动控制能力。对于外观控制，我们借用中间体
文本到图像（T2I）生成的潜在特征及其特征，以确保
生成的第一帧等于给定的生成图像。对于时间
控制，我们替换了原始T2V模型的全局时间注意力
使用我们提出的位置校正窗口注意力来确保其他帧
与第一帧对齐。在所提出的方法的支持下，AnimateZero
无需进一步训练即可成功控制生成进度。作为一个
针对给定图像的零镜头图像动画制作器，AnimateZero 还支持多个
新应用，包括交互式视频生成和真实图像
动画片。详细的实验证明了该方法的有效性
T2V 和相关应用中提出的方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.03793</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:47 GMT</pubDate>
    </item>
    <item>
      <title>Lite-Mind：迈向高效且多功能的大脑表示网络。 （arXiv：2312.03781v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03781</link>
      <description><![CDATA[解码大脑视觉信息的研究，特别是通过
非侵入性功能磁共振成像方法正在迅速发展。挑战来自于
有限的数据可用性和功能磁共振成像的低信噪比
信号，导致功能磁共振成像到图像检索的低精度任务。
最先进的 MindEye 显着改善了 fMRI 到图像的检索
通过利用具有高参数计数阶数的深度 MLP 来提高性能
幅度，即每个受试者 996M MLP 主干，以将 fMRI 嵌入与
CLIP 视觉转换器的最后一个隐藏层。然而，显着
即使在相同的实验中，受试者之间也存在个体差异
设置，要求训练特定主题的模型。实质性的
参数对在实际应用中部署功能磁共振成像解码提出了重大挑战
设备，特别是每个主题都需要特定的模型。
为此，我们提出了 Lite-Mind，一种轻量级、高效且多功能的
基于离散傅里叶变换的大脑表示网络
有效地将 fMRI 体素与 CLIP 的细粒度信息对齐。我们的
实验表明 Lite-Mind 达到了令人印象深刻的 94.3%
受试者 1 在 NSD 数据集上的 fMRI 到图像检索准确度为 98.7%
参数比 MindEye 少。 Lite-Mind 也被证明能够
迁移到较小的大脑数据集并建立了新的最先进的
GOD 数据集上的零样本分类。该代码可在
https://github.com/gongzix/Lite-Mind。
]]></description>
      <guid>http://arxiv.org/abs/2312.03781</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:46 GMT</pubDate>
    </item>
    <item>
      <title>新颖的类发现符合 3D 语义分割的基础模型。 （arXiv：2312.03782v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03782</link>
      <description><![CDATA[语义分割中的新类发现（NCD）任务需要
训练一个能够准确分割未标记（新）类的模型，依赖
关于来自带注释的（基）类的可用监督。虽然
在 2D 图像数据中进行了广泛的研究，将 NCD 任务扩展到
3D 点云领域代表了一项开创性的努力，其特点是
二维情况下不存在的假设和挑战。这张纸
代表了点云数据分析在四个方面的进步
方向。首先介绍了点云NCD的新任务
语义分割。其次，它证明了直接转置
现有的唯一用于 2D 图像语义分割到 3D 数据的 NCD 方法
次优结果。第三，一种基于在线聚类的新NCD方法，
提出了不确定性估计和语义蒸馏。最后附上一本小说
提出评估方案以严格评估非传染性疾病的表现
点云语义分割。通过综合评价
SemanticKITTI、SemanticPOSS 和 S3DIS 数据集，论文演示了
所提出的方法相对于所考虑的基线具有显着的优越性。
]]></description>
      <guid>http://arxiv.org/abs/2312.03782</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:46 GMT</pubDate>
    </item>
    <item>
      <title>FAAC：使用锚框和条件控制生成面部动画，以实现卓越的保真度和可编辑性。 （arXiv：2312.03775v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03775</link>
      <description><![CDATA[近年来，扩散模型促进了重大进步
在视频生成中。然而，人脸视频的创作仍面临挑战
面部保真度低、帧一致性缺乏、受限等问题
可编辑性和无法控制的人体姿势。为了应对这些挑战，我们
介绍一种增强双方人脸身份的面部动画生成方法
保真度和编辑功能，同时确保帧一致性。这
方法结合了锚框架的概念来抵消
原始文本到图像模型的生成能力下降
合并运动模块。我们为此提出了两种策略
目标：免训练和基于训练的锚框方法。我们的方法是
功效已在多个有代表性的 DreamBooth 和 LoRA 上得到验证
模型，在原始结果的基础上提供了实质性的改进
面部保真度、文本到图像的可编辑性和视频运动。此外，我们
使用 3D 参数化人脸模型引入条件控制来捕获
准确的面部动作和表情。该解决方案增强了创意
通过集成生成面部动画的可能性
多个控制信号。如需更多样品，请访问
https://anonymous.4open.science/r/FAAC。
]]></description>
      <guid>http://arxiv.org/abs/2312.03775</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:45 GMT</pubDate>
    </item>
    <item>
      <title>关于大型多模态模型对抗图像对抗攻击的鲁棒性。 （arXiv：2312.03777v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03777</link>
      <description><![CDATA[指令调优的最新进展导致了
最先进的大型多模式模型 (LMM)。鉴于这些的新颖性
模型中，视觉对抗攻击对 LMM 的影响尚未得到研究
彻底检查。我们对稳健性进行了全面的研究
针对不同对抗性攻击的各种 LMM，跨任务进行评估
包括图像分类、图像字幕和视觉问答
（VQA）。我们发现，一般来说，LMM 对于视觉对抗性并不鲁棒
输入。然而，我们的研究结果表明，通过以下方式向模型提供上下文
提示，例如问答对中的问题有助于减轻视觉影响
对抗性输入。值得注意的是，评估的 LMM 表现出了显着的
ScienceQA 任务对此类攻击的抵御能力仅下降了 8.10%
与视觉同类产品相比，性能下降了 99.73%。我们也
提出一种现实世界图像分类的新方法，我们称之为查询
分解。通过将存在查询合并到我们的输入提示中，我们
观察攻击有效性的降低和图像的改善
分类准确率。这项研究强调了以前尚未充分探索的
LMM 稳健性的一个方面，并为未来的工作奠定了基础
加强多式联运系统在敌对环境中的复原力。
]]></description>
      <guid>http://arxiv.org/abs/2312.03777</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:45 GMT</pubDate>
    </item>
    <item>
      <title>DiffusionAtlas：高保真一致的扩散视频编辑。 （arXiv：2312.03772v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03772</link>
      <description><![CDATA[我们提出了一个基于扩散的视频编辑框架，即 DiffusionAtlas，
在编辑视频时可以同时实现帧一致性和高保真度
物体外观。尽管图像编辑取得了成功，扩散模型仍然
由于以下原因，在视频编辑时遇到重大障碍
保持物体外观的时空一致性的挑战
跨帧。另一方面，基于图集的技术允许传播
对分层表示的编辑始终返回到框架。然而，他们
经常很难创建正确遵循的编辑效果
由于编辑的限制，用户提供的文本或视觉条件
固定 UV 映射场上的纹理图集。我们的方法利用
视觉文本扩散模型可直接在扩散上编辑对象
图集，确保跨帧的对象标识一致。我们设计一个损失项
具有基于图集的约束并构建预训练的文本驱动的扩散模型
作为细化形状扭曲和校正纹理的像素级指导
偏差。定性和定量实验表明我们的方法
在实现一致的高保真度方面优于最先进的方法
视频对象编辑。
]]></description>
      <guid>http://arxiv.org/abs/2312.03772</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:44 GMT</pubDate>
    </item>
    <item>
      <title>OctreeOcc：使用八叉树查询进行高效且多粒度的占用预测。 （arXiv：2312.03774v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03774</link>
      <description><![CDATA[近年来，占用预测越来越受到人们的关注。
它对 3D 场景的精细理解。传统方法通常
依赖于密集的、规则的网格表示，这通常会导致过度
小物体的计算需求和空间细节的损失。这
论文介绍了 OctreeOcc，一种创新的 3D 占用预测框架
利用八叉树表示来自适应地捕获有价值的
3D 信息，提供可变粒度以适应对象形状
以及不同大小和复杂性的语义区域。特别是，我们
结合图像语义信息，提高初始精度
八叉树结构并设计有效的整改机制来细化
迭代八叉树结构。我们的广泛评估表明 OctreeOcc 不
不仅超越了占用预测方面最先进的方法，而且
与相比，计算开销减少了 15%-24%
基于密集网格的方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.03774</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:44 GMT</pubDate>
    </item>
    <item>
      <title>源免费开放集域适应的未知样本发现。 （arXiv：2312.03767v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03767</link>
      <description><![CDATA[开放集域适应（OSDA）旨在适应在源上训练的模型
域到经历分布转移并包含的目标域
来自源域之外的新类的样本。无源 OSDA
(SF-OSDA) 技术消除了访问源域样本的需要，但是
当前的 SF-OSDA 方法仅利用目标域中的已知类别
适应，并且需要访问整个目标域，即使在
适应后推理，区分已知和未知
样品。在本文中，我们介绍未知样本发现（USD）作为
SF-OSDA 方法利用时间集成教师模型来进行
已知-未知目标样本分离并使学生模型适应
使用协同训练和时间一致性来确定所有类别的目标域
老师和学生之间。美元提升詹森-香农距离（JSD）
作为已知未知样品分离的有效措施。我们的
师生框架显着减少了由此产生的错误积累
避免不完美的已知与未知样本分离，而课程指导有助于
可靠地学习已知目标和未知目标之间的区别
子空间。 USD 在目标模型上附加了一个未知的类节点，因此
轻松地将目标样本分类为任何已知或未知类别
随后的适应后推理阶段。实证结果表明美元
优于现有的 SF-OSDA 方法，并且与当前的 OSDA 具有竞争力
在适应过程中同时利用源域和目标域的模型。
]]></description>
      <guid>http://arxiv.org/abs/2312.03767</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:43 GMT</pubDate>
    </item>
    <item>
      <title>DreamInpainter：使用扩散模型进行文本引导的主题驱动图像修复。 （arXiv：2312.03771v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03771</link>
      <description><![CDATA[这项研究介绍了文本引导的主题驱动图像修复，这是一部小说
结合文本和示例图像进行图像修复的任务。虽然两者
文本和示例图像在之前的工作中已独立使用，
它们的综合利用仍有待探索。同时包容
由于内在的平衡，这两种情况都构成了重大挑战
可编辑性和主题保真度之间需要。为了应对这一挑战，我们
DreamInpainter 提出了两步走的方法。首先，我们计算密集主题
确保准确的主题复制的功能。然后，我们雇佣了一个
区分性标记选择模块，以消除冗余的主题细节，
保留主体的身份，同时允许根据其他人进行更改
条件，例如蒙版形状和文本提示。此外，我们还引入了一个
解耦正则化技术，以增强存在的文本控制
示例图像。我们广泛的实验证明了卓越的性能
我们的方法在视觉质量、身份保留和文本方面的表现
控制，展示其在文本引导背景下的有效性
主题驱动的图像修复。
]]></description>
      <guid>http://arxiv.org/abs/2312.03771</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:43 GMT</pubDate>
    </item>
    <item>
      <title>Gaussian3Diff：用于 3D 全头合成和编辑的 3D 高斯扩散。 （arXiv：2312.03763v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.03763</link>
      <description><![CDATA[我们提出了一种用于生成逼真 3D 人体头部的新颖框架
随后以非凡的灵活性操纵和放置它们。这
所提出的方法利用 3D 人体的隐式函数表示
头部，采用锚定在参数化面部模型上的 3D 高斯。加强
表示能力和编码空间信息，我们嵌入
每个高斯内的轻量级三平面有效负载，而不是直接存储
颜色和不透明度。此外，我们在 2D UV 空间中参数化高斯
通过 3DMM，能够有效利用 3D 头部的扩散模型
阿凡达一代。我们的方法有助于创造多样化和现实的
3D 人头，对面部特征和表情进行精细编辑。
大量的实验证明了我们方法的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2312.03763</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:42 GMT</pubDate>
    </item>
    <item>
      <title>Mismatch Quest：图像文本未对齐的视觉和文本反馈。 （arXiv：2312.03766v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2312.03766</link>
      <description><![CDATA[虽然现有的图像文本对齐模型达到了高质量的二进制
评估中，他们未能查明偏差的确切来源。
在本文中，我们提出了一种提供详细文本和视觉的方法
对检测到的文本-图像对之间未对齐的解释。我们利用
大语言模型和视觉基础模型自动构建
包含给定图像的合理未对齐标题的训练集，以及
相应的文字解释和视觉指示符。我们还发布了一个新的
人类策划的测试集，包括真实的文本和视觉错位
注释。实证结果表明，微调视觉语言模型
我们的训练集使他们能够清楚地表达错位并直观地指示
它们在图像中，在二进制对齐方面都优于强基线
分类和解释生成任务。我们的方法代码和人类
策划的测试集可在以下位置获取：https://mismatch-quest.github.io/
]]></description>
      <guid>http://arxiv.org/abs/2312.03766</guid>
      <pubDate>Fri, 08 Dec 2023 03:14:42 GMT</pubDate>
    </item>
    </channel>
</rss>