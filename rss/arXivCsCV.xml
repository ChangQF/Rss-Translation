<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 18 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于在线测试时间适应的特定领域块选择和成对视图伪标记</title>
      <link>https://arxiv.org/abs/2404.10966</link>
      <description><![CDATA[arXiv:2404.10966v1 公告类型：新
摘要：测试时适应（TTA）旨在使预训练模型适应新的测试领域，而无需在部署后访问源数据。现有方法通常依赖于使用伪标签进行自我训练，因为无法从测试数据中获得真实值。尽管伪标签的质量对于稳定和准确的长期适应很重要，但之前尚未得到解决。在这项工作中，我们提出了 DPLOT，这是一种简单而有效的 TTA 框架，由两个组件组成：(1) 特定于域的块选择和 (2) 使用成对视图图像生成伪标签。具体来说，我们选择涉及特定领域特征提取的块，并通过熵最小化来训练这些块。在针对当前测试域调整块后，我们通过平均给定的测试图像和相应的翻转副本来生成伪标签。通过简单地使用翻转增强，我们可以防止伪标签质量的下降，这可能是由强增强产生的域间隙引起的。我们的实验结果表明，DPLOT 在 CIFAR10-C、CIFAR100-C 和 ImageNet-C 基准测试中优于之前的 TTA 方法，误差分别降低了 5.4%、9.1% 和 2.9%。此外，我们还提供了广泛的分析来证明我们框架的有效性。代码可在 https://github.com/gist-ailab/domain-specific-block-selection-and-paired-view-pseudo-labeling-for-online-TTA 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.10966</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>利用 3D LiDAR 传感器增强城市安全和公共健康：行人监测和异常活动检测</title>
      <link>https://arxiv.org/abs/2404.10978</link>
      <description><![CDATA[arXiv:2404.10978v1 公告类型：新
摘要：光探测和测距（LiDAR）与物联网（IoT）技术的集成为城市安全和行人福祉方面的公共卫生信息学提供了变革机遇。本文提出了一种利用这些技术来增强城市交通场景中的 3D 对象检测和活动分类的新颖框架。通过采用高架 LiDAR，我们获得详细的 3D 点云数据，从而实现精确的行人活动监控。为了克服城市数据稀缺的问题，我们通过 Blender 模拟交通环境创建专门的数据集，以促进有针对性的模型训练。我们的方法采用改进的基于点体素区域的卷积神经网络 (PV-RCNN) 进行稳健的 3D 检测，并采用 PointNet 来对行人活动进行分类，通过提供对行人行为的洞察并促进更安全的城市环境，显着有利于城市交通管理和公共健康。我们的双模型方法不仅增强了城市交通管理，还通过提供对行人行为的洞察并促进更安全的城市环境，为公共健康做出了重大贡献。]]></description>
      <guid>https://arxiv.org/abs/2404.10978</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>使用图变换器神经网络进行基于神经形态视觉的运动分割</title>
      <link>https://arxiv.org/abs/2404.10940</link>
      <description><![CDATA[arXiv:2404.10940v1 公告类型：新
摘要：移动对象分割对于解释具有挑战性的环境中的机器人导航系统的场景动态至关重要。神经形态视觉传感器由于其异步特性、高时间分辨率和降低的功耗而专为运动感知而定制。然而，它们的非常规输出需要新颖的感知范式来利用其空间稀疏和时间密集的性质。在这项工作中，我们提出了一种使用图变换神经网络的新型基于事件的运动分割算法，称为 GTNN。我们提出的算法通过一系列非线性变换将事件流处理为 3D 图，以揭示事件之间的局部和全局时空相关性。基于这些相关性，属于移动对象的事件可以从背景中分割出来，而无需事先了解动态场景几何形状。该算法使用所提出的训练方案在公开可用的数据集（包括 MOD、EV-IMO 和 \textcolor{black}{EV-IMO2}）上进行训练，以促进对广泛数据集的高效训练。此外，我们引入了动态对象掩模感知事件标签（DOMEL）方法，用于为基于事件的运动分割数据集生成近似的地面实况标签。我们使用 DOMEL 来标记我们自己记录的运动分割事件数据集 (EMS-DOMEL)，并将其向公众发布以供进一步研究和基准测试。在几个未见过的公开数据集上进行了严格的实验，结果表明，在存在动态背景变化、运动模式以及具有不同大小和速度的多个动态对象的情况下，GTNN 的性能优于最先进的方法。 GTNN 取得了显着的性能提升，在运动分割精度（IoU%）和检测率（DR%）方面平均分别提高了 9.4% 和 4.5%。]]></description>
      <guid>https://arxiv.org/abs/2404.10940</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>残余连接损害自监督抽象特征学习</title>
      <link>https://arxiv.org/abs/2404.10947</link>
      <description><![CDATA[arXiv:2404.10947v1 公告类型：新
摘要：我们证明，添加一个权重因子来衰减残差网络中身份快捷方式的强度可以极大地改善最先进的自监督掩码自动编码（MAE）范式中的语义特征学习。我们对 MAE 的 VIT-B/16 主干内的身份快捷方式进行修改，将 ImageNet 上的线性探测精度从 67.3% 提高到 72.3%。这一显着差距表明，虽然残差连接结构在促进梯度传播方面发挥着重要作用，但它可能会产生有害的副作用，即通过将较浅表示的回声注入到更深的层中来降低抽象学习的能力。我们通过一个固定公式来改善这个缺点，该公式随着层深度的增加单调减少身份连接的贡献。我们的设计促进了特征抽象的逐步发展，而不影响网络的可训练性。分析我们修改后的残差网络学习到的表示，我们发现低有效特征等级和下游任务性能之间的相关性。]]></description>
      <guid>https://arxiv.org/abs/2404.10947</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>从 Llama2 7B 权重的无损 (~1.5:1) 压缩算法到 CNN 和 LLM 的可变精度、可变范围、压缩数值数据类型</title>
      <link>https://arxiv.org/abs/2404.10896</link>
      <description><![CDATA[arXiv:2404.10896v1 公告类型：新
摘要：本文从一种简单的无损 ~1.5:1 压缩算法开始，用于大型语言模型 (LLM) Llama2 7B [1] 的权重，该算法可以在 AMD FPGA 中的 ~200 个 LUT 中实现，每个处理超过 8 亿个 bfloat16 数字。第二。然后，该框架扩展到可变精度、可变范围、压缩数值数据类型，这些数据类型是用户定义的浮点数和位置的超集 [2]。然后，本文讨论了基于 ANS（非对称数字系统）[3] 的这种格式的简单硬件实现，它充当这种灵活的数据格式和计算引擎之间的桥梁，同时实现带宽减少。还给出了使用权重压缩和共享的令牌工厂的示例。]]></description>
      <guid>https://arxiv.org/abs/2404.10896</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>用于面部表情识别的多任务多模态自监督学习</title>
      <link>https://arxiv.org/abs/2404.10904</link>
      <description><![CDATA[arXiv:2404.10904v1 公告类型：新
摘要：人类沟通是多模式的；例如，面对面的互动涉及听觉信号（语音）和视觉信号（面部动作和手势）。因此，在设计基于机器学习的面部表情识别系统时，必须利用多种模式。此外，考虑到捕捉人类面部表情的视频数据量不断增长，此类系统应该利用原始的未标记视频，而不需要昂贵的注释。因此，在这项工作中，我们采用多任务多模式自监督学习方法来从野外视频数据中进行面部表情识别。我们的模型结合了三个自我监督的目标函数：首先，多模态对比损失，它将同一视频的不同数据模态拉到表示空间中。其次，多模态聚类损失保留了表示空间中输入数据的语义结构。最后，多模态数据重建损失。我们在三个面部表情识别基准上对这种多模态多任务自监督学习方法进行了全面的研究。为此，我们通过面部表情识别下游任务的自监督任务的不同组合来检查学习的表现。我们的模型 ConCluGen 在 CMU-MOSEI 数据集上优于多个多模式自监督和完全监督基线。我们的结果通常表明，多模式自我监督任务为面部表情识别等具有挑战性的任务提供了巨大的性能提升，同时还减少了所需的手动注释量。我们公开发布预训练模型和源代码]]></description>
      <guid>https://arxiv.org/abs/2404.10904</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>在地球观测图像中保留空间背景的简洁平铺策略</title>
      <link>https://arxiv.org/abs/2404.10927</link>
      <description><![CDATA[arXiv:2404.10927v1 公告类型：新
摘要：我们提出了一种新的平铺策略 Flip-n-Slide，该策略是专为大型地球观测卫星图像而开发的，当感兴趣对象 (OoI) 的位置未知并且空间上下文可能需要分类时消歧义。 Flip-n-Slide 是一种简洁且简约的方法，允许在多个图块位置和方向上表示 OoI。该策略引入了空间上下文信息的多个视图，而不在训练集中引入冗余。通过为每个图块重叠保持不同的变换排列，我们增强了训练集的通用性，而不会歪曲真实的数据分布。我们的实验验证了 Flip-n-Slide 在语义分割任务中的有效性，语义分割是地球物理研究中必要的数据产品。我们发现 Flip-n-Slide 在所有评估指标中都优于之前最先进的平铺数据增强例程。对于代表性不足的类别，Flip-n-Slide 将精确度提高了 15.8%。]]></description>
      <guid>https://arxiv.org/abs/2404.10927</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>HumMUSS：使用状态空间模型理解人体运动</title>
      <link>https://arxiv.org/abs/2404.10880</link>
      <description><![CDATA[arXiv:2404.10880v1 公告类型：新
摘要：从视频中理解人体运动对于一系列应用至关重要，包括姿势估计、网格恢复和动作识别。虽然最先进的方法主要依赖于基于变压器的架构，但这些方法在实际场景中存在局限性。当实时连续预测帧流时，Transformer 速度较慢，并且不能推广到新的帧速率。鉴于这些限制，我们基于状态空间模型的最新进展，提出了一种新颖的无注意力时空模型，用于人类运动理解。我们的模型不仅在各种运动理解任务中与基于 Transformer 的模型的性能相匹配，而且还带来了额外的好处，例如对不同视频帧速率的适应性以及在处理较长的关键点序列时提高训练速度。此外，所提出的模型支持离线和实时应用。对于实时顺序预测，我们的模型不仅内存高效，而且比基于变压器的方法快几倍，同时保持高精度。]]></description>
      <guid>https://arxiv.org/abs/2404.10880</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>用于诊断整个幻灯片图像的语义感知注意力指导</title>
      <link>https://arxiv.org/abs/2404.10894</link>
      <description><![CDATA[arXiv:2404.10894v1 公告类型：新
摘要：准确的癌症诊断仍然是数字病理学的一个关键挑战，这主要是由于整个幻灯片图像中存在千兆像素大小和复杂的空间关系。传统的多实例学习 (MIL) 方法经常难以处理这些复杂性，尤其是在保留准确诊断所需的背景方面。作为回应，我们引入了一个名为语义感知注意力引导 (SAG) 的新框架，其中包括 1) 将诊断相关实体转换为注意力信号的技术，以及 2) 灵活的注意力损失，可有效整合各种语义上重要的信息，例如组织解剖和癌变区域。我们在两个不同的癌症数据集上的实验表明，使用两个最先进的基线模型，准确度、精确度和召回率都有持续的提高。定性分析进一步表明，启发式指导的结合使模型能够专注于对诊断至关重要的区域。SAG 不仅对这里讨论的模型有效，而且它的适应性扩展到任何基于注意力的诊断模型。这为进一步提高癌症诊断的准确性和效率开辟了令人兴奋的可能性。]]></description>
      <guid>https://arxiv.org/abs/2404.10894</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>UruDendro，松树粪便横截面图像的公共数据集</title>
      <link>https://arxiv.org/abs/2404.10856</link>
      <description><![CDATA[arXiv:2404.10856v1 公告类型：新
摘要：在过去的十年中，随着机器学习和图像技术的进步以及树木年代学界的需求不断增加，利用图像分析自动检测树木年轮边界和其他解剖特征已经取得了长足的进步。本文介绍了一个公开数据库，其中包含来自乌拉圭北部商业种植的火炬松树横切面的 64 张扫描图像，树龄从 17 到 24 年不等。该系列包含自动环检测的几个具有挑战性的特征，包括照明和表面处理变化、真菌感染（蓝色污点）、结形成、外环缺失皮质或中断以及径向裂纹。该数据集可用于开发和测试自动树木年轮检测算法。本文向树木年代学界介绍了一种这样的方法，即横截面树木年轮检测（CS-TRD），该方法可识别并标记树种横截面中的完整年轮，从而明确定义早材和晚材。我们将 CS-TRD 性能与 UruDendro 数据集上所有环的真实手动描绘进行比较。 CS-TRD 软件在每张图像不到 20 秒的时间内识别出整个数据库的平均 F 分数为 89%、RMSE 误差为 5.27px 的环。最后，我们提出了使用与检测到的树木年轮所包围的面积相同的圆的\emph{等效半径}来对年轮生长进行稳健的测量。总的来说，这项研究为树木年代学家提供了快速、低成本方法的工具箱，可以自动检测针叶树物种的年轮，特别是使用整个横截面测量直径增长率和茎横面积。]]></description>
      <guid>https://arxiv.org/abs/2404.10856</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>无词汇图像分类和语义分割</title>
      <link>https://arxiv.org/abs/2404.10864</link>
      <description><![CDATA[arXiv:2404.10864v1 公告类型：新
摘要：大型视觉语言模型彻底改变了图像分类和语义分割范式。然而，他们通常在测试时假设一组预定义的类别或词汇来编写文本提示。在语义上下文未知或不断变化的场景中，这种假设是不切实际的。在这里，我们解决这个问题并引入无词汇图像分类（VIC）任务，其目的是在不需要已知词汇的情况下将不受约束的语言引发的语义空间中的类分配给输入图像。由于语义空间广阔，VIC 具有挑战性，其中包含数百万个概念，包括细粒度类别。为了解决 VIC，我们提出了从外部数据库进行类别搜索 (CaSED)，这是一种利用预先训练的视觉语言模型和外部数据库的免训练方法。 CaSED 首先从数据库中语义最相似的字幕中提取候选类别集，然后根据相同的视觉语言模型将图像分配给最匹配的候选类别。此外，我们证明 CaSED 可以局部应用来生成对图像区域进行分类的粗分割掩模，引入无词汇语义分割的任务。 CaSED 及其变体在分类和语义分割基准方面优于其他更复杂的视觉语言模型，同时使用更少的参数。]]></description>
      <guid>https://arxiv.org/abs/2404.10864</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>OSR-ViT：用于开放集对象检测和发现的简单模块化框架</title>
      <link>https://arxiv.org/abs/2404.10865</link>
      <description><![CDATA[arXiv:2404.10865v1 公告类型：新
摘要：对象检测器在开放世界部署期间检测和标记 \textit{novel} 对象的能力对于许多现实世界的应用程序至关重要。不幸的是，当今开放对象检测中的许多工作都是脱节的，并且无法充分解决优先考虑未知对象召回\textit{以及已知类别准确性的应用程序。为了弥补这一差距，我们提出了一项名为开放集对象检测和发现（OSODD）的新任务，并作为解决方案提出了具有 ViT 特征的开放集区域（OSR-ViT）检测框架。 OSR-ViT 将与类别无关的提议网络与强大的基于 ViT 的分类器相结合。其模块化设计简化了优化，并允许用户轻松交换提案解决方案和特征提取器，以最适合他们的应用程序。使用我们的多方面评估协议，我们表明 OSR-ViT 获得的性能水平远远超过最先进的监督方法。我们的方法在低数据设置中也表现出色，使用一小部分训练数据优于监督基线。]]></description>
      <guid>https://arxiv.org/abs/2404.10865</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>从预训练的多模态大型模型中动态自适应多尺度蒸馏，实现高效的跨模态表示学习</title>
      <link>https://arxiv.org/abs/2404.10838</link>
      <description><![CDATA[arXiv:2404.10838v1 公告类型：新
摘要：近年来，预训练的多模态大模型因其在各种多模态应用中的出色表现而受到广泛关注。尽管如此，训练所需的大量计算资源和大量数据集给在计算资源有限的环境中部署带来了重大障碍。为了应对这一挑战，我们首次从预训练的多模态大模型中提出了一种新颖的动态自适应多尺度蒸馏，用于高效的跨模态表示学习。与现有的蒸馏方法不同，我们的策略采用多尺度视角，能够从预先训练的多模态大型模型中提取结构知识。确保学生模型继承对教师知识的全面而细致的理解。为了以平衡和有效的方式优化每个蒸馏损失，我们提出了一种动态自适应蒸馏损失平衡器，这是一种无需手动调整损失重量的新颖组件，可以在蒸馏过程中动态平衡每个损失项。我们的方法仅使用其输出特征和原始图像级信息来简化预训练的多模态大型模型，需要最少的计算资源。这种高效的方法适用于各种应用，即使在资源有限的环境中也允许部署先进的多式联运技术。大量的实验表明，我们的方法保持高性能，同时显着降低模型复杂性和训练成本。此外，我们的蒸馏学生模型仅利用图像级信息在跨模式检索任务上实现最先进的性能，超越了以前依赖区域级信息的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.10838</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>Gasformer：一种基于变压器的光学气体成像中牲畜甲烷排放分割架构</title>
      <link>https://arxiv.org/abs/2404.10841</link>
      <description><![CDATA[arXiv:2404.10841v1 公告类型：新
摘要：牲畜（尤其是牛）的甲烷排放对气候变化有重大影响。随着全球人口和畜产品需求的增加，有效的甲烷减排战略至关重要。我们介绍了 Gasformer，这是一种新颖的语义分割架构，用于检测牲畜的低流量甲烷排放，并使用光学气体成像进行控制释放实验。我们展示了使用 FLIR GF77 OGI 相机捕获的两个独特数据集。 Gasformer 利用 Mix Vision Transformer 编码器和 Light-Ham 解码器来生成多尺度特征并细化分割图。 Gasformer 在这两个数据集上均优于其他最先进的模型，证明了其在受控和现实场景中检测和分割甲烷羽流的有效性。在牲畜数据集上，Gasformer 的 mIoU 达到 88.56%，超过了其他最先进的模型。材料可在以下网址获取：github.com/toqitahamid/Gasformer。]]></description>
      <guid>https://arxiv.org/abs/2404.10841</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>利用中央凹传感器实现基于语义的类人视觉任务主动感知</title>
      <link>https://arxiv.org/abs/2404.10836</link>
      <description><![CDATA[arXiv:2404.10836v1 公告类型：新
摘要：这项工作的目的是确定最近基于语义的中央凹主动感知模型能够如何准确地完成人类经常执行的视觉任务，即场景探索和视觉搜索。该模型利用当前对象检测器对大量对象类进行本地化和分类以及跨多个注视点更新场景的语义描述的能力。它以前曾用于场景探索任务。在本文中，我们重新审视该模型并将其应用扩展到视觉搜索任务。为了说明在场景探索和视觉搜索任务中使用语义信息的好处，我们将其性能与传统的基于显着性的模型进行了比较。在场景探索任务中，与传统的基于显着性的模型相比，基于语义的方法在准确表示视觉场景中存在的语义信息方面表现出优越的性能。在视觉搜索实验中，与显着性驱动模型和随机注视选择算法相比，在包含多个干扰物的视野中搜索目标类的实例显示出优越的性能。我们的结果表明，自上而下的语义信息显着影响视觉探索和搜索任务，这表明将其与传统的自下而上线索相结合的潜在研究领域。]]></description>
      <guid>https://arxiv.org/abs/2404.10836</guid>
      <pubDate>Thu, 18 Apr 2024 06:18:07 GMT</pubDate>
    </item>
    </channel>
</rss>