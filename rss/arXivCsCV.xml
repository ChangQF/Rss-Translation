<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 30 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有相对位置感知的无实例文本到点云定位</title>
      <link>https://arxiv.org/abs/2404.17845</link>
      <description><![CDATA[arXiv:2404.17845v1 公告类型：新
摘要：文本到点云跨模式定位是一项新兴的视觉语言任务，对于未来机器人与人类的协作至关重要。它试图根据一些自然语言指令从城市规模的点云场景中定位位置。在本文中，我们解决了现有方法的两个关键局限性：1）它们依赖于真实实例作为输入； 2）他们忽视了潜在实例之间的相对位置。我们提出的模型遵循两阶段流程，包括用于文本单元检索的粗略阶段和用于位置估计的精细阶段。在这两个阶段中，我们引入了一个实例查询提取器，其中单元格由 3D 稀疏卷积 U-Net 编码以生成多尺度点云特征，并且一组查询迭代地处理这些特征以表示实例。在粗略阶段，设计了行列相对位置感知自注意力（RowColRPA）模块来捕获实例查询之间的空间关系。在精细阶段，开发了多模态相对位置感知交叉注意（RPCA）模块来融合文本和点云特征以及空间关系，以改进精细位置估计。 KITTI360Pose 数据集上的实验结果表明，我们的模型无需将真实实例作为输入即可实现与最先进模型的竞争性能。]]></description>
      <guid>https://arxiv.org/abs/2404.17845</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>GLIMS：用于体积语义分割的注意力引导轻量级多尺度混合网络</title>
      <link>https://arxiv.org/abs/2404.17854</link>
      <description><![CDATA[arXiv:2404.17854v1 公告类型：新
摘要：卷积神经网络（CNN）已广泛应用于医学图像分割任务，表现出良好的性能。然而，卷积架构中固有的归纳偏差限制了它们模拟远程依赖性和空间相关性的能力。虽然最近基于 Transformer 的架构通过利用自注意力机制来编码远程依赖关系并学习表达表示来解决这些限制，但它们通常难以提取低级特征并且高度依赖于数据可用性。这激励我们开发 GLIMS，一种数据高效的注意力引导混合体积分割网络。 GLIMS 利用扩张特征聚合器卷积块 (DACB) 来有效捕获局部-全局特征相关性。此外，基于 Swin Transformer 的瓶颈连接了局部和全局特征，以提高模型的鲁棒性。此外，GLIMS 通过通道和空间注意块 (CSAB) 采用注意力引导分割方法来本地化表达特征，以实现细粒度边界分割。胶质母细胞瘤和多器官 CT 分割任务的定量和定性结果证明了 GLIMS 在复杂性和准确性方面的有效性。 GLIMS在BraTS2021和BTCV数据集上表现出了出色的性能，超越了Swin UNETR的性能。值得注意的是，GLIMS 通过显着减少可训练参数的数量实现了这种高性能。具体来说，GLIMS 有 47.16M 可训练参数和 72.30G FLOP，而 Swin UNETR 有 61.98M 可训练参数和 394.84G FLOP。该代码可在 https://github.com/yaziciz/GLIMS 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.17854</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>ODCR：用于不成对图像去雾的正交解耦对比正则化</title>
      <link>https://arxiv.org/abs/2404.17825</link>
      <description><![CDATA[arXiv:2404.17825v1 公告类型：新
摘要：由于获取具有相同背景的雾/清晰图像对的挑战，非成对图像去雾（UID）具有重要的研究重要性。本文提出了一种新的 UID 方法，称为正交解耦对比正则化（ODCR）。我们的方法基于这样的假设：图像由影响雾霾程度的与雾霾相关的特征以及与雾霾无关的特征（例如纹理和语义信息）组成。 ODCR 旨在确保去雾结果中与雾霾相关的特征与清晰图像的特征非常相似，而与雾霾无关的特征与输入雾霾图像一致。为了实现这一动机，提出了在 Stiefel 流形上进行几何优化的正交 MLP，它可以将图像特征投影到正交空间中，从而减少不同特征之间的相关性。此外，提出了一种任务驱动的深度特征分类器（DWFC），它根据每个通道特征在以自监督方式预测特征源是模糊还是清晰时的贡献为正交特征分配权重。最后，引入加权 PatchNCE (WPNCE) 损失，以实现将输出图像中与雾霾相关的特征拉向清晰图像的特征，同时使与雾霾无关的特征接近雾霾输入的特征。大量实验证明了我们的 ODCR 方法在 UID 上的优越性能。]]></description>
      <guid>https://arxiv.org/abs/2404.17825</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>使用单目视频和稀疏 IMU 的混合 3D 人体姿势估计</title>
      <link>https://arxiv.org/abs/2404.17837</link>
      <description><![CDATA[arXiv:2404.17837v1 公告类型：新
摘要：由于 2D 到 3D 提升的深度模糊性，单目视频的时间 3D 人体姿势估计是以人为中心的计算机视觉中的一项具有挑战性的任务。为了提高准确性并解决遮挡问题，引入了惯性传感器来提供补充信息源。然而，集成异构传感器数据以生成物理合理的 3D 人体姿势仍然具有挑战性。在本文中，我们提出了一种新颖的框架，即实时优化和融合（RTOF）来解决这个问题。我们首先将稀疏惯性方向合并到参数化人体骨骼中，以细化运动学中的 3D 姿势。然后通过基于视觉和惯性观察的能量函数来优化姿势，以减少时间抖动。我们的框架输出平滑且生物力学上合理的人体运动。消融研究的综合实验证明了其合理性和效率。在 Total Capture 数据集上，与基线方法相比，姿态估计误差显着降低。]]></description>
      <guid>https://arxiv.org/abs/2404.17837</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>DVS 像素中弱光响应的表征：事件触发时间的不连续性</title>
      <link>https://arxiv.org/abs/2404.17771</link>
      <description><![CDATA[arXiv:2404.17771v1 公告类型：新
摘要：与传统的基于帧的相机相比，动态视觉传感器（DVS）由于具有宽动态范围和低延迟的优势，最近引起了人们的极大兴趣。然而，弱光条件下的复杂行为仍不清楚，限制了DVS的应用。本文对典型的DVS电路进行分析，发现其事件触发时间存在不连续性。在昏暗的光线条件下，不连续性变得突出。我们指出，不连续性完全取决于光强度的变化速度。真实事件数据的实验结果验证了分析和不连续性的存在，揭示了 DVS 在弱光条件下的非一阶行为。]]></description>
      <guid>https://arxiv.org/abs/2404.17771</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>使用高斯面元进行高质量表面重建</title>
      <link>https://arxiv.org/abs/2404.17774</link>
      <description><![CDATA[arXiv:2404.17774v1 公告类型：新
摘要：我们提出了一种新颖的基于点的表示方法，高斯面元，结合了 3D 高斯点灵活优化过程和面元的表面对齐特性的优点。这是通过直接将 3D 高斯点的 z 尺度设置为 0 来实现的，有效地将原始 3D 椭球展平为 2D 椭圆。这样的设计为优化器提供了明确的指导。通过将局部 z 轴视为法线方向，大大提高了优化稳定性和表面对准。虽然在此设置中根据协方差矩阵计算出的局部 z 轴导数为零，但我们设计了一种自我监督的法线深度一致性损失来解决此问题。结合单眼正常先验和前景掩模来提高重建质量，减轻与高光和背景相关的问题。我们提出了一种体积切割方法来聚合高斯面元的信息，从而去除由 alpha 混合生成的深度图中的错误点。最后，我们将筛选泊松重建方法应用于融合深度图以提取表面网格。实验结果表明，与最先进的神经体积渲染和基于点的渲染方法相比，我们的方法在表面重建方面表现出优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.17774</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>CLFT：用于自动驾驶语义分割的相机-激光雷达融合变压器</title>
      <link>https://arxiv.org/abs/2404.17793</link>
      <description><![CDATA[arXiv:2404.17793v1 公告类型：新
摘要：基于摄像头和激光雷达的自动驾驶语义对象分割的批判性研究极大地受益于深度学习的最新发展。具体来说，视觉变压器是一种新颖的突破，成功地将多头注意力机制引入计算机视觉应用。因此，我们提出了一种基于视觉变换器的网络来进行相机-LiDAR融合，以应用于自动驾驶的语义分割。我们的建议在双向网络上使用视觉变换器的新型渐进式组装策略，然后将结果集成到变换器解码器层上的交叉融合策略中。与文献中的其他作品不同，我们的相机-激光雷达融合变压器已经在雨天和低照度等具有挑战性的条件下进行了评估，显示出强大的性能。该论文报告了不同模式下车辆和人类类别的分割结果：仅相机、仅激光雷达和相机-激光雷达融合。我们针对同样为语义分割设计的其他网络进行 CLFT 的连贯受控基准实验。这些实验旨在从多模态传感器融合和骨干架构两个角度独立评估 CLFT 的性能。定量评估表明，与基于全卷积神经网络 (FCN) 相机-LiDAR 融合神经网络相比，我们的 CLFT 网络在具有挑战性的暗湿条件下可提高高达 10%。与具有变压器主干但使用单一模态输入的网络相比，全面改进为 5-10%。]]></description>
      <guid>https://arxiv.org/abs/2404.17793</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>大型多模态模型辅助人工智能生成的图像质量评估</title>
      <link>https://arxiv.org/abs/2404.17762</link>
      <description><![CDATA[arXiv:2404.17762v1 公告类型：新
摘要：传统的基于深度神经网络（DNN）的图像质量评估（IQA）模型利用卷积神经网络（CNN）或 Transformer 来学习质量感知特征表示，在自然场景图像上取得了值得称赞的性能。然而，当应用于人工智能生成的图像 (AGI) 时，这些基于 DNN 的 IQA 模型表现出较差的性能。这种情况很大程度上是由于生成过程的不可控性导致某些 AGI 固有的语义不准确。因此，辨别语义内容的能力对于评估 AGI 的质量变得至关重要。传统的基于DNN的IQA模型，受限于有限的参数复杂度和训练数据，难以捕获复杂的细粒度语义特征，使得把握整个图像语义内容的存在性和连贯性具有挑战性。为了解决当前 IQA 模型在语义内容感知方面的不足，我们引入了一种大型多模态模型辅助人工智能生成图像质量评估（MA-AGIQA）模型，该模型利用语义信息指导来感知语义信息并通过仔细地提取语义向量。设计了文字提示。此外，它采用专家混合 (MoE) 结构将语义信息与传统基于 DNN 的 IQA 模型提取的质量感知特征动态集成。对两个人工智能生成的内容数据集 AIGCQA-20k 和 AGIQA-3k 进行的综合实验表明，MA-AGIQA 实现了最先进的性能，并展示了其在评估 AGI 质量方面的卓越泛化能力。代码可在 https://github.com/wangpuyi/MA-AGIQA 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.17762</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>RFL-CDNet：通过更丰富的特征学习实现准确的变化检测</title>
      <link>https://arxiv.org/abs/2404.17765</link>
      <description><![CDATA[arXiv:2404.17765v1 公告类型：新
摘要：变化检测是遥感图像分析中一项至关重要但极具挑战性的任务，随着深度学习的快速发展已经取得了很大进展。然而，现有的基于深度学习的变化检测方法大多主要关注复杂的特征提取和多尺度特征融合，而忽略了中间阶段特征的利用不足，从而导致结果次优。为此，我们提出了一种名为 RFL-CDNet 的新颖框架，它利用更丰富的特征学习来提高变化检测性能。具体来说，我们首先引入深度多重监督来增强中间表示，从而在每个阶段释放骨干特征提取器的潜力。此外，我们设计了粗到细引导（C2FG）模块和可学习融合（LF）模块，以进一步改进特征学习并获得更具辨别力的特征表示。 C2FG模块旨在以由粗到细的方式将先前粗尺度的侧面预测无缝地整合到当前的精细尺度预测中，而LF模块假设每个阶段和每个空间位置的贡献是独立的，因此设计一个可学习的模块来融合多个预测。在多个基准数据集上的实验表明，我们提出的 RFL-CDNet 在 WHU 耕地数据集和 CDD 数据集上实现了最先进的性能，在 WHU 建筑数据集上实现了第二好的性能。源代码和模型可在 https://github.com/Hhaizee/RFL-CDNet 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.17765</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>利用跨模式邻居表示来改进 CLIP 分类</title>
      <link>https://arxiv.org/abs/2404.17753</link>
      <description><![CDATA[arXiv:2404.17753v1 公告类型：新
摘要：CLIP 由于其对图像文本对比学习任务的训练而展示了卓越的跨模态匹配能力。然而，如果没有针对单模态场景进行具体优化，其在单模态特征提取中的性能可能不是最优的。尽管如此，一些研究还是直接使用 CLIP 的图像编码器来完成少样本分类等任务，从而在其预训练目标和特征提取方法之间引入了不一致。这种不一致会降低图像特征表示的质量，从而对 CLIP 在目标任务中的有效性产生不利影响。在本文中，我们将文本特征视为 CLIP 空间中图像特征的精确邻居，并提出了一种基于图像与其邻居文本之间的距离结构的新型交叉模态邻居表示（CODER）。这种特征提取方法更符合 CLIP 的预训练目标，从而充分利用 CLIP 强大的跨模态功能。构建高质量的 CODER 的关键在于如何创建大量高质量且多样化的文本来与图像匹配。我们引入自动文本生成器（ATG），以无数据、免训练的方式自动生成所需的文本。我们将 CODER 应用于 CLIP 的零样本和少样本图像分类任务。各种数据集和模型的实验结果证实了 CODER 的有效性。代码位于：https://github.com/YCaigogogo/CVPR24-CODER。]]></description>
      <guid>https://arxiv.org/abs/2404.17753</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>对抗性示例：面部识别系统背景下的生成提案</title>
      <link>https://arxiv.org/abs/2404.17760</link>
      <description><![CDATA[arXiv:2404.17760v1 公告类型：新
摘要：在本文中，我们通过从攻击者的角度引入一种新方法来研究面部识别系统在对抗性示例中呈现的漏洞。该技术基于自动编码器潜在空间的使用，并通过主成分分析进行组织。我们打算分析针对最先进的系统制作适合躲避和模仿攻击的对抗性示例的潜力。我们最初的假设并没有得到结果的强烈支持，它指出可以区分“身份”和“面部表情”特征以产生高质量的示例。尽管研究结果并不支持它，但结果激发了对对抗性示例生成的见解，并开辟了该领域的新研究途径。]]></description>
      <guid>https://arxiv.org/abs/2404.17760</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>BlenderAlchemy：使用视觉语言模型编辑 3D 图形</title>
      <link>https://arxiv.org/abs/2404.17672</link>
      <description><![CDATA[arXiv:2404.17672v1 公告类型：新
摘要：图形设计对于各种应用都很重要，包括电影制作和游戏设计。为了创建高质量的场景，设计人员通常需要在 Blender 等软件上花费数小时，他们可能需要交错和重复操作，例如连接材质节点数百次。此外，稍微不同的设计目标可能需要完全不同的序列，这使得自动化变得困难。在本文中，我们提出了一个系统，该系统利用视觉语言模型（VLM）（例如 GPT-4V）来智能搜索设计操作空间，以获得可以满足用户意图的答案。具体来说，我们设计了一个基于视觉的编辑生成器和状态评估器，它们一起工作以找到实现目标的正确操作顺序。受视觉想象力在人类设计过程中的作用的启发，我们用图像生成模型中的“想象”参考图像来补充 VLM 的视觉推理能力，为抽象语言描述提供视觉基础。在本文中，我们提供的经验证据表明我们的系统可以为诸如从文本和/或参考图像编辑程序材料以及调整复杂场景中的产品渲染的照明配置等任务生成简单但繁琐的 Blender 编辑序列。]]></description>
      <guid>https://arxiv.org/abs/2404.17672</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>生成数据集蒸馏：平衡全局结构和局部细节</title>
      <link>https://arxiv.org/abs/2404.17732</link>
      <description><![CDATA[arXiv:2404.17732v1 公告类型：新
摘要：在本文中，我们提出了一种新的数据集蒸馏方法，该方法在将大型数据集中的信息蒸馏为生成模型时考虑平衡全局结构和局部细节。数据集蒸馏被提出来减少训练模型时所需数据集的大小。传统的数据集蒸馏方法面临着重新部署时间长、跨架构性能差的问题。此外，以前的方法过于关注合成数据集和原始数据集之间的高级语义属性，而忽略了纹理和形状等局部特征。基于上述理解，我们提出了一种将原始图像数据集提炼为生成模型的新方法。我们的方法涉及使用条件生成对抗网络来生成蒸馏数据集。随后，我们确保在蒸馏过程中平衡全局结构和局部细节，不断优化生成器以生成更多信息密集的数据集。]]></description>
      <guid>https://arxiv.org/abs/2404.17732</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>MMA-UNet：用于红外和可见光图像融合的多模态非对称 UNet 架构</title>
      <link>https://arxiv.org/abs/2404.17747</link>
      <description><![CDATA[arXiv:2404.17747v1 公告类型：新
摘要：多模态图像融合（MMIF）将来自不同模态的有用信息映射到相同的表示空间中，从而产生信息丰富的融合图像。然而，现有的融合算法倾向于对称地融合多模态图像，导致融合结果的某些区域中浅层信息丢失或偏向单一模态。在本研究中，我们分析了不同模态信息的空间分布差异，并证明在同一网络内编码特征不利于实现多模态图像的同时深度特征空间对齐。为了克服这个问题，提出了多模态非对称UNet（MMA-UNet）。我们分别为不同模态训练专门的特征编码器，并实施跨尺度融合策略，以将不同模态的特征保持在同一表示空间内，确保平衡的信息融合过程。此外，还进行了广泛的融合和下游任务实验，以证明 MMA-UNet 在融合红外和可见光图像信息方面的效率，产生视觉自然和语义丰富的融合结果。其性能超越了最先进的比较融合方法。]]></description>
      <guid>https://arxiv.org/abs/2404.17747</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>单指纹图像的密集畸变场回归</title>
      <link>https://arxiv.org/abs/2404.17610</link>
      <description><![CDATA[arXiv:2404.17610v1 公告类型：新
摘要：皮肤变形是指纹匹配中长期存在的挑战，它会导致错误的不匹配。先前的研究表明，通过从扭曲的指纹中估计扭曲场，然后将其校正为正常指纹，可以提高识别率。然而，现有的校正方法基于畸变场的主成分表示，其不准确并且对手指姿势非常敏感。在本文中，我们提出了一种校正方法，其中利用基于自参考的网络来直接估计扭曲指纹的密集扭曲场而不是其低维表示。该方法可以输出具有各种手指姿势和扭曲模式的扭曲指纹的准确扭曲场。我们在FVC2004 DB1\_A、扩展的清华扭曲指纹数据库（以及不同手指姿势和扭曲模式下的附加扭曲指纹）和潜在指纹数据库上进行了实验。实验结果表明，我们提出的方法在畸变场估计和校正指纹匹配方面实现了最先进的校正性能。]]></description>
      <guid>https://arxiv.org/abs/2404.17610</guid>
      <pubDate>Tue, 30 Apr 2024 06:18:05 GMT</pubDate>
    </item>
    </channel>
</rss>