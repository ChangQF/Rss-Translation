<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Mon, 27 Nov 2023 11:47:51 GMT</lastBuildDate>
    <item>
      <title>用于解决在线持续学习挑战的基于密度分布的学习框架。 （arXiv：2311.13623v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.13623</link>
      <description><![CDATA[在本文中，我们通过以下方式解决在线持续学习 (CL) 的挑战：
引入基于密度分布的学习框架。 CL，特别是
类增量学习，能够适应新的测试分布，同时
从单遍训练数据流中不断学习，这更重要
符合现实场景的实际应用需求。
然而，现有的 CL 方法经常遭受灾难性遗忘和
由于算法设计复杂，计算成本较高，限制了其
实际使用。我们提出的框架通过实现克服了这些限制
卓越的平均精度和时空效率，提升性能
CL 和经典机器学习之间的差距。具体来说，我们采用
每个 CL 任务的独立生成核密度估计 (GKDE) 模型。
在测试阶段，GKDE 使用自我报告的最大概率
密度值来确定哪一个负责预测传入
测试实例。基于 GKDE 的学习目标可以确保样本具有
相同的标签被分组在一起，而不同的实例被推送
相距较远。在多个 CL 数据集上进行的大量实验验证了
我们提出的框架的有效性。我们的方法优于流行的 CL
以显着的优势接近，同时保持竞争性的时空
效率，使我们的框架适合实际应用。代码
将在 https://github.com/xxxx/xxxx 提供。
]]></description>
      <guid>http://arxiv.org/abs/2311.13623</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:51 GMT</pubDate>
    </item>
    <item>
      <title>Vamos：用于视频理解的多功能动作模型。 （arXiv：2311.13627v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13627</link>
      <description><![CDATA[什么是适合视频理解的良好视频表示，例如
预测未来的活动，或回答视频问题？尽管
早期的方法侧重于直接从视频像素进行端到端学习，我们
建议重新审视基于文本的表示，例如离散动作标签，
或自由格式的视频字幕，这些字幕是可解释的并且可以直接
由大型语言模型（LLM）使用。直观上，不同的视频
理解任务可能需要互补的表征
不同的粒度。为此，我们提出了多功能的行动模型
（Vamos），一个由大型语言模型驱动的学习框架
“推理者”，并且可以灵活地利用视觉嵌入、动作标签和
从视频中提取的自由格式描述作为其输入。我们对 Vamos 的评价是
四个互补的视频理解基准：Ego4D、Next-QA、IntentQA、
和 EgoSchema，其能够对时间动态进行建模、对视觉进行编码
历史，并进行推理。令人惊讶的是，我们观察到基于文本的
代表在所有基准上始终保持有竞争力的表现，
并且视觉嵌入只能提供边际性能提升或没有性能提升，
展示法学硕士中基于文本的视频表示的有效性
时代。我们进行广泛的消融研究和定性分析来支持
我们的观察，并在三个基准上实现了最先进的性能。
]]></description>
      <guid>http://arxiv.org/abs/2311.13627</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:51 GMT</pubDate>
    </item>
    <item>
      <title>图像生成模型在生成多分量图像时面临的挑战。 （arXiv：2311.13620v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13620</link>
      <description><![CDATA[文本到图像生成器的最新进展带来了巨大的进步
图像生成能力。然而，提示的复杂性
生成图像质量的瓶颈。一个特殊的、尚未探索的
facet 是生成模型创建高质量图像的能力
包括作为先验给出的多个组件。在本文中，我们提出并
验证称为组件包含分数 (CIS) 的指标来评估
模型可以正确生成多个组件的程度。我们的成果
揭示了评估的模型很难整合所有的视觉效果
来自具有多个组件的提示的元素（每个组件的 CIS 下降 8.53%）
对于所有评估的模型）。我们还发现，
图像的质量和图像中的上下文感知作为数量
组件增加（初始分数减少 15.91%，初始分数增加 9.62%）
弗雷谢起始距离）。为了解决这个问题，我们对稳定版进行了微调
Diffusion V2 在具有多个组件的自定义创建的测试数据集上，
优于其香草同行。总而言之，这些发现揭示了
现有文本到图像生成器的关键限制，揭示了
使用以下方法在单个图像中生成多个组件的挑战
复杂的提示。
]]></description>
      <guid>http://arxiv.org/abs/2311.13620</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:50 GMT</pubDate>
    </item>
    <item>
      <title>来自黑暗面的知识：用于平衡知识转移的熵重加权知识蒸馏。 （arXiv：2311.13621v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13621</link>
      <description><![CDATA[知识蒸馏（KD）从更大的“教师”模型转移知识
紧凑的“学生”模式，引导学生“暗知识”
$\unicode{x2014}$老师软文中隐含的见解
预测。尽管现有的 KD 已显示出转移的潜力
就知识而言，两方之间的差距仍然存在。与一系列
调查显示，我们认为这种差距是学生过度自信的结果
在预测中，表明对显着特征的关注不平衡，而
忽视了微妙而关键的黑暗知识。为了克服这个问题，我们
介绍熵重加权知识蒸馏（ER-KD），一种新颖的
利用教师预测中的熵来重新加权的方法
基于样本的 KD 损失。 ER-KD 准确地将学生的注意力重新集中在
具有挑战性的实例丰富了教师细致入微的见解，同时减少了
强调更简单的案例，从而实现更平衡的知识转移。
因此，ER-KD 不仅表现出与各种
最先进的 KD 方法，还进一步提高了它们的性能
成本可以忽略不计。这种方法提供了一种简化且有效的策略
完善 KD 中的知识转移流程，树立新的范式
细致处理暗知识。我们的代码位于
https://github.com/cpsu00/ER-KD。
]]></description>
      <guid>http://arxiv.org/abs/2311.13621</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:50 GMT</pubDate>
    </item>
    <item>
      <title>TDiffDe：遥感高光谱图像去噪的截断扩散模型。 （arXiv：2311.13622v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13622</link>
      <description><![CDATA[高光谱图像在精准农业、
环境监测或生态分析。然而，由于传感器
设备和成像环境下，观测到的高光谱图像为
常常不可避免地受到各种噪音的破坏。在这项研究中，我们提出了一个
截断扩散模型，称为 TDiffDe，用于恢复其中的有用信息
高光谱图像逐渐。不是从纯粹的噪音开始，
输入数据包含高光谱图像去噪中的图像信息。因此，
我们将经过训练的扩散模型从小步骤进行削减，以避免破坏
有效信息。
]]></description>
      <guid>http://arxiv.org/abs/2311.13622</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:50 GMT</pubDate>
    </item>
    <item>
      <title>Boosting3D：通过渐进式学习在 3D 先验之前增强 2D 扩散，实现高保真图像到 3D。 （arXiv：2311.13617v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13617</link>
      <description><![CDATA[我们提出了 Boosting3D，一种多阶段单图像到 3D 生成方法
可以在不同的数据域中稳健地生成合理的 3D 对象。这
这项工作的重点是解决单个视图的一致性问题
通过建模合理的几何结构进行图像引导 3D 生成。为了
为此，我们建议在训练 NeRF 之前使用更好的 3D。更多的
具体来说，我们使用以下方法为目标对象训练对象级 LoRA
原始图像和NeRF的渲染输出。然后我们训练 LoRA 并
NeRF 使用渐进式训练策略。 LoRA 和 NeRF 将各自提升
训练时的其他。经过渐进式训练后，LoRA 学习 3D
生成对象的信息并最终转化为对象级 3D
事先的。在最后阶段，我们从训练好的 NeRF 中提取网格并使用
经过训练的 LoRA 可以优化网格的结构和外观。这
实验证明了该方法的有效性。 Boosting3D
学习特定于对象的 3D 先验，这超出了预训练的能力
扩散先验并在单个中实现了最先进的性能
图像到 3D 生成任务。
]]></description>
      <guid>http://arxiv.org/abs/2311.13617</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:49 GMT</pubDate>
    </item>
    <item>
      <title>偷我的作品进行微调？用于检测文本到图像模型中的艺术品盗窃模仿的水印框架。 （arXiv：2311.13619v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13619</link>
      <description><![CDATA[文本到图像模型的进步带来了惊人的艺术效果
表演。然而，一些工作室和网站非法对这些进行微调
模特利用艺术家的作品来模仿他们的风格以获取利润，这违反了
艺术家的版权并削弱他们创作原创的动力
作品。目前，针对这一问题的研究明显缺乏。
在本文中，我们提出了一种新颖的水印框架来检测模仿
通过微调在文本到图像模型中。这个框架嵌入了微妙的
在数字艺术作品中添加水印以保护其版权，同时仍然
保留艺术家的视觉表达。如果有人带水印
艺术品作为模仿艺术家风格的训练数据，这些水印可以
作为可检测指标。通过分析这些分布
一系列生成图像中的水印，使用微调模仿行为
被盗的受害者数据将被暴露。在各种微调场景和对抗中
水印攻击方法，我们的研究证实，分析分布
人工生成图像中的水印可靠地检测未经授权的
模仿。
]]></description>
      <guid>http://arxiv.org/abs/2311.13619</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:49 GMT</pubDate>
    </item>
    <item>
      <title>HEViTPose：用于人体姿势估计的高效视觉转换器。 （arXiv：2311.13615v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13615</link>
      <description><![CDATA[复杂情况下的人体姿态估计一直是一个具有挑战性的问题
任务。最近提出了许多基于 Transformer 的姿势网络，
在提高绩效方面取得令人鼓舞的进展。但是，那
位姿网络的卓越性能总是伴随着沉重的负担
计算成本和庞大的网络规模。为了解决这个问题，
本文提出了一种用于人体姿势的高效视觉变换器
估计（HEViTPose）。在 HEViTPose 中，级联组空间缩减
提出了多头注意力模块（CGSR-MHA），减少了
通过特征分组和空间退化机制的计算成本，
同时通过多个低维注意力保持特征多样性
头。此外，定义了补丁嵌入重叠宽度（PEOW）的概念
帮助理解重叠量和局部之间的关系
连续性。通过优化 PEOW，我们的模型获得了性能提升，
参数和 GFLOP。

在两个基准数据集（MPII和COCO）上进行综合实验
证明小型和大型 HEViTPose 模型与
最先进的模型，同时更轻。具体来说，HEViTPose-B
在 MPII 测试集上达到 90.7 PCK@0.5，在 COCO test-dev2017 上达到 72.6 AP
放。与HRNet-W32和Swin-S相比，我们的HEViTPose-B显着减少了
参数 ($\downarrow$62.1%,$\downarrow$80.4%,) 和 GFLOP
（$\downarrow$43.4%，$\downarrow$63.8%，）。代码和型号可在以下网址获取
\url{此处}。
]]></description>
      <guid>http://arxiv.org/abs/2311.13615</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:48 GMT</pubDate>
    </item>
    <item>
      <title>使用时空查找表增强在线视频质量。 (arXiv:2311.13616v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2311.13616</link>
      <description><![CDATA[低延迟率对于基于在线视频的应用程序至关重要，例如
视频会议和云游戏，提高视频质量
在线场景变得越来越重要。然而，现有的质量提升
方法受到推理速度慢和时间要求的限制
未来框架中包含的信息，使得部署它们变得具有挑战性
直接在在线任务中。在本文中，我们提出了一种新方法，STLVQE，
专为解决很少研究的在线视频质量而设计
增强（在线-VQE）问题。我们的 STLVQE 设计了一个新的 VQE 框架，
包含一个与模块无关的特征提取器，可以大大减少冗余
计算并重新设计传播、对齐和增强模块
网络。提出了时空查找表（STL），其中
提取视频中的时空信息，同时节省大量时间
推理时间。据我们所知，我们是第一个利用
用于提取视频任务中的时间信息的 LUT 结构。广泛的
在 MFQE 2.0 数据集上的实验表明我们的 STLVQE 实现了
令人满意的性能与速度权衡。
]]></description>
      <guid>http://arxiv.org/abs/2311.13616</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:48 GMT</pubDate>
    </item>
    <item>
      <title>描述符和单词汤：克服分布外小样本学习的参数效率和准确性权衡。 （arXiv：2311.13612v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13612</link>
      <description><![CDATA[在过去的一年里，围绕
使用 GPT 描述符进行零样本评估。这些研究促进了零样本
具有特定标签文本集合的预训练 VL 模型的准确性
由 GPT 生成。最近的一项研究 WaffleCLIP 表明，类似的
零样本精度可以通过随机描述符的集合来实现。
然而，这两种零样本方法都是不可训练的，因此不是最优的
当一些少数样本分布外（OOD）训练数据可用时。
受这些先前工作的启发，我们提出了两种更灵活的方法，称为
描述符和单词汤，在测试时不需要法学硕士，并且可以
利用训练数据提高 OOD 目标准确性。描述符汤
使用通用的少样本贪婪地选择一小组文本描述符
训练数据，然后使用所选的计算稳健的类嵌入
描述符。 Word soup 以类似的方式贪婪地组装一串单词。
与现有的少样本软提示调整方法相比，word soup 需要
构造参数更少，GPU 内存更少，因为它不需要
反向传播。两种汤的性能都优于当前发布的少样本方法，甚至
与 SoTA 零样本方法结合时，在跨数据集和域上
泛化基准。与 SoTA 提示和描述符集成相比
方法，例如ProDA和WaffleCLIP，word soup实现了更高的OOD准确率
乐团成员较少。请查看我们的代码：
github.com/Chris210634/word_soups
]]></description>
      <guid>http://arxiv.org/abs/2311.13612</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:47 GMT</pubDate>
    </item>
    <item>
      <title>跨越训练进度：用于增强数据集修剪的时间双深度评分 (TDDS)。 （arXiv：2311.13613v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13613</link>
      <description><![CDATA[数据集剪枝旨在构建能够实现性能的核心集
与原始的完整数据集相当。大多数现有的数据集修剪方法
通常依靠基于快照的标准来识别代表性样本
导致各种修剪和跨架构的泛化能力较差
场景。最近的研究通过扩大范围来解决这个问题
考虑的训练动态，包括遗忘事件和
概率变化，通常使用平均方法。然而，这些作品
努力整合更广泛的训练动态而不忽视
概括性良好的样本，在
平均方式。在本研究中，我们提出了一种新颖的数据集修剪方法
称为时间双深度评分（TDDS）来解决这个问题。 TDDS
利用双深度策略来实现合并之间的平衡
广泛的训练动态并识别数据集的代表性样本
修剪。在第一个深度中，我们估计每个样本个体的序列
跨越培训进度的贡献，确保全面
整合训练动态。在第二个深度，我们重点关注
在第一深度中确定的样本贡献的可变性
突出显示普遍化的样本。在 CIFAR 上进行了大量实验
和ImageNet数据集验证了TDDS相对于之前SOTA的优越性
方法。特别是在 CIFAR-100 上，我们的方法达到了 54.51% 的准确率
仅 10% 训练数据，超过随机选择 7.83% 等
比较方法至少提高了 12.69%。
]]></description>
      <guid>http://arxiv.org/abs/2311.13613</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:47 GMT</pubDate>
    </item>
    <item>
      <title>HalluciDoctor：减轻视觉指令数据中的幻觉毒性。 （arXiv：2311.13614v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13614</link>
      <description><![CDATA[根据机器生成调整的多模态大型语言模型 (MLLM)
指令跟踪数据在各种方面都表现出了卓越的性能
多模态理解和生成任务。然而，幻觉
机器生成的数据所固有的，这可能会导致幻觉输出
在 MLLM 中，仍有待探索。这项工作旨在调查各种
幻觉（即物体、关系、属性幻觉）并减轻
大规模机器生成视觉中的那些幻觉毒性
指令数据集。利用人类识别事实错误的能力，
我们提出了一种新颖的幻觉检测和消除框架，
HalluciDoctor，基于交叉检查范式。我们使用我们的框架
自动识别并消除训练数据中的幻觉。
有趣的是，HalluciDoctor 还指出，出现了虚假相关性
长尾物体共现会导致幻觉。基于
我们执行反事实视觉指令扩展来平衡数据
分布，从而增强 MLLM 对幻觉的抵抗力。
幻觉评估基准的综合实验表明，我们的
该方法成功地相对减轻了 44.6% 的幻觉并维持
与 LLaVA 相比具有竞争力的性能。源代码将在
\url{https://github.com/Yuqifan1117/HalluciDoctor}。
]]></description>
      <guid>http://arxiv.org/abs/2311.13614</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:47 GMT</pubDate>
    </item>
    <item>
      <title>使用文本到视频先验为草图注入生命力。 （arXiv：2311.13608v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13608</link>
      <description><![CDATA[草图是人类用来表达的最直观、最通用的工具之一
以视觉方式传达他们的想法。动画草图打开了另一个维度
表达想法，并被设计师广泛用于各种目的。
动画草图是一个费力的过程，需要丰富的经验和
专业的设计能力。在这项工作中，我们提出了一种方法
自动为单个主题草图添加运动（因此，“呼吸生命
进入其中”），只需提供指示所需动作的文本提示即可。
输出是以矢量表示形式提供的短动画，可以是
轻松编辑。我们的方法不需要大量的培训，而是
利用大型预训练文本到视频扩散模型的运动先验
使用分数蒸馏损失来指导笔划的放置。推广
自然流畅的运动并更好地保留草图的外观，我们
通过两个组件对学习到的运动进行建模。第一个管理小型地方
变形，第二个控制全局仿射变换。
令人惊讶的是，我们发现即使是难以生成草图视频的模型
其本身仍然可以作为抽象动画的有用支柱
交涉。
]]></description>
      <guid>http://arxiv.org/abs/2311.13608</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:46 GMT</pubDate>
    </item>
    <item>
      <title>TRIDENT：隐式神经表征的非线性三部曲。 （arXiv：2311.13610v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13610</link>
      <description><![CDATA[隐式神经表征（INR）引起了人们的极大兴趣
最近因其能够对复杂的高维数据进行建模而无需
显式参数化。在这项工作中，我们介绍了 TRIDENT，一个新颖的函数
对于以三部曲为特征的隐式神经表示
非线性。首先，它被设计用来表示高阶特征
通过顺序紧凑性。其次，TRIDENT高效捕捉频率
信息，一种称为频率紧凑性的特征。第三，它具有
表示信号或图像的能力，使其大部分能量
集中在有限的空间区域，表示空间紧凑。我们
通过对各种反问题的大量实验证明，我们的
所提出的函数优于现有的隐式神经表示
功能。
]]></description>
      <guid>http://arxiv.org/abs/2311.13610</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:46 GMT</pubDate>
    </item>
    <item>
      <title>用于通过车载自组织网络 (VANET) 进行低延迟多描述视频流的跨层方案。 （arXiv：2311.13603v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.13603</link>
      <description><![CDATA[当今车辆通信对实时性的需求不断增长
需要视频辅助的应用程序。新的最先进的
高效视频编码（HEVC）标准在实时性方面非常有前景
视频流。它提供高编码效率以及专用低
延迟编码结构。其中，全帧内（AI）编码结构
以较高的视频比特率为代价保证最短的编码时间，这
因此会损害传输性能。在这项工作中，我们提出了一个
独创的跨层系统，以提高接收视频质量
车辆通讯。该系统复杂度低，依赖于多个
描述编码（MDC）方法。它基于自适应映射机制
应用在 IEEE 802.11p 标准媒体访问控制 (MAC) 层。
真实车辆环境中的仿真结果表明
低延迟视频通信，所提出的方法提供了显着的视频
接收端的质量改进。
]]></description>
      <guid>http://arxiv.org/abs/2311.13603</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:45 GMT</pubDate>
    </item>
    </channel>
</rss>