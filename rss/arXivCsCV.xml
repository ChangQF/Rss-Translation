<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 10 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>嫦娥五号相机姿态稳健陨石坑检测</title>
      <link>https://arxiv.org/abs/2406.04569</link>
      <description><![CDATA[arXiv:2406.04569v1 公告类型：新
摘要：由于太空任务旨在探索越来越危险的地形，因此需要准确及时的位置估计以确保安全导航。基于视觉的导航通过将机载图像中可见的撞击坑与已知数据库相关联以估计飞行器的姿势来实现这一目标。然而，现有文献尚未充分评估包含离天底视角的图像的陨石坑检测算法 (CDA) 性能。在这项工作中，我们评估了 Mask R-CNN 用于陨石坑检测的性能，比较了在包含离天底视角的模拟数据上预训练的模型和在真实月球图像上预训练的模型。我们证明，尽管缺乏包含离天底视角的图像，但对真实月球图像的预训练仍然更胜一筹，实现了 63.1 F1 分数的检测性能和 0.701 交并比椭圆回归性能。这项工作首次定量分析了 CDA 在包含离天底视角的图像上的性能。为了开发日益强大的 CDA，我们还提供了第一个带有注释的 CDA 数据集，其中包含来自嫦娥五号着陆相机的非天底视角。]]></description>
      <guid>https://arxiv.org/abs/2406.04569</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:34 GMT</pubDate>
    </item>
    <item>
      <title>用于多照明图像异常检测的注意力融合逆向蒸馏</title>
      <link>https://arxiv.org/abs/2406.04573</link>
      <description><![CDATA[arXiv:2406.04573v1 公告类型：新
摘要：本研究针对多照明图像异常检测 (MLIAD)，其中利用多种照明条件来增强成像质量和异常检测性能。虽然已经提出了许多图像异常检测方法，但它们缺乏处理单个样本的多个输入的能力，例如 MLIAD 中的多照明图像。因此，本研究提出了注意力融合逆向蒸馏 (AFRD) 来处理 MLIAD 中的多个输入。为此，AFRD 利用预先训练的教师网络从多个输入中提取特征。然后通过注意力模块将这些特征聚合成融合特征。随后，利用相应的学生网络对注意力融合特征进行回归。回归误差在推理过程中表示为异常分数。在 Eyecandies 上的实验表明，AFRD 比其他 MLIAD 替代方案实现了更出色的 MLIAD 性能，也凸显了使用多种照明条件进行异常检测的好处。]]></description>
      <guid>https://arxiv.org/abs/2406.04573</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:34 GMT</pubDate>
    </item>
    <item>
      <title>CVPR 2024 PVUW 研讨会 MOSE 赛道第一名解决方案：复杂视频对象分割</title>
      <link>https://arxiv.org/abs/2406.04600</link>
      <description><![CDATA[arXiv:2406.04600v1 公告类型：新
摘要：在复杂场景中跟踪和分割多个对象一直是视频对象分割领域的挑战，尤其是在对象被遮挡并分割成部分的场景中。在这种情况下，对象的定义变得非常模糊。MOSE 数据集背后的动机是如何在复杂场景中清楚地识别和区分对象。在这个挑战中，我们提出了一个语义嵌入视频对象分割模型，并使用对象的显着特征作为查询表示。语义理解有助于模型识别对象的各个部分，而显着特征可以捕获对象更具辨别力的特征。在大规模视频对象分割数据集上进行训练后，我们的模型在 PVUW Challenge 2024：复杂视频对象分割赛道的测试集中取得了第一名（\textbf{84.45\%}）。]]></description>
      <guid>https://arxiv.org/abs/2406.04600</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:34 GMT</pubDate>
    </item>
    <item>
      <title>食品：使用短程 FMCW 雷达进行面部身份验证和分布外检测</title>
      <link>https://arxiv.org/abs/2406.04546</link>
      <description><![CDATA[arXiv:2406.04546v1 公告类型：新
摘要：本文提出了一种基于短距离 FMCW 雷达的面部认证和分布外 (OOD) 检测框架。我们的管道联合估计分布内 (ID) 样本的正确类别并检测 OOD 样本以防止其预测不准确。我们基于重建的架构由一个具有一个编码器和多解码器配置的主卷积块以及中间线性编码器-解码器部分组成。这些元素共同构成了一个准确的人脸分类器和一个强大的 OOD 检测器。对于我们使用 60 GHz 短距离 FMCW 雷达收集的数据集，我们的网络在识别分布内人脸方面实现了 98.07% 的平均分类准确率。作为 OOD 检测器，它实现了 98.50% 的平均接收者操作特性 (AUROC) 曲线下面积和 6.20% 的 95% 真实阳性率 (FPR95) 的平均假阳性率。此外，我们大量的实验表明，就常见的 OOD 检测指标而言，所提出的方法优于以前的 OOD 检测器。]]></description>
      <guid>https://arxiv.org/abs/2406.04546</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:33 GMT</pubDate>
    </item>
    <item>
      <title>利用情境化的 Vendi 评分指导提高生成图像的地理多样性</title>
      <link>https://arxiv.org/abs/2406.04551</link>
      <description><![CDATA[arXiv:2406.04551v1 公告类型：新
摘要：随着文本到图像生成模型的日益普及，人们越来越关注了解其风险和偏见。最近的研究发现，最先进的模型难以用现实世界的真实多样性来描绘日常物体，并且地理区域之间存在显着差距。在这项工作中，我们的目标是增加常见物体生成图像的多样性，以便每个区域的变化都代表现实世界。我们引入了一种推理时间干预，即情境化 Vendi 分数指导 (c-VSG)，它指导潜在扩散模型的后退步骤，以增加样本的多样性（与先前生成的图像的“记忆库”相比），同时限制现实世界情境化图像示例集内的变化量。我们使用两个具有地理代表性的数据集评估 c-VSG，发现它显着增加了生成图像的多样性，无论是对于表现最差的区域还是平均水平，同时保持或提高图像质量和一致性。此外，定性分析表明，生成的图像的多样性得到了显著改善，包括与原始模型中存在的简化区域描绘一致。我们希望这项工作是朝着反映世界真实地理多样性的文本到图像生成模型迈出的一步。]]></description>
      <guid>https://arxiv.org/abs/2406.04551</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:33 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络对非母语手写字符进行分类</title>
      <link>https://arxiv.org/abs/2406.04511</link>
      <description><![CDATA[arXiv:2406.04511v1 公告类型：新
摘要：卷积神经网络 (CNN) 的使用加速了手写字符分类/识别的进展。手写字符识别 (HCR) 已应用于各种领域，例如交通信号检测、语言翻译和文档信息提取。然而，现有 HCR 技术尚未得到广泛使用，因为它不能提供具有出色准确性的可靠字符识别。HCR 不可靠的原因之一是现有的 HCR 方法没有考虑非母语作者的笔迹风格。因此，需要进一步改进以确保字符识别技术在关键任务中的可靠性和广泛部署。在这项工作中，通过提出一个定制的 CNN 模型来对非母语用户编写的英文字符进行分类。我们使用一个称为手写孤立英文字符 (HIEC) 数据集的新数据集来训练这个 CNN。该数据集由从 260 人收集的 16,496 张图像组成。本文还对我们的 CNN 进行了一项消融研究，通过调整超参数来确定 HIEC 数据集的最佳模型。所提出的具有五个卷积层和一个隐藏层的模型在字符识别准确度方面优于最先进的模型，准确度达到 $\mathbf{97.04}$%。与第二好的模型相比，我们的模型在分类准确度方面的相对改进是 $\mathbf{4.38}$%。]]></description>
      <guid>https://arxiv.org/abs/2406.04511</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:32 GMT</pubDate>
    </item>
    <item>
      <title>MambaDepth：增强自监督细结构单目深度估计的长距离依赖性</title>
      <link>https://arxiv.org/abs/2406.04532</link>
      <description><![CDATA[arXiv:2406.04532v1 公告类型：新
摘要：在自监督深度估计领域，卷积神经网络 (CNN) 和 Transformer 传统上占主导地位。然而，由于它们的局部焦点或计算需求，这两种架构都难以有效处理长距离依赖关系。为了克服这一限制，我们提出了 MambaDepth，这是一个专为自监督深度估计而量身定制的多功能网络。从 Mamba 架构的优势中汲取灵感，该架构以其对长序列的熟练处理和通过状态空间模型 (SSM) 有效捕获全局上下文的能力而闻名，我们引入了 MambaDepth。这种创新架构将 U-Net 在自监督深度估计中的有效性与 Mamba 的高级功能相结合。MambaDepth 围绕纯基于 Mamba 的编码器-解码器框架构建，结合了跳过连接以维护网络各个级别的空间信息。这种配置促进了广泛的特征学习过程，从而能够在深度图中捕获精细的细节和更广泛的上下文。此外，我们在 Mamba 模块内开发了一种新颖的集成技术，以促进编码器和解码器组件之间不间断的连接和信息流，从而提高深度精度。在成熟的 KITTI 数据集上进行的全面测试表明，MambaDepth 在自监督深度估计任务中优于领先的 CNN 和基于 Transformer 的模型，使其能够实现最先进的性能。此外，MambaDepth 在其他数据集（如 Make3D 和 Cityscapes）上证明了其卓越的泛化能力。MambaDepth 的性能预示着自监督深度估计的有效远程依赖建模的新时代的到来。]]></description>
      <guid>https://arxiv.org/abs/2406.04532</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:32 GMT</pubDate>
    </item>
    <item>
      <title>M&M VTO：多件服装虚拟试穿和编辑</title>
      <link>https://arxiv.org/abs/2406.04542</link>
      <description><![CDATA[arXiv:2406.04542v1 公告类型：新
摘要：我们介绍了 M&amp;M VTO，这是一种混合搭配虚拟试穿方法，输入多件服装图像、服装布局的文字描述和一个人的图像。示例输入包括：一件衬衫的图像、一条裤子的图像、“卷起袖子、塞进衬衫”和一个人的图像。输出是这些服装（按所需布局）在给定人身上看起来如何的可视化。我们方法的主要贡献在于：1）基于单级扩散的模型，没有超分辨率级联，允许在 1024x512 分辨率下混合搭配多件服装，同时保留和扭曲复杂的服装细节；2）架构设计（VTO UNet 扩散变换器）将去噪与个人特定特征分离，从而实现高度有效的身份保存微调策略（每个个人 6MB 模型 vs 使用 Dreambooth 微调等实现的 4GB 模型）；解决了当前虚拟试穿方法中常见的身份丢失问题；3）通过专门针对虚拟试穿任务的 PaLI-3 微调的文本输入控制多件服装的布局。实验结果表明，M&amp;M VTO 在质量和数量上都达到了最先进的性能，并为通过语言引导和多件服装试穿进行虚拟试穿开辟了新的机会。]]></description>
      <guid>https://arxiv.org/abs/2406.04542</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:32 GMT</pubDate>
    </item>
    <item>
      <title>CORU：全面的 OCR 后解析和收据理解数据集</title>
      <link>https://arxiv.org/abs/2406.04493</link>
      <description><![CDATA[arXiv:2406.04493v1 公告类型：新
摘要：在光学字符识别 (OCR) 和自然语言处理 (NLP) 领域，集成多语言功能仍然是一项关键挑战，尤其是在考虑具有复杂脚本的语言（例如阿拉伯语）时。本文介绍了综合 OCR 后解析和收据理解数据集 (CORU)，这是一个新颖的数据集，专门用于增强涉及阿拉伯语和英语的多语言环境中收据的 OCR 和信息提取。CORU 包含来自超市和服装店等不同零售环境的 20,000 多张带注释的收据，以及用于识别每条检测到的线的 30,000 张带注释的 OCR 图像，以及 10,000 个带注释的用于详细信息提取的项目。这些注释捕获了基本细节，例如商家名称、商品描述、总价、收据编号和日期。它们的结构支持三个主要计算任务：对象检测、OCR 和信息提取。我们在 CORU 上为一系列模型建立了基准性能，以评估 Tesseract OCR 等传统方法和更先进的基于神经网络的方法的有效性。这些基准对于处理现实世界收据中常见的复杂且嘈杂的文档布局以及推进自动多语言文档处理的状态至关重要。我们的数据集是公开的 (https://github.com/Update-For-Integrated-Business-AI/CORU)。]]></description>
      <guid>https://arxiv.org/abs/2406.04493</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:31 GMT</pubDate>
    </item>
    <item>
      <title>OCCAM：实现成本高效且准确度高的图像分类推理</title>
      <link>https://arxiv.org/abs/2406.04508</link>
      <description><![CDATA[arXiv:2406.04508v1 公告类型：新
摘要：图像分类是大多数计算机视觉应用的基本构建块。随着机器学习模型的日益普及和容量的不断增长，人们可以轻松地在线或离线访问经过训练的图像分类器作为服务。然而，模型的使用是有成本的，容量更高的分类器通常会产生更高的推理成本。为了利用不同分类器的各自优势，我们提出了一种原则性方法 OCCAM，用于计算图像分类查询的最佳分类器分配策略（称为最佳模型组合），以便在用户指定的成本预算下最大化聚合准确度。我们的方法使用无偏和低方差的准确度估计器，并通过求解整数线性规划问题有效地计算出最优解。在各种现实世界的数据集上，OCCAM 实现了 40% 的成本降低，而准确度几乎没有下降。]]></description>
      <guid>https://arxiv.org/abs/2406.04508</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:31 GMT</pubDate>
    </item>
    <item>
      <title>DeTra：物体检测和轨迹预测的统一模型</title>
      <link>https://arxiv.org/abs/2406.04426</link>
      <description><![CDATA[arXiv:2406.04426v1 公告类型：新
摘要：物体检测和轨迹预测任务在理解自动驾驶场景中起着至关重要的作用。这些任务通常以级联方式执行，因此容易出现复合错误。此外，这两个任务之间的接口通常非常薄弱，从而造成信息丢失瓶颈。为了应对这些挑战，我们的方法将两个任务的结合表述为轨迹细化问题，其中第一个姿势是检测（当前时间），后续姿势是多个预测（未来时间）的航点。为了解决这个统一的任务，我们设计了一个细化转换器，可以直接从 LiDAR 点云和高清地图推断物体的存在、姿势和多模态未来行为。我们将这个模型称为 DeTra，即物体检测和轨迹预测的缩写。在我们的实验中，我们观察到 \ourmodel{} 在 Argoverse 2 传感器和 Waymo 开放数据集上的表现远超最新水平，而且在多项指标上都遥遥领先。最后但同样重要的是，我们进行了广泛的消融研究，证明了改进对于这项任务的价值，每个提议的组件都对其性能产生了积极贡献，并且做出了关键的设计选择。]]></description>
      <guid>https://arxiv.org/abs/2406.04426</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:30 GMT</pubDate>
    </item>
    <item>
      <title>通过综合基准评估大型视觉语言模型对现实世界复杂性的理解</title>
      <link>https://arxiv.org/abs/2406.04470</link>
      <description><![CDATA[arXiv:2406.04470v1 公告类型：新
摘要：本研究评估了大型视觉语言模型 (LVLM) 区分 AI 生成图像和人类生成图像的能力。它为此评估引入了一种新的自动基准构建方法。实验使用 AI 和人类创建的图像的混合数据集将常见的 LVLM 与人类参与者进行了比较。结果表明，LVLM 可以在一定程度上区分图像类型，但表现出向右偏差，并且与人类相比表现明显更差。为了在这些发现的基础上，我们开发了一个使用 AI 的自动基准构建过程。该过程涉及主题检索、叙述脚本生成、错误嵌入和图像生成，从而创建了一组具有故意错误的多样化文本图像对。我们通过构建两个可用的基准来验证我们的方法。这项研究强调了 LVLM 在现实世界理解中的优势和劣势，并推进了基准构建技术，为 AI 模型评估提供了一种可扩展且自动化的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.04470</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:30 GMT</pubDate>
    </item>
    <item>
      <title>走出去寻找：使用增量数据进行热启动训练</title>
      <link>https://arxiv.org/abs/2406.04484</link>
      <description><![CDATA[arXiv:2406.04484v1 公告类型：新
摘要：在现实世界的深度学习应用（例如自动驾驶）中，数据通常会随着时间的推移按顺序到达。当有新的训练数据可用时，从头开始训练模型会破坏利用所学知识的好处，从而导致大量的训练成本。从先前训练的检查点进行热启动是保留知识和推进学习的最直观方法。然而，现有文献表明，这种热启动会降低泛化能力。在本文中，我们提倡热启动，但要走出之前的收敛点，从而可以更好地适应新数据而不会损害先前的知识。我们提出了知识整合和获取 (CKCA)，这是一种具有两个新组件的持续模型改进算法。首先，一种新颖的特征正则化 (FeatReg) 来保留和细化现有检查点中的知识；其次，我们提出了自适应知识蒸馏 (AdaKD)，这是一种遗忘缓解和知识转移的新方法。我们在 ImageNet 上使用训练数据的多个分割测试了我们的方法。我们的方法比原始热启动方法实现了高达 $8.39\%$ 的 top1 准确率，并且始终以较大的优势超越现有技术。]]></description>
      <guid>https://arxiv.org/abs/2406.04484</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:30 GMT</pubDate>
    </item>
    <item>
      <title>使用多尺度视觉变换器根据重症监护室的低分辨率热视频预测护理活动得分</title>
      <link>https://arxiv.org/abs/2406.04364</link>
      <description><![CDATA[arXiv:2406.04364v1 公告类型：新
摘要：医院护士的护理人员工作量过大与患者护理质量下降和员工倦怠增加有关。重症监护病房 (ICU) 中的这种工作量测量通常使用护理活动评分 (NAS) 进行，但这通常是手动和零星记录的。先前的工作利用了环境智能 (AmI)，通过使用计算机视觉被动地得出护理人员与患者的互动时间来监控员工的工作量。在这封信中，我们建议使用多尺度视觉变换器 (MViT) 从 ICU 中记录的低分辨率热视频中被动预测 NAS。从澳大利亚墨尔本的 ICU 获得了 458 个视频，并使用间接预测和直接预测方法训练 MViTv2 模型。间接方法在推断 NAS 之前从视频中预测了 8 个可能可识别的 NAS 活动中的 1 个。直接方法立即从视频中预测 NAS 分数。间接方法的平均 5 倍准确率为 57.21%，受试者工作特征曲线下面积 (ROC AUC) 为 0.865，F1 得分为 0.570，均方误差 (MSE) 为 28.16。直接方法的 MSE 为 18.16。我们还表明，在相同设置下，MViTv2 的表现优于类似模型，例如 R(2+1)D 和 ResNet50-LSTM。
本研究显示了使用 MViTv2 被动预测 ICU 中的 NAS 并自动监控工作人员工作量的可行性。我们上述结果还显示，直接预测 NAS 的准确性高于间接预测 NAS。我们希望我们的研究可以为未来的工作提供方向，并进一步提高被动 NAS 监测的准确性。]]></description>
      <guid>https://arxiv.org/abs/2406.04364</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:29 GMT</pubDate>
    </item>
    <item>
      <title>通过特定属性的提示学习实现高效的 3D 感知面部图像编辑</title>
      <link>https://arxiv.org/abs/2406.04413</link>
      <description><![CDATA[arXiv:2406.04413v1 公告类型：新
摘要：利用 StyleGAN 的表现力和解开的潜在空间，现有的 2D 方法采用文本提示来编辑具有不同属性的面部图像。相比之下，生成不同目标姿势面部的 3D 感知方法需要特定于属性的分类器，为每个属性学习单独的模型权重，并且对于新属性不可扩展。在这项工作中，我们提出了一种基于特定属性提示学习的高效、即插即用、3D 感知面部编辑框架，能够在各种目标姿势下生成具有可控属性的面部图像。为此，我们引入了一个基于文本驱动的可学习风格标记的潜在属性编辑器 (LAE)。LAE 利用预先训练的视觉语言模型在任何预先训练的 3D 感知 GAN 的潜在空间中找到文本引导的属性特定编辑方向。它利用可学习的风格标记和风格映射器来学习并将此编辑方向转换为 3D 潜在空间。为了训练具有多种属性的 LAE，我们使用方向对比损失和风格标记损失。此外，为了确保不同姿势和属性之间的视图一致性和身份保存，我们采用了多种 3D 感知身份和姿势保存损失。我们的实验表明，我们提出的框架可以生成具有 3D 感知和视图一致性的高质量图像，同时保持特定于属性的特征。我们证明了我们的方法对不同面部属性的有效性，包括头发颜色和样式、表情等。代码：https://github.com/VIROBO-15/Efficient-3D-Aware-Facial-Image-Editing。]]></description>
      <guid>https://arxiv.org/abs/2406.04413</guid>
      <pubDate>Mon, 10 Jun 2024 06:21:29 GMT</pubDate>
    </item>
    </channel>
</rss>