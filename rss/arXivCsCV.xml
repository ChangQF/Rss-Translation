<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 17 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>ChangeAnywhere：通过语义潜在扩散模型进行遥感变化检测的样本生成</title>
      <link>https://arxiv.org/abs/2404.08892</link>
      <description><![CDATA[arXiv:2404.08892v1 公告类型：新
摘要：遥感变化检测（CD）是一种基于多时相图像精确定位全球范围内变化的关键技术。随着最近深度学习的扩展，基于监督深度学习的 CD 模型已经表现出了令人满意的性能。然而，CD样本标记非常耗时，因为它标记密集并且需要专业知识。为了缓解这个问题，我们引入了 ChangeAnywhere，一种使用语义潜在扩散模型和单时间图像的新颖 CD 样本生成方法。具体来说，ChangeAnywhere 利用获取大型单时态语义数据集的相对容易性来生成大规模、多样化且带语义注释的双时态 CD 数据集。 ChangeAnywhere抓住了CD样本的两个本质，即变化意味着语义上的不同，而不变化意味着在相同语义约束下的合理变化。我们根据所提出的方法生成了 ChangeAnywhere-100K，这是最大的合成 CD 数据集，包含 100,000 对 CD 样本。正如传输实验所证明的那样，ChangeAnywhere-100K 显着提高了各种基于深度学习的 CD 模型的两个 CD 基准数据集的零样本和少样本性能。本文描述了 ChangeAnywhere 在 CD 样本生成方面的巨大潜力，并演示了模型性能的后续增强。因此，ChangeAnywhere 为遥感 CD 提供了一个强大的工具。所有代码和预训练模型均可在 https://github.com/tangkai-RS/ChangeAnywhere 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.08892</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>HEAT：视觉变压器的头级参数高效适应泰勒展开重要性分数</title>
      <link>https://arxiv.org/abs/2404.08894</link>
      <description><![CDATA[arXiv:2404.08894v1 公告类型：新
摘要：先前的计算机视觉研究广泛探索了如何将预训练的视觉变换器（ViT）应用于下游任务。然而，大量需要调整的参数导致了人们对参数高效迁移学习（PETL）的关注，作为一种通过仅训练参数子集来有效调整大型预训练模型的方法，从而实现参数和存储效率。尽管显着减少的参数在迁移学习场景下表现出了良好的性能，但模型固有的结构冗余仍然留有改进的空间，值得进一步研究。在本文中，我们提出了带有泰勒扩展重要性得分（HEAT）的头级高效适应：一种在头级有效微调 ViT 的简单方法。特别是，采用一阶泰勒展开来计算每个头的重要性得分，称为泰勒展开重要性得分（TIS），表明其对特定任务的贡献。此外，还采用了三种计算 TIS 的策略来最大限度地提高 TIS 的有效性。这些策略从不同的角度计算TIS，反映了参数的不同贡献。除了 ViT 之外，HEAT 还应用于 Swin Transformer 等分层变压器，展示了其在不同变压器架构中的多功能性。通过大量的实验，HEAT 在 VTAB-1K 基准测试中展示了优于最先进的 PETL 方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.08894</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>通过进化策略对 MRI 检测脉络膜转移的不确定性进行量化</title>
      <link>https://arxiv.org/abs/2404.08853</link>
      <description><![CDATA[arXiv:2404.08853v1 公告类型：新
摘要：不确定性量化通过解决人们对可信度日益增长的担忧，在促进放射学人工智能的实际实施方面发挥着至关重要的作用。考虑到在该领域获取大型带注释数据集所面临的挑战，需要一种能够在针对放射学图像定制的小数据人工智能方法中实现不确定性量化的方法。在这项研究中，我们重点关注基于小数据进化策略的深度神经进化（DNE）技术背景下的不确定性量化。具体来说，我们使用 DNE 来训练一个简单的卷积神经网络 (CNN)，并使用眼睛的 MRI 图像进行二元分类。目标是区分正常眼睛和患有转移性肿瘤（称为脉络膜转移瘤）的眼睛。训练集包含 18 个有脉络膜转移的图像和 18 个无肿瘤的图像，而测试集包含的肿瘤与正常人的比例为 15:15。
  我们通过 DNE 训练 CNN 模型权重约 40,000 集，最终在训练集上达到 100% 准确率的收敛。我们保存了所有达到最大训练集准确性的模型。然后，通过将这些模型应用于测试集，我们建立了一种用于不确定性量化的集成方法。保存的模型集为每个测试集图像生成了正常和含肿瘤两类之间的分布。相对频率允许模型预测的不确定性量化。有趣的是，我们发现人类放射科医生所欣赏的主观特征解释了不确定性较高的图像，凸显了人工智能驱动的放射分析中不确定性量化的重要性。]]></description>
      <guid>https://arxiv.org/abs/2404.08853</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>使用事件相机进行在线眼动追踪的轻量级时空网络</title>
      <link>https://arxiv.org/abs/2404.08858</link>
      <description><![CDATA[arXiv:2404.08858v1 公告类型：新
摘要：基于事件的数据在边缘计算环境中经常遇到，其中效率和低延迟至关重要。为了与这些数据交互并利用其丰富的时间特征，我们提出了一种因果时空卷积网络。该解决方案的目标是通过三种方式在资源有限的边缘适用硬件上高效实现：1) 刻意以简单的架构和一组操作（卷积、ReLU 激活）为目标 2) 可以配置为通过缓冲层输出来高效地执行在线推理 3 ）可以通过训练期间的正则化实现超过 90% 的激活稀疏性，从而在基于事件的处理器上实现非常显着的效率提升。此外，我们提出了一种直接作用于事件的通用仿射增强策略，这缓解了基于事件的系统的数据集稀缺问题。我们将我们的模型应用于 AIS 2024 基于事件的眼动追踪挑战赛，在 Kaggle 私人测试集上达到 0.9916 p10 准确率。]]></description>
      <guid>https://arxiv.org/abs/2404.08858</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>EIVEN：使用多模态 LLM 进行高效隐式属性值提取</title>
      <link>https://arxiv.org/abs/2404.08886</link>
      <description><![CDATA[arXiv:2404.08886v1 公告类型：新
摘要： 在电子商务中，从多模态数据中准确提取产品属性值对于提高用户体验和零售商的运营效率至关重要。然而，以前的多模态属性值提取方法通常难以处理嵌入图像或文本中的隐式属性值，严重依赖于广泛的标记数据，并且很容易混淆相似的属性值。为了解决这些问题，我们引入了 EIVEN，这是一种数据和参数高效的生成框架，它率先使用多模式 LLM 进行隐式属性值提取。 EIVEN 利用预训练的 LLM 和视觉编码器丰富的固有知识来减少对标记数据的依赖。我们还引入了一种新颖的比较学习技术，通过强制属性值比较和差异识别来减少模型混乱。此外，我们构建了用于多模式隐式属性值提取的初始开源数据集。我们广泛的实验表明，EIVEN 在提取隐式属性值方面明显优于现有方法，同时需要较少的标记数据。]]></description>
      <guid>https://arxiv.org/abs/2404.08886</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>量化扩散模型图像生成一致性的语义方法</title>
      <link>https://arxiv.org/abs/2404.08799</link>
      <description><![CDATA[arXiv:2404.08799v1 公告类型：新
摘要：在这项研究中，我们确定了对扩散模型中图像生成的可重复性或一致性的可解释的定量评分的需求。我们提出了一种语义方法，使用成对平均 CLIP（对比语言图像预训练）分数作为我们的语义一致性分数。我们应用这个指标来比较两种最先进的开源图像生成扩散模型，Stable Diffusion XL 和 PixArt-{\alpha}，我们发现模型的语义一致性分数之间存在统计上的显着差异。所选模型的语义一致性评分与聚合的人工注释之间的一致性为 94%。我们还探讨了 SDXL 和 LoRA 微调版本的 SDXL 的一致性，发现微调模型在生成的图像中具有显着更高的语义一致性。这里提出的语义一致性分数提供了图像生成对齐的衡量标准，有助于评估特定任务的模型架构，并有助于做出有关模型选择的明智决策。]]></description>
      <guid>https://arxiv.org/abs/2404.08799</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>E3：专家嵌入器集合，用于使用有限数据将合成图像检测器适应新生成器</title>
      <link>https://arxiv.org/abs/2404.08814</link>
      <description><![CDATA[arXiv:2404.08814v2 公告类型：新
摘要：随着生成式人工智能的快速发展，新的合成图像生成器不断快速出现。传统的检测方法在适应这些生成器时面临两个主要挑战：新技术合成图像的取证痕迹可能与训练期间学到的图像有很大不同，并且对这些新生成器的数据访问通常受到限制。为了解决这些问题，我们引入了专家嵌入器集成（E3），这是一种用于更新合成图像检测器的新型持续学习框架。 E3 能够使用最少的训练数据准确检测来自新出现的生成器的图像。我们的方法首先采用迁移学习来开发一套专家嵌入器，每个嵌入器专门研究特定生成器的取证痕迹。然后，专家知识融合网络对所有嵌入进行联合分析，以产生准确可靠的检测决策。我们的实验表明，E3 优于现有的持续学习方法，包括专门为合成图像检测开发的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.08814</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>单图像驱动的 3D 视点训练数据增强可实现有效的葡萄酒标签识别</title>
      <link>https://arxiv.org/abs/2404.08820</link>
      <description><![CDATA[arXiv:2404.08820v1 公告类型：新
摘要：面对复杂图像识别领域训练数据不足的严峻挑战，本文介绍了一种专为葡萄酒标签识别量身定​​制的新型 3D 视点增强技术。该方法通过从单个真实世界的葡萄酒标签图像生成视觉逼真的训练样本来增强深度学习模型的性能，克服文本和徽标的复杂组合带来的挑战。经典的生成对抗网络（GAN）方法无法合成如此复杂的内容组合。我们提出的解决方案利用经过时间考验的计算机视觉和图像处理策略来扩展我们的训练数据集，从而扩大深度学习应用的训练样本范围。这种创新的数据增强方法规避了有限培训资源的限制。通过在 Vision Transformer (ViT) 架构上进行批量三元组度量学习，使用增强训练图像，我们可以获得每个葡萄酒标签最具辨别力的嵌入特征，使我们能够在训练类中对现有葡萄酒标签进行一次性识别或未来新收集的葡萄酒标签在培训中不可用。实验结果表明，与传统的二维数据增强技术相比，识别精度显着提高。]]></description>
      <guid>https://arxiv.org/abs/2404.08820</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>使用合成数据集实现模拟到真实的工业零件分类</title>
      <link>https://arxiv.org/abs/2404.08778</link>
      <description><![CDATA[arXiv:2404.08778v1 公告类型：新
摘要：本文是关于有效利用合成数据来训练工业零件分类的深度神经网络，特别是考虑到与真实世界图像的域差距。为此，我们引入了一个合成数据集，可以作为 Sim-to-Real 挑战的初步测试平台；它包含 6 个工业用例的 17 个对象，包括独立和组装的零件。一些物体子集在形状和反照率方面表现出很大的相似性，以反映工业零件的挑战性情况。所有样本图像都带有或不带有随机背景和后处理，用于评估域随机化的重要性。我们将其称为合成工业零件数据集 (SIP-17)。我们通过对五个最先进的深度网络模型（监督和自监督）的性能进行基准测试来研究 SIP-17 的实用性，仅在合成数据上进行训练，同时在真实数据上进行测试。通过分析结果，我们得出了一些关于使用合成数据进行工业零件分类和进一步开发更大规模合成数据集的可行性和挑战的见解。我们的数据集和代码是公开的。]]></description>
      <guid>https://arxiv.org/abs/2404.08778</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>压力下：基于学习的野外模拟仪表读数</title>
      <link>https://arxiv.org/abs/2404.08785</link>
      <description><![CDATA[arXiv:2404.08785v1 公告类型：新
摘要：我们提出了一种用于读取模拟仪表的可解释框架，该框架可部署在现实世界的机器人系统上。我们的框架将阅读任务分为不同的步骤，以便我们可以检测每个步骤的潜在失败。我们的系统不需要预先了解仪表类型或刻度范围，并且能够提取所使用的单位。我们表明，我们的仪表读数算法能够提取相对读数误差小于 2% 的读数。]]></description>
      <guid>https://arxiv.org/abs/2404.08785</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 CLIP 检测 AI 生成的图像</title>
      <link>https://arxiv.org/abs/2404.08788</link>
      <description><![CDATA[arXiv:2404.08788v1 公告类型：新
摘要：随着人工智能生成图像（AIGI）方法变得更加强大和易于使用，确定图像是真实的还是人工智能生成的已成为一项关键任务。由于 AIGI 缺乏照片的签名，并且有自己独特的模式，因此需要新的模型来确定图像是否是人工智能生成的。在本文中，我们研究了对比语言-图像预训练（CLIP）架构（在大规模互联网规模数据集上进行预训练）执行这种区分的能力。我们在真实图像和来自多个生成模型的 AIGI 上对 CLIP 进行微调，使 CLIP 能够确定图像是否是 AI 生成的，如果是，则确定使用什么生成方法来创建它。我们表明，经过微调的 CLIP 架构能够比专门设计用于检测 AIGI 的架构的模型更好地区分 AIGI。我们的方法将显着增加对 AIGI 检测工具的访问，并减少 AIGI 对社会的负面影响，因为我们的 CLIP 微调程序不需要对公开可用的模型存储库进行架构更改，并且比其他 AIGI 检测模型消耗的 GPU 资源要少得多。]]></description>
      <guid>https://arxiv.org/abs/2404.08788</guid>
      <pubDate>Wed, 17 Apr 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>SCOUT+：实现实用的任务驱动驾驶员视线预测</title>
      <link>https://arxiv.org/abs/2404.08756</link>
      <description><![CDATA[arXiv:2404.08756v1 公告类型：新
摘要：驾驶员视线的准确预测是基于视觉的驾驶员监控和辅助系统的重要组成部分。特别令人感兴趣的是安全关键事件，例如进行机动或穿越十字路口。在这种情况下，驾驶员的视线分布会发生显着变化并且变得难以预测，特别是如果任务和上下文信息是隐式表示的，这在许多最先进的模型中很常见。然而，影响驾驶员注意力的自上而下因素的显式建模通常需要额外的信息和注释，而这些信息和注释可能不容易获得。
  在本文中，我们解决了在实际系统中使用通用数据源对任务和上下文进行有效建模的挑战。为此，我们引入了 SCOUT+，这是一种用于驾驶员视线预测的任务和上下文感知模型，它利用从常用 GPS 数据推断出的路线和地图信息。我们在两个数据集 DR(eye)VE 和 BDD-A 上评估我们的模型，并证明与自下而上的模型相比，使用地图可以改善结果，并达到与依赖于特权地面实况信息的自上而下的模型 SCOUT 相当的性能。代码可在 https://github.com/ykotseruba/SCOUT 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.08756</guid>
      <pubDate>Wed, 17 Apr 2024 06:19:59 GMT</pubDate>
    </item>
    <item>
      <title>“鹰眼和狐狸耳朵”：广义零样本学习的部分原型网络</title>
      <link>https://arxiv.org/abs/2404.08761</link>
      <description><![CDATA[arXiv:2404.08761v1 公告类型：新
摘要：广义零样本学习（GZSL）中的当前方法建立在基本模型的基础上，该模型仅考虑整个图像上的单类属性向量表示。这是对新类别识别过程的过度简化，其中图像的不同区域可能具有来自不同所见类别的属性，因此具有不同的主要属性。考虑到这一点，我们采取了一种根本不同的方法：采用对属性信息敏感的预训练视觉语言检测器（VINVL）来有效地获取区域特征。学习函数将区域特征映射到用于构造类部分原型的区域特定属性注意。我们在由 CUB、SUN 和 AWA2 数据集组成的流行 GZSL 基准上进行了实验，其中与其他流行的基础模型相比，我们提出的零件原型网络 (PPN) 取得了有希望的结果。相应的消融研究和分析表明，我们的方法非常实用，并且在本地化提案可用时比全局属性注意力具有明显的优势。]]></description>
      <guid>https://arxiv.org/abs/2404.08761</guid>
      <pubDate>Wed, 17 Apr 2024 06:19:59 GMT</pubDate>
    </item>
    <item>
      <title>LLM-Seg：连接图像分割和大型语言模型推理</title>
      <link>https://arxiv.org/abs/2404.08767</link>
      <description><![CDATA[arXiv:2404.08767v1 公告类型：新
摘要：理解人类指令来识别目标对象对于感知系统至关重要。近年来，大型语言模型（LLM）的进步为图像分割带来了新的可能性。在这项工作中，我们深入研究了推理分割，这是一项新颖的任务，使分割系统能够通过大语言模型推理来推理和解释隐式用户意图，然后分割相应的目标。我们在推理分割方面的工作有助于方法设计和数据集标记。对于该模型，我们提出了一个名为 LLM-Seg 的新框架。 LLM-Seg 通过掩模提案选择有效连接当前的基础分段任何模型和 LLM。对于数据集，我们提出了一个自动数据生成管道，并构建了一个名为 LLM-Seg40K 的新推理分割数据集。实验表明，与现有方法相比，我们的 LLM-Seg 表现出具有竞争力的性能。此外，我们提出的管道可以有效地生成高质量的推理分割数据集。通过该流程开发的 LLM-Seg40K 数据集可作为训练和评估各种推理分割方法的新基准。我们的代码、模型和数据集位于 https://github.com/wangjunchi/LLMSeg。]]></description>
      <guid>https://arxiv.org/abs/2404.08767</guid>
      <pubDate>Wed, 17 Apr 2024 06:19:59 GMT</pubDate>
    </item>
    <item>
      <title>模拟自上而下对驾驶员注意力影响的数据限制</title>
      <link>https://arxiv.org/abs/2404.08749</link>
      <description><![CDATA[arXiv:2404.08749v1 公告类型：新
摘要：驾驶是一项视觉运动任务，即驾驶员所看到的与他们所做的之间存在联系。虽然一些驾驶员视线模型考虑了驾驶员行为的自上而下的影响，但大多数模型仅学习人类视线和驾驶镜头之间自下而上的相关性。问题的关键是缺乏带有注释的公共数据，这些数据可用于训练自上而下的模型并评估任何类型的模型捕获任务对注意力的影响的效果。因此，自上而下的模型是根据私人数据进行训练和评估的，而公共基准仅衡量与人类数据的整体拟合度。
  在本文中，我们通过检查四个大型公共数据集 DR(eye)VE、BDD-A、MAAD 和 LBW 来关注数据限制，这些数据集用于训练和评估驾驶员注视预测的算法。我们定义了一组已知会影响驾驶员注意力的驾驶任务（横向和纵向操纵）和上下文元素（交叉口和通行权），根据上述定义使用注释增强数据集，并分析数据记录的特征和处理管道 w.r.t.捕捉驾驶员的所见所为。总之，这项工作的贡献是：1）量化公共数据集的偏差，2）检查 SOTA 自下而上模型在涉及重要驾驶员行为的数据子集上的性能，3）链接底层的缺点- 针对数据限制的模型，以及 4) 对未来数据收集和处理的建议。用于重现结果的新注释和代码可在 https://github.com/ykotseruba/SCOUT 上找到。]]></description>
      <guid>https://arxiv.org/abs/2404.08749</guid>
      <pubDate>Wed, 17 Apr 2024 06:19:58 GMT</pubDate>
    </item>
    </channel>
</rss>