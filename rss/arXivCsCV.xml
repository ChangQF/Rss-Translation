<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 27 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过骨架变换实现基于骨架的动作识别的富有表现力的关键点</title>
      <link>https://arxiv.org/abs/2406.18011</link>
      <description><![CDATA[arXiv:2406.18011v1 公告类型：新
摘要：在基于骨架的动作识别领域，传统方法依赖于粗略的身体关键点，无法捕捉细微的人体动作。在这项工作中，我们提出了富有表现力的关键点，结合手和脚的细节来形成细粒度的骨架表示，提高了现有模型在辨别复杂动作方面的判别能力。为了有效地对富有表现力的关键点进行建模，提出了骨架变换策略，通过分配重要性权重来逐步对关键点进行下采样并优先考虑突出的关节。此外，还利用即插即用的实例池模块将我们的方法扩展到多人场景，而不会增加计算成本。七个数据集上的大量实验结果表明，与基于骨架的人体动作识别的最新方法相比，我们的方法更具优势。代码可在 https://github.com/YijieYang23/SkeleT-GCN 获得。]]></description>
      <guid>https://arxiv.org/abs/2406.18011</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:51 GMT</pubDate>
    </item>
    <item>
      <title>DICE：从单个图像中端到端捕获手脸交互的变形</title>
      <link>https://arxiv.org/abs/2406.17988</link>
      <description><![CDATA[arXiv:2406.17988v1 公告类型：新 
摘要：从单个图像重建带有变形的 3D 手脸交互是一项具有挑战性但至关重要的任务，广泛应用于 AR、VR 和游戏。挑战源于单视图手脸交互过程中的自我遮挡、手和脸之间的不同空间关系、复杂的变形以及单视图设置的模糊性。第一种也是唯一一种手脸交互恢复方法 Decaf 引入了一种全局拟合优化，由接触和变形估计网络引导，该网络在带有 3D 注释的工作室收集数据上进行训练。然而，由于 Decaf 依赖于手脸交互数据的 3D 注释，因此优化过程耗时且泛化能力有限。为了解决这些问题，我们提出了 DICE，这是第一种从单个图像进行变形感知手脸交互恢复的端到端方法。 DICE 使用基于 Transformer 的架构同时估计手部和面部的姿势、接触和变形。它的特点是将局部变形场和全局网格顶点位置的回归分解为两个网络分支，增强变形和接触估计，从而实现精确而稳健的手部和面部网格恢复。为了提高通用性，我们提出了一种弱监督训练方法，该方法使用没有 3D 地面实况注释的自然图像来增强训练集，使用现成模型估计的 2D 关键点深度和姿势的对抗先验进行监督。我们的实验表明，DICE 在标准基准和自然数据上在准确性和物理合理性方面实现了最先进的性能。此外，我们的方法在 Nvidia 4090 GPU 上以交互速率（20 fps）运行，而 Decaf 需要超过 15 秒才能处理一张图像。我们的代码将在发布后公开。]]></description>
      <guid>https://arxiv.org/abs/2406.17988</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:50 GMT</pubDate>
    </item>
    <item>
      <title>Changen2：多时相遥感生成变化基础模型</title>
      <link>https://arxiv.org/abs/2406.17998</link>
      <description><![CDATA[arXiv:2406.17998v1 公告类型：新
摘要：深度视觉模型促进了我们对地球表面时间动态的理解，这些模型通常需要大量带标签的多时相图像进行训练。然而，大规模收集、预处理和注释多时相遥感图像并非易事，因为它成本高昂且需要大量知识。在本文中，我们提出了基于生成模型的变化数据生成器，这些生成器成本低廉且自动化，可以缓解这些数据问题。我们的主要思想是模拟随时间变化的随机变化过程。我们将随机变化过程描述为概率图模型 (GPCM)，它将复杂的模拟问题分解为两个更易处理的子问题，即变化事件模拟和语义变化合成。为了解决这两个问题，我们提出了 Changen2，这是一种具有分辨率可扩展扩散变换器的 GPCM，它可以从标记或未标记的单时间图像中生成图像的时间序列及其语义和变化标签。Changen2 是一个生成性变化基础模型，可以通过自监督进行大规模训练，并可以从未标记的单时间图像中产生变化监督信号。与现有的基础模型不同，Changen2 综合变化数据来训练特定于任务的变化检测基础模型。生成的模型具有固有的零样本变化检测能力和出色的可迁移性。实验表明 Changen2 具有出色的时空可扩展性，例如，在 256$^2$ 像素单时间图像上训练的 Changen2 模型可以产生任意长度的时间序列和 1,024$^2$ 像素的分辨率。 Changen2 预训练模型表现出优异的零样本性能（与全监督模型相比，在 LEVIR-CD 上的性能差距缩小至 3%，在 S2Looking 和 SECOND 上的性能差距缩小至约 10%）并且可在多种类型的变化任务中转移。]]></description>
      <guid>https://arxiv.org/abs/2406.17998</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:50 GMT</pubDate>
    </item>
    <item>
      <title>MAGIC：元能力引导的交互式蒸馏链，实现有效且高效的视觉和语言导航</title>
      <link>https://arxiv.org/abs/2406.17960</link>
      <description><![CDATA[arXiv:2406.17960v1 公告类型：新 
摘要：尽管最近大型模型在具身人工智能 (E-AI) 中取得了显着发展，但它们与机器人技术的集成受到参数大小过大和计算需求的阻碍。针对视觉和语言导航 (VLN) 任务（E-AI 中的一项核心任务），本文通过提出一种元能力引导交互式蒸馏链 (MAGIC) 方法，揭示了使用知识蒸馏获得轻量级学生模型的巨大潜力。具体而言，提出了一种元能力知识蒸馏 (MAKD) 框架，用于解耦和细化 VLN 代理所需的元能力。结合元知识随机化加权 (MKRW) 和元知识可转移确定 (MKTD) 模块，分别在元能力和样本级别动态调整聚合权重。超越传统的一步式单向蒸馏，我们提出了一种交互式蒸馏链 (ICoD) 学习策略，允许学生向老师提供反馈，形成一种新的多步骤师生共同进化管道。值得注意的是，在 R2R 测试未见公开排行榜上，我们最小的模型 MAGIC-S（大小仅为老师模型的 5%（11M））在相同训练数据下优于所有以前的方法。此外，我们最大的模型 MAGIC-L 在 SPL 上超越了之前的最先进模型 5.84%，在 SR 上超越了 3.18%。此外，我们从生活环境中收集并注释了一个新数据集，其中 MAGIC-S 表现出了卓越的性能和实时效率。我们的代码可在 https://github.com/CrystalSixone/VLN-MAGIC 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2406.17960</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>通过知识蒸馏方法设计高度约束编码孔径成像系统</title>
      <link>https://arxiv.org/abs/2406.17970</link>
      <description><![CDATA[arXiv:2406.17970v1 公告类型：新
摘要：计算光学成像 (COI) 系统已实现通过光学编码元件 (OCE) 获取高维信号。OCE 将高维信号编码为一个或多个快照，然后使用计算算法对其进行解码。目前，COI 系统通过端到端 (E2E) 方法进行优化，其中 OCE 被建模为神经网络的一层，其余层执行特定的成像任务。然而，通过 E2E 优化的 COI 系统的性能受到这些系统施加的物理约束的限制。本文提出了一种知识蒸馏 (KD) 框架，用于设计高度物理受限的 COI 系统。这种方法采用了 KD 方法，它由师生关系组成，其中高性能、不受约束的 COI 系统（老师）指导优化物理受限的系统（学生），其特点是快照数量有限。我们验证了所提出的方法，使用二进制编码孔径单像素相机进行单色和多光谱图像重建。模拟结果证明了 KD 方案在设计高度物理约束的 COI 系统方面优于传统的 E2E 优化。]]></description>
      <guid>https://arxiv.org/abs/2406.17970</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型和实例分割对全景 X 光片中的牙齿状况进行半监督分类：真实世界数据集评估</title>
      <link>https://arxiv.org/abs/2406.17915</link>
      <description><![CDATA[arXiv:2406.17915v1 公告类型：新
摘要：牙科全景射线照片提供了巨大的诊断机会，但由于标记数据的短缺，训练监督深度学习网络以自动分析这些放射图像受到阻碍。这里，介绍了对这个问题的不同看法。提出了一个半监督学习框架来对全景射线照片上的十三种牙科疾病进行分类，特别强调牙齿。探索了大型语言模型，以根据牙科报告注释最常见的牙科疾病。此外，使用蒙版自动编码器对分类神经网络进行预训练，并使用 Vision Transformer 利用未标记的数据。使用文献中最广泛的两个数据集验证了分析，包括 8,795 张全景射线照片和 8,029 份配对报告和图像。令人鼓舞的是，结果始终达到或超过了马修斯相关系数的基线指标。通过统计分析将所提出的解决方案与人类从业者进行比较，突出了其有效性和性能局限性；基于专家之间的一致程度，该解决方案显示出与初级专家相当的准确度。]]></description>
      <guid>https://arxiv.org/abs/2406.17915</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>热距离：结合独热嵌入和有符号距离嵌入进行分割</title>
      <link>https://arxiv.org/abs/2406.17936</link>
      <description><![CDATA[arXiv:2406.17936v1 公告类型：新
摘要：机器学习模型的好坏取决于它们所适合的数据。因此，在训练模型中使用尽可能多的数据总是更好的选择。哪些数据可用于拟合模型在很大程度上取决于任务的制定。我们引入了 Hot-Distance，这是一种新颖的分割目标，它将有符号边界距离预测的强度与独热编码的灵活性结合起来，以增加可用于聚焦离子束扫描电子显微镜 (FIB-SEM) 中亚细胞结构分割的训练数据量。]]></description>
      <guid>https://arxiv.org/abs/2406.17936</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>ET tu, CLIP? 解决未知环境中的常见对象错误</title>
      <link>https://arxiv.org/abs/2406.17876</link>
      <description><![CDATA[arXiv:2406.17876v1 公告类型：新
摘要：我们介绍了一种简单的方法，该方法采用预先训练的 CLIP 编码器来增强 ALFRED 任务中的模型泛化能力。与以前的文献中 CLIP 取代视觉编码器不同，我们建议通过辅助对象检测目标将 CLIP 用作附加模块。我们在最近提出的 Episodic Transformer 架构上验证了我们的方法，并证明了结合 CLIP 可以提高未见验证集上的任务性能。此外，我们的分析结果支持 CLIP 特别有助于利用对象描述、检测小对象和解释罕见单词。]]></description>
      <guid>https://arxiv.org/abs/2406.17876</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>MLLM 作为视频叙述者：缓解视频时刻检索中的模态不平衡</title>
      <link>https://arxiv.org/abs/2406.17880</link>
      <description><![CDATA[arXiv:2406.17880v1 公告类型：新
摘要：视频时刻检索 (VMR) 旨在根据自然语言查询在未修剪的长视频中定位特定的时间段。现有方法通常存在训练注释不足的问题，即句子通常与前景中突出的视频内容的一小部分匹配，措辞多样性有限。这种内在的模态不平衡导致相当一部分视觉信息与文本不一致。它将跨模态对齐知识限制在有限的文本语料库范围内，从而导致视觉文本建模不理想且通用性差。通过利用多模态大型语言模型 (MLLM) 的视觉文本理解能力，在本文中，我们将 MLLM 作为视频叙述者来生成视频的合理文本描述，从而缓解模态不平衡并增强时间定位。为了有效地保持定位的时间敏感性，我们设计为每个特定的视频时间戳获取文本叙述，并构建一个具有时间信息的结构化文本段落，该段落在时间上与视觉内容对齐。然后，我们在时间感知叙述和相应的视频时间特征之间执行跨模态特征合并，以生成用于查询定位的语义增强视频表示序列。随后，我们引入了一种单模态叙述查询匹配机制，该机制鼓励模型从上下文凝聚性描述中提取互补信息以改进检索。在两个基准上进行的大量实验证明了我们提出的方法的有效性和通用性。]]></description>
      <guid>https://arxiv.org/abs/2406.17880</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>深度驱动的几何提示学习用于腹腔镜肝脏标志检测</title>
      <link>https://arxiv.org/abs/2406.17858</link>
      <description><![CDATA[arXiv:2406.17858v1 公告类型：新
摘要：腹腔镜肝脏手术为外科医生带来了复杂的术中动态环境，区分肝脏内部关键甚至隐藏的结构仍然是一项重大挑战。肝脏解剖标志，例如脊和韧带，是 2D-3D 对准的重要标记，可以显著增强外科医生的空间感知，从而实现精准手术。为了便于检测腹腔镜肝脏标志，我们收集了一个名为 L3D 的新数据集，该数据集包含来自两个医疗站点的 39 名患者的手术视频中的 1,152 帧，并带有详细的标志注释。为了进行基准测试，选择了 12 种主流检测方法并在 L3D 上进行了全面评估。此外，我们提出了一个深度驱动的几何提示学习网络，即 D2GPLand。具体来说，我们设计了一个深度感知提示嵌入 (DPE) 模块，该模块由自监督提示引导，并利用从基于 SAM 的特征中提取的全局深度线索生成语义相关的几何信息。此外，还引入了语义特定的几何增强 (SGA) 方案，通过逆向解剖感知有效地合并 RGB-D 空间和几何信息。实验结果表明，D2GPLand 在 L3D 上获得了最佳性能，DICE 得分为 63.52%，IoU 得分为 48.68%。结合 2D-3D 融合技术，我们的方法可以直接为外科医生在腹腔镜场景中提供直观的指导信息。]]></description>
      <guid>https://arxiv.org/abs/2406.17858</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>通过选择基帧实现突发图像超分辨率</title>
      <link>https://arxiv.org/abs/2406.17869</link>
      <description><![CDATA[arXiv:2406.17869v1 公告类型：新
摘要：连拍图像超分辨率近年来一直是研究的热门话题，因为它能够利用连拍中多个帧之间的互补信息来获得高分辨率图像。在这项工作中，我们探索使用非均匀曝光的连拍照片来应对现实世界的实际场景，方法是引入一个新的基准数据集，称为非均匀曝光连拍图像 (NEBI)，其中包括不同曝光时间的连拍帧，以获得场景内更广泛的辐照度和运动特性。由于非均匀曝光的连拍照片表现出不同程度的退化，将连拍照片的信息融合到第一帧作为基准帧可能无法获得最佳图像质量。为了解决这个限制，我们提出了一种用于非均匀场景的帧选择网络 (FSN)。该网络以即插即用的方式无缝集成到现有的超分辨率方法中，计算成本低。比较分析揭示了非均匀设置对于实际场景的有效性以及我们的 FSN 对于合成/真实 NEBI 数据集的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.17869</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>RACon：检索增强模拟角色运动控制</title>
      <link>https://arxiv.org/abs/2406.17795</link>
      <description><![CDATA[arXiv:2406.17795v1 公告类型：新
摘要：在计算机动画中，驱动具有逼真动作的模拟角色具有挑战性。当前的生成模型虽然能够推广到各种动作，但通常会对最终用户控制的响应能力构成挑战。为了解决这些问题，我们引入了 RACon：检索增强模拟角色运动控制。我们的端到端分层强化学习方法利用检索器和运动控制器。检索器以面向任务的方式从用户指定的数据库中搜索运动专家，从而提高对用户控制的响应能力。然后将选定的运动专家和操纵信号传输到控制器以驱动模拟角色。此外，检索增强鉴别器旨在稳定训练过程。正如我们的实证研究所证明的那样，我们的方法在运动控制的质量和数量上都超越了现有技术。此外，通过切换广泛的数据库进行检索，它可以在运行时适应不同的运动类型。]]></description>
      <guid>https://arxiv.org/abs/2406.17795</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>SUM：通过 Mamba 实现显著性统一，实现视觉注意力建模</title>
      <link>https://arxiv.org/abs/2406.17815</link>
      <description><![CDATA[arXiv:2406.17815v1 公告类型：新
摘要：视觉注意力建模对于解释和优先考虑视觉刺激非常重要，在营销、多媒体和机器人等应用中发挥着重要作用。传统的显着性预测模型，尤其是基于卷积神经网络 (CNN) 或 Transformers 的模型，通过利用大规模注释数据集取得了显著的成功。然而，目前使用 Transformers 的最先进 (SOTA) 模型计算成本高昂。此外，每种图像类型通常都需要单独的模型，缺乏统一的方法。在本文中，我们提出了通过 Mamba 进行显着性统一 (SUM)，这是一种新颖的方法，它将 Mamba 的高效远程依赖关系建模与 U-Net 相结合，为各种图像类型提供统一的模型。使用新颖的条件视觉状态空间 (C-VSS) 块，SUM 可以动态适应各种图像类型，包括自然场景、网页和商业图像，确保跨不同数据类型的通用适用性。我们对五个基准的全面评估表明，SUM 可以无缝适应不同的视觉特征，并且始终优于现有模型。这些结果将 SUM 定位为一种多功能且强大的工具，用于推进视觉注意力建模，提供适用于不同类型视觉内容的强大解决方案。]]></description>
      <guid>https://arxiv.org/abs/2406.17815</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型估计人类色彩概念的细粒度关联</title>
      <link>https://arxiv.org/abs/2406.17781</link>
      <description><![CDATA[arXiv:2406.17781v1 公告类型：新
摘要：概念（无论是抽象的还是具体的）都会在感知色彩空间中引起关联强度的分布，这会影响从对象识别到信息可视化解释等视觉认知的各个方面。虽然先前的研究假设颜色概念关联可以从跨模态经验统计结构中学习，但目前尚不清楚自然环境是否具有这种结构，如果是，学习系统是否能够在没有强大先验约束的情况下发现和利用它。我们通过研究 GPT-4（一种多模态大型语言模型）在无需任何额外训练的情况下估计类似人类的颜色概念关联的能力来解决这些问题。从人类对 71 个颜色集（涵盖感知色彩空间 (\texttt{UW-71})）和抽象程度各异的概念的颜色概念关联评分开始，我们评估了 GPT-4 生成的关联评分对预测人类评分的效果如何。 GPT-4 评分与人类评分相关，其性能可与自动从图像中估计颜色概念关联的最先进方法相媲美。GPT-4 在不同概念上的表现差异可以通过概念的颜色概念关联分布的特殊性来解释。这项研究表明，在互联网的自然环境中表达的语言和感知之间的高阶协方差包含足够的信息来支持学习类似人类的颜色概念关联，并提供了一个存在性证明，即学习系统可以在没有初始约束的情况下编码此类关联。这项研究进一步表明，GPT-4 可用于有效估计广泛概念的颜色关联分布，可能成为设计有效且直观的信息可视化的关键工具。]]></description>
      <guid>https://arxiv.org/abs/2406.17781</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:44 GMT</pubDate>
    </item>
    <item>
      <title>实时神经编织织物渲染</title>
      <link>https://arxiv.org/abs/2406.17782</link>
      <description><![CDATA[arXiv:2406.17782v1 公告类型：新
摘要：机织织物广泛用于逼真的渲染应用，其中实时能力也至关重要。然而，由于其复杂的结构和光学外观，实时渲染逼真的机织织物具有挑战性，如果没有大量样本，就会造成混叠和噪声。这个问题的核心是织物着色模型的多尺度表示，这允许快速范围查询。一些以前的神经方法以对每种材料进行训练为代价来处理这个问题，这限制了它们的实用性。在本文中，我们提出了一种轻量级神经网络来表示不同尺度下不同类型的机织织物。由于机织织物图案的规律性和重复性，我们的网络可以将织物图案和参数编码为一个小的潜在向量，然后由一个小的解码器进行解释，从而能够表示不同类型的织物。通过将像素的足迹作为输入，我们的网络实现了多尺度表示。此外，由于我们的网络结构轻量，因此速度快且占用的存储空间小。因此，我们的方法在 RTX 3090 上实现了以每秒近 60 帧的速度渲染和编辑编织织物，呈现出接近真实质量的质量，并且没有明显的混叠和噪声。]]></description>
      <guid>https://arxiv.org/abs/2406.17782</guid>
      <pubDate>Thu, 27 Jun 2024 06:20:44 GMT</pubDate>
    </item>
    </channel>
</rss>