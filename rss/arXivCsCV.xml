<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 11 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过非对称混合进行面向感知的视频帧插值</title>
      <link>https://arxiv.org/abs/2404.06692</link>
      <description><![CDATA[arXiv:2404.06692v1 公告类型：新
摘要：以前的视频帧插值（VFI）方法遇到了挑战，特别是模糊和重影效果的表现。这些问题可以追溯到两个关键因素：不可避免的运动错误和监督失准。在实践中，运动估计常常被证明容易出错，从而导致特征未对齐。此外，重建损失往往会带来模糊的结果，特别是在未对准的区域。为了缓解这些挑战，我们提出了一种称为 PerVFI（面向感知的视频帧插值）的新范例。我们的方法采用了非对称协同混合模块（ASB），该模块利用双方的特征来协同混合中间特征。一个参考框架强调主要内容，而另一个则提供补充信息。为了对混合过程施加严格的约束，我们引入了一种自学习的稀疏准二进制掩模，它可以有效地减轻输出中的重影和模糊伪影。此外，我们采用基于归一化流的生成器，并利用负对数似然损失来学习输出的条件分布，这进一步有利于生成清晰细腻的细节。实验结果验证了 PerVFI 的优越性，表明与现有方法相比，感知质量有了显着改善。代码可在 \url{https://github.com/mulns/PerVFI} 获取]]></description>
      <guid>https://arxiv.org/abs/2404.06692</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>动态 3D 扫描中运动误差的二项式自补偿</title>
      <link>https://arxiv.org/abs/2404.06693</link>
      <description><![CDATA[arXiv:2404.06693v1 公告类型：新
摘要：相移轮廓测量（PSP）由于其高精度、鲁棒性和像素级特性而在高精度 3D 扫描中受到青睐。然而，动态测量中违反了 PSP 的一个基本假设，即物体应保持静态，使得 PSP 容易受到物体移动的影响，从而导致点云中出现波纹状误差。我们提出了一种逐像素和逐帧的可循环二项式自补偿（BSC）算法，可以有效、灵活地消除四步 PSP 中的运动误差。我们的数学模型表明，通过对由二项式系数加权的连续运动影响相帧求和，运动误差随着二项式阶数的增加呈指数减小，通过运动影响相序列实现自动误差补偿，无需任何中间变量的帮助。大量实验表明，我们的 BSC 在减少运动误差方面优于现有方法，同时实现了与相机采集速率（90 fps）相同的深度图帧速率，从而能够以准单次帧速率进行高精度 3D 重建。]]></description>
      <guid>https://arxiv.org/abs/2404.06693</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>SafeGen：减少文本到图像模型中不安全内容的生成</title>
      <link>https://arxiv.org/abs/2404.06666</link>
      <description><![CDATA[arXiv:2404.06666v1 公告类型：新
摘要：文本到图像（T2I）模型，例如稳定扩散，近年来在从文本描述生成高质量图像方面表现出了卓越的性能。然而，文本到图像模型可能会被欺骗生成不安全工作（NSFW）内容，特别是在性场景中。现有的对策主要集中在过滤不适当的输入和输出，或抑制不适当的文本嵌入，这可以阻止与 NSFW 相关的明确内容（例如，裸体或性感），但仍然可能容易受到看似无辜但恶意的对抗性提示输入的影响。在本文中，我们提出了 SafeGen，这是一个框架，可以通过文本到图像模型以与文本无关的方式减少不安全内容的生成。关键思想是从模型中消除不安全的视觉表示，无论文本输入如何。通过这种方式，文本到图像模型可以抵抗对抗性提示，因为不安全的视觉表示从内部被阻挡。对四个数据集进行的广泛实验证明了 SafeGen 在减少不安全内容生成同时保留良性图像的高保真度方面的有效性。 SafeGen 的性能优于八种最先进的基线方法，并实现了 99.1% 的色情内容删除性能。此外，我们构建的对抗性提示基准为未来开发和评估反 NSFW 生成方法提供了基础。]]></description>
      <guid>https://arxiv.org/abs/2404.06666</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>通过伪标签校正和模态级对齐进行无监督可见红外 ReID</title>
      <link>https://arxiv.org/abs/2404.06683</link>
      <description><![CDATA[arXiv:2404.06683v1 公告类型：新
摘要：无监督可见光-红外行人重识别（UVI-ReID）由于其在无需标记的情况下增强不同环境中的人体检测的潜力而最近受到了极大的关注。以前的方法利用模态内聚类和跨模态特征匹配来实现 UVI-ReID。然而，存在两个挑战：1）在聚类过程中可能会产生噪声伪标签，2）通过匹配可见光和红外模态的边缘分布进行的跨模态特征对齐可能会导致两种模态的不同身份不一致。在本文中，我们首先进行理论分析，引入可解释的泛化上限。基于分析，我们提出了一种新颖的无监督跨模态人员重新识别框架（PRAISE）。具体来说，为了解决第一个挑战，我们提出了一种伪标签校正策略，该策略利用 Beta 混合模型来预测基于错误聚类的网络记忆效应的概率，并通过在对比学习中添加感知项来纠正对应关系。接下来，我们引入一种模态级对齐策略，该策略生成配对的可见光-红外潜在特征，并通过对齐可见光和红外特征的标记函数来学习身份区分和模态不变特征来减少模态间隙。两个基准数据集的实验结果表明，我们的方法比无监督可见 ReID 方法实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.06683</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>多模式文档呈现攻击检测与取证跟踪解缠</title>
      <link>https://arxiv.org/abs/2404.06663</link>
      <description><![CDATA[arXiv:2404.06663v1 公告类型：新
摘要：文档呈现攻击检测（DPAD）是保护文档图像真实性的重要措施。然而，最近的 DPAD 方法需要额外的资源，例如收集额外数据或了解采集设备参数的手动工作。这项工作提出了一种基于多模解缠迹（MMDT）的 DPAD 方法，没有上述缺点。我们首先通过自监督的解缠和合成网络解开重新捕获的痕迹，以增强具有不同内容和布局的文档图像的泛化能力。然后，与仅依赖于 RGB 域中的数据的现有 DPAD 方法不同，我们建议通过自适应多模态适配器明确使用解开的重新捕获的迹线作为变压器主干中的新模态，以有效地融合 RGB/迹线特征。解开痕迹的可视化证实了所提出的方法在不同文档内容中的有效性。对三个基准数据集的广泛实验证明了我们的 MMDT 方法在表示重新捕获失真的取证痕迹方面的优越性。]]></description>
      <guid>https://arxiv.org/abs/2404.06663</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>多模态环境中的深度生成数据同化</title>
      <link>https://arxiv.org/abs/2404.06665</link>
      <description><![CDATA[arXiv:2404.06665v1 公告类型：新
摘要：物理知识和数据的稳健集成是改进计算模拟（例如地球系统模型）的关键。数据同化对于实现这一目标至关重要，因为它提供了一个系统框架，可以通过观测来校准模型输出，其中可以包括遥感图像和地面站测量，并具有不确定性量化。传统方法（包括卡尔曼滤波器和变分方法）本质上依赖于简化线性和高斯假设，并且计算成本可能很高。然而，随着数据驱动方法在计算科学许多领域的快速采用，我们看到了用深度学习（尤其是生成模型）模拟传统数据同化的潜力。特别是，基于扩散的概率框架与数据同化原理有很大的重叠：两者都允许使用贝叶斯逆框架有条件地生成样本。这些模型在文本条件图像生成或图像控制视频合成方面取得了显着的成功。同样，我们可以将数据同化作为观察条件状态校准。在这项工作中，我们提出了 SLAMS：多模态环境中基于分数的潜在同化。具体来说，我们吸收现场气象站数据和异地卫星图像来校准全球垂直温度剖面。通过广泛的消融，我们证明了 SLAMS 即使在低分辨率、嘈杂和稀疏的数据设置中也具有鲁棒性。据我们所知，我们的工作是第一个使用真实世界数据集应用深度生成框架进行多模态数据同化的工作；这是构建强大的计算模拟器（包括下一代地球系统模型）的重要一步。我们的代码位于：https://github.com/yongquan-qu/SLAMS]]></description>
      <guid>https://arxiv.org/abs/2404.06665</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>GeoSynth：上下文感知高分辨率卫星图像合成</title>
      <link>https://arxiv.org/abs/2404.06637</link>
      <description><![CDATA[arXiv:2404.06637v1 公告类型：新
摘要：我们提出了 GeoSynth，一种用于合成具有全局样式和图像驱动布局控制的卫星图像的模型。全局样式控制是通过文本提示或地理位置进行的。它们分别能够指定场景语义或区域外观，并且可以一起使用。我们在配对卫星图像的大型数据集（带有自动生成的标题）和 OpenStreetMap 数据上训练我们的模型。我们评估控制输入的各种组合，包括不同类型的布局控件。结果表明，我们的模型可以生成多样化的高质量图像，并表现出出色的零样本泛化能力。代码和模型检查点可在 https://github.com/mvrl/GeoSynth 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.06637</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>FlameFinder：通过专注的深度度量学习通过烟雾照亮模糊的火焰</title>
      <link>https://arxiv.org/abs/2404.06653</link>
      <description><![CDATA[arXiv:2404.06653v1 公告类型：新
摘要：FlameFinder 是一种深度度量学习 (DML) 框架，旨在在野火监测期间使用消防员无人机的热图像准确检测火焰，即使被烟雾遮蔽也是如此。传统的 RGB 相机在这种情况下难以应对，但热感相机可以捕捉烟雾遮蔽的火焰特征。然而，它们缺乏绝对的热参考点，从而导致误报。为了解决这个问题，FlameFinder 利用成对的热 RGB 图像进行训练。通过从无烟样品中学习潜在火焰特征，该模型对相对热梯度的偏差变得较小。在测试中，它通过分析烟雾的等效热域分布来识别烟雾中的火焰。该方法使用监督和基于距离的聚类指标来提高性能。该框架结合了火焰分割方法和 DML 辅助检测框架。这包括利用中心损失 (CL)、三元组中心损失 (TCL) 和三元组余弦中心损失 (TCCL) 来识别用于分类的最佳聚类代表。然而，中心损失相对于其他损失的主导地位导致模型丢失对它们敏感的特征。为了解决这个限制，提出了一种注意力机制。这种机制允许非均匀的特征贡献，放大了余弦和三元组损失在 DML 框架中的关键作用。此外，它还提高了可解释性、类别歧视，并减少了类别内的差异。因此，所提出的模型在 FLAME2 数据集中超过基线 4.4%，在 FLAME3 数据集中超过基线 7%，以实现清晰的火焰检测精度。此外，与 VGG19、ResNet18 和三个专为火焰检测定制的骨干模型相比，它在模糊场景中展示了增强的类分离。]]></description>
      <guid>https://arxiv.org/abs/2404.06653</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>在基于分数的扩散模型中使用分数嵌入进行高效去噪</title>
      <link>https://arxiv.org/abs/2404.06661</link>
      <description><![CDATA[arXiv:2404.06661v1 公告类型：新
摘要：众所周知，训练基于去噪分数的扩散模型需要数万个 epoch 和大量图像数据来训练模型。在本文中，我们建议提高训练基于分数的扩散模型的效率。我们的方法允许我们减少训练扩散模型所需的时期数。我们通过数值求解对数密度福克普朗克（FP）方程来计算训练前的分数 \textit{before} 来实现这一点。预先计算的分数嵌入到图像中，以鼓励在切片 Wasserstein 距离下进行更快的训练。因此，它还允许我们减少训练神经网络以学习准确分数所需的图像数量。我们通过数值实验证明，与标准的基于分数的扩散模型相比，我们提出的方法具有改进的性能。我们提出的方法可以更快地达到与标准方法相似的质量。]]></description>
      <guid>https://arxiv.org/abs/2404.06661</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>RoadBEV：鸟瞰路面重建</title>
      <link>https://arxiv.org/abs/2404.06605</link>
      <description><![CDATA[arXiv:2404.06605v1 公告类型：新
摘要：路面条件，特别是几何形状，极大地影响自动驾驶汽车的驾驶性能。基于视觉的在线道路重建有望提前捕获道路信息。单目深度估计和立体匹配等现有解决方案的性能较差。最近的鸟瞰 (BEV) 感知技术为更可靠、更准确的重建提供了巨大的潜力。本文统一提出了两种简单而有效的 BEV 道路高程重建模型，名为 RoadBEV-mono 和 RoadBEV-stereo，分别用单目和立体图像估计道路高程。前者根据从图像视图查询的体素特征直接拟合高程值，而后者根据表示左右体素特征之间差异的 BEV 体积有效地识别道路高程模式。富有洞察力的分析揭示了它们的一致性和差异性。在真实数据集上的实验验证了模型的有效性和优越性。 RoadBEV-mono 和 RoadBEV-stereo 的仰角误差分别达到 1.83 厘米和 0.56 厘米。基于单目图像的BEV估计性能提高了50%。我们的模型具有良好的实际应用前景，为自动驾驶中基于视觉的纯电动汽车感知提供了有价值的参考。代码发布于 https://github.com/ztsrxh/RoadBEV。]]></description>
      <guid>https://arxiv.org/abs/2404.06605</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>使用预训练的视觉 Transformer 校准少样本类增量学习的高阶统计量</title>
      <link>https://arxiv.org/abs/2404.06622</link>
      <description><![CDATA[arXiv:2404.06622v1 公告类型：新
摘要：少镜头类增量学习（FSCIL）旨在使模型从很少的数据（5 个样本）适应新的类，而不忘记以前学习的类。最近的多样本 CIL (MSCIL) 工作（使用所有可用的训练数据）利用预训练模型来减少遗忘并实现更好的可塑性。以类似的方式，我们使用在大规模数据集上预训练的 ViT 模型进行小样本设置，这面临着可塑性低的关键问题。 FSCIL 方法从第一个任务的多样本任务开始，学习一个非常好的特征提取器，然后从第二个任务开始转向少样本设置。虽然最近研究的重点是如何学习多镜头第一个任务，以便模型泛化到所有未来的少镜头任务，但我们在这项工作中探索如何使用预训练模型更好地对少镜头数据进行建模，无论第一个任务是如何训练的。受 MSCIL 最近工作的启发，我们探索使用高阶特征统计如何影响少样本类别的分类。我们确定了从少样本数据中获得良好协方差矩阵的主要挑战，并建议根据与多样本基类的语义相似性来校准新类的协方差矩阵。将校准后的特征统计数据与现有方法结合使用，可以显着改进几个 FSCIL 基准上的小样本连续分类。代码可在 https://github.com/dipamgoswami/FSCIL-Calibration 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.06622</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>MambaAD：探索多类无监督异常检测的状态空间模型</title>
      <link>https://arxiv.org/abs/2404.06564</link>
      <description><![CDATA[arXiv:2404.06564v1 公告类型：新
摘要：异常检测领域的最新进展已经见证了基于 CNN 和 Transformer 的方法的有效性。然而，CNN 与长程依赖性作斗争，而 Transformer 则承受着二次计算复杂性的负担。基于 Mamba 的模型凭借其卓越的远程建模和线性效率，引起了广泛关注。这项研究开创了 Mamba 在多类无监督异常检测中的应用，提出了 MambaAD，它由预先训练的编码器和 Mamba 解码器组成，具有多尺度的局部增强状态空间 (LSS) 模块。所提出的 LSS 模块集成了并行级联（混合状态空间）HSS 块和多内核卷积运算，可有效捕获远程和局部信息。 HSS 块利用（混合扫描）HS 编码器，将特征图编码为五种扫描方法和八个方向，从而通过（状态空间模型）SSM 加强全局连接。希尔伯特扫描和八个方向的使用显着改善了特征序列建模。对六个不同异常检测数据集和七个指标的综合实验证明了 SoTA 性能，证实了该方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.06564</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>用于相似性搜索的空间优化紧凑深度度量学习模型</title>
      <link>https://arxiv.org/abs/2404.06593</link>
      <description><![CDATA[arXiv:2404.06593v1 公告类型：新
摘要：在许多计算机视觉任务中，空间优化经常被忽视。过滤器应该能够识别对象的特征，无论它位于图像中的哪个位置。相似性搜索是一项至关重要的任务，其中空间特征决定重要的输出。卷积捕获不同位置的视觉模式的能力是有限的。与卷积相反，卷积核是根据已学习的像素值和参数在每个像素处动态创建的。这项研究表明，利用单层对合特征提取器和紧凑卷积模型可以显着提高相似性搜索的性能。此外，我们通过使用 GELU 激活函数而不是 ReLU 来改进预测。与具有更好性能的紧凑模型相结合的权重参数数量可以忽略不计，使得该模型在现实世界的实现中非常有用。我们提出的模型大小低于 1 兆字节。我们已经在 CIFAR-10、FashionMNIST 和 MNIST 数据集上试验了我们提出的方法和其他模型。我们提出的方法在所有三个数据集上都表现出色。]]></description>
      <guid>https://arxiv.org/abs/2404.06593</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>具有离线扩散增强原型生成功能的免训练开放词汇分割</title>
      <link>https://arxiv.org/abs/2404.06542</link>
      <description><![CDATA[arXiv:2404.06542v1 公告类型：新
摘要：开放词汇语义分割旨在分割以文本形式表达的任意类别。之前的工作已经对大量图像标题对进行了训练，以强制执行像素级多模态对齐。然而，标题提供了有关给定图像语义的全局信息，但缺乏单个概念的直接本地化。此外，大规模数据集的训练不可避免地会带来巨大的计算成本。在本文中，我们提出了 FreeDA，一种用于开放词汇语义分割的免训练扩散增强方法，它利用扩散模型的能力来可视化定位生成的概念和局部全局相似性，以将类别不可知区域与语义类别相匹配。我们的方法涉及一个离线阶段，其中收集文本视觉参考嵌入，从大量标题开始并利用视觉和语义上下文。在测试时，查询这些以支持视觉匹配过程，该过程是通过联合考虑类不可知区域和全局语义相似性来执行的。广泛的分析表明，FreeDA 在五个数据集上实现了最先进的性能，在 mIoU 方面超越了以前的方法超过 7.0 个平均点，并且不需要任何训练。]]></description>
      <guid>https://arxiv.org/abs/2404.06542</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>打印和扫描在异构变形评估场景中的影响</title>
      <link>https://arxiv.org/abs/2404.06559</link>
      <description><![CDATA[arXiv:2404.06559v1 公告类型：新
摘要：人脸变形攻击对人脸识别系统构成了新的威胁。最重要的是，打印和扫描变形图像可能会掩盖变形过程中产生的伪影，这使得变形图像检测变得更加困难。在这项工作中，我们通过一系列异构测试研究了打印和扫描对变形攻击的影响。我们的实验表明，当向人脸识别（FR）提供已打印和扫描的图像（无论其是变形的还是真实的）时，我们可以将 DiM 的错误匹配可能性增加高达 5.64%，将 StyleGAN2 的错误匹配可能性增加 16.00%。 ） 系统。同样，使用 Frechet 初始距离 (FID) 指标，严格打印扫描的变形攻击的执行效果比非打印扫描的数字变形攻击平均强 9.185%。]]></description>
      <guid>https://arxiv.org/abs/2404.06559</guid>
      <pubDate>Thu, 11 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    </channel>
</rss>