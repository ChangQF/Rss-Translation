<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 09 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>图像分辨率对语义分割的影响</title>
      <link>https://arxiv.org/abs/2402.05398</link>
      <description><![CDATA[高分辨率语义分割需要大量的计算资源。该领域的传统方法通常在处理之前缩小输入图像的尺寸，然后将低分辨率输出放大回其原始尺寸。虽然这种策略有效地识别了广泛的区域，但它常常会错过更精细的细节。在这项研究中，我们证明了能够直接生成高分辨率分割的简化模型可以与生成较低分辨率结果的更复杂系统的性能相匹配。通过简化网络架构，我们能够以原始分辨率处理图像。我们的方法利用跨不同尺度的自下而上的信息传播技术，我们的经验表明该技术可以提高分割准确性。我们使用领先的语义分割数据集严格测试了我们的方法。具体来说，对于 Cityscapes 数据集，我们通过应用嘈杂的学生训练技术进一步提高准确性。]]></description>
      <guid>https://arxiv.org/abs/2402.05398</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:48 GMT</pubDate>
    </item>
    <item>
      <title>通过混合集群条件专家的任务定制屏蔽自动编码器</title>
      <link>https://arxiv.org/abs/2402.05382</link>
      <description><![CDATA[Masked Autoencoder~（MAE）是一种流行的自监督学习方法，在模型预训练中取得了可喜的结果。然而，当各种下游任务的数据分布与预训练数据不同时，语义上不相关的预训练信息可能会导致负迁移，从而阻碍 MAE 的可扩展性。为了解决这个问题，我们提出了一种新的基于 MAE 的预训练范例，即集群条件专家混合 (MoCE)，它可以训练一次，但为不同的下游任务提供定制的预训练模型。与专家混合 (MoE) 不同，我们的 MoCE 通过使用聚类条件门仅使用语义相关的图像来训练每个专家。因此，每个下游任务可以分配给使用与下游数据最相似的数据预先训练的定制模型。对 11 个下游任务集合的实验表明，MoCE 的性能平均优于普通 MAE 2.45%。它还在检测和分割方面获得了最先进的自监督学习结果。]]></description>
      <guid>https://arxiv.org/abs/2402.05382</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:47 GMT</pubDate>
    </item>
    <item>
      <title>通过语言引导的示例学习增强零样本计数</title>
      <link>https://arxiv.org/abs/2402.05394</link>
      <description><![CDATA[最近，与类别特定计数（CSC）相比，类别不可知计数（CAC）问题由于其有趣的通用性和卓越的效率而受到越来越多的关注。本文提出了一种新颖的 ExpressCount，通过深入研究语言引导的样本学习来增强零样本对象计数。具体来说，ExpressCount 由创新的面向语言的范例感知器和下游视觉零样本计数管道组成。其中，感知器通过从流行的预训练大型语言模型（LLM）继承丰富的语义先验，致力于从协作语言视觉信号中挖掘准确的样本线索，而计数管道擅长通过双分支和交叉注意方案，有助于高质量的相似性学习。除了在流行的法学硕士和视觉计数任务之间架起一座桥梁之外，表达引导的样本估计还显着提高了用于计算任意类实例的零样本学习能力。此外，设计带有细致语言表达注释的 FSC-147-Express 开创了开发和验证基于语言的计数模型的新场所。大量的实验证明了我们的 ExpressCount 的最先进的性能，甚至展示了与部分 CSC 模型相当的准确性。]]></description>
      <guid>https://arxiv.org/abs/2402.05394</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:47 GMT</pubDate>
    </item>
    <item>
      <title>反扫描：使用色彩校正扩散模型从扫描图像到原始图像</title>
      <link>https://arxiv.org/abs/2402.05350</link>
      <description><![CDATA[大量的模拟信息（即文档和图像）已以扫描副本的形式数字化，以便在数字世界中存储、共享和/或分析。然而，由于物理世界中的打印、存储和扫描过程造成的各种失真，这些内容的质量严重下降。尽管从扫描副本中恢复高质量内容已成为许多产品不可或缺的任务，但尚未对其进行系统性探索，并且据我们所知，没有可用的公共数据集。在本文中，我们将这个问题定义为反扫描，并引入一个新的高质量、大规模数据集，名为 DESCAN-18K。它包含 18K 对在野外收集的原始图像和扫描图像，其中包含多种复杂的退化。为了消除这种复杂的退化，我们提出了一种称为 DescanDiffusion 的新图像恢复模型，由校正全局颜色退化的颜色编码器和消除局部退化的条件去噪扩散概率模型（DDPM）组成。为了进一步提高 DescanDiffusion 的泛化能力，我们还通过再现扫描图像中的显着退化来设计合成数据生成方案。通过全面的实验和分析，我们证明我们的 DescanDiffusion 在客观和主观上优于其他基准，包括商业修复产品。]]></description>
      <guid>https://arxiv.org/abs/2402.05350</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>CIC：具有文化意识的图像字幕框架</title>
      <link>https://arxiv.org/abs/2402.05374</link>
      <description><![CDATA[图像字幕使用 BLIP 等视觉语言预训练模型 (VLP) 从图像生成描述性句子，该模型已得到很大改进。然而，当前的方法缺乏为图像中描绘的文化元素生成详细的描述性说明，例如亚洲文化群体的人们所穿的传统服装。在本文中，我们提出了一个新的框架，\textbf{文化感知图像字幕（CIC）}，它生成字幕并描述从代表文化的图像中的文化视觉元素中提取的文化元素。受到通过适当的提示将视觉模态和大语言模型（LLM）相结合的方法的启发，我们的框架（1）根据图像的文化类别生成问题，（2）使用生成的问题从视觉问答（VQA）中提取文化视觉元素，以及(3) 使用法学硕士和提示生成具有文化意识的字幕。我们对来自 4 个不同文化群体的 45 名参与者进行了人类评估，这些参与者对相应文化有高度的了解，结果表明，与基于 VLP 的图像字幕基线相比，我们提出的框架生成了更具文化描述性的字幕。我们的代码和数据集将在接受后公开发布。]]></description>
      <guid>https://arxiv.org/abs/2402.05374</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>得到你想要的，而不是你不想要的：文本到图像扩散模型的图像内容抑制</title>
      <link>https://arxiv.org/abs/2402.05375</link>
      <description><![CDATA[最近的文本到图像扩散模型的成功很大程度上归功于它们能够由复杂的文本提示引导，这使得用户能够精确地描述所需的内容。然而，这些模型很难有效地抑制不需要的内容的生成，这些内容被明确要求从提示中生成的图像中省略。在本文中，我们分析了如何操作文本嵌入并从中删除不需要的内容。我们引入两个贡献，我们将其称为 $\textit{软加权正则化}$ 和 $\textit{推理时文本嵌入优化}$。第一个规则化文本嵌入矩阵并有效抑制不需要的内容。第二种方法旨在进一步抑制提示中不想要的内容的产生，并鼓励想要的内容的产生。我们通过大量实验对我们的方法进行定量和定性评估，验证其有效性。此外，我们的方法可推广到像素空间扩散模型（即 DeepFloyd-IF）和潜在空间扩散模型（即稳定扩散）。]]></description>
      <guid>https://arxiv.org/abs/2402.05375</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>双解纠缠深度多重聚类</title>
      <link>https://arxiv.org/abs/2402.05310</link>
      <description><![CDATA[近年来，多重聚类因其从不同角度揭示数据的多个隐藏结构的潜力而引起了广泛关注。大多数多重聚类方法首先通过控制特征之间的不相似性来导出特征表示，然后采用传统的聚类方法（例如k-means）来实现最终的多重聚类结果。然而，学习到的特征表示与不同聚类的最终目标的相关性较弱。此外，这些特征通常不是为了聚类的目的而明确学习的。因此，在本文中，我们通过学习解缠结表示提出了一种新颖的双解缠结深度多重聚类方法，称为 DDMC。具体来说，DDMC 是通过变分期望最大化 (EM) 框架实现的。在E步骤中，解缠学习模块采用粗粒度和细粒度的解缠表示来从数据中获取更多样化的潜在因子集。在 M 步骤中，聚类分配模块利用聚类目标函数来增强聚类输出的有效性。我们的大量实验表明，DDMC 在七个常用任务中始终优于最先进的方法。我们的代码可在 https://github.com/Alexander-Yao/DDMC 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.05310</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:45 GMT</pubDate>
    </item>
    <item>
      <title>废弃网络以进行早期野火检测</title>
      <link>https://arxiv.org/abs/2402.05349</link>
      <description><![CDATA[早期发现野火对于快速响应工作至关重要，从而最大限度地减少野火蔓延的负面影响。为此，我们提出了 \Pyro，一个基于网络抓取的数据集，由来自摄像机网络的野火视频组成，并通过手动边界框级注释进行了增强。我们的数据集根据提高数据质量和多样性的策略进行过滤，将最终数据减少到一组 10,000 张图像。我们使用最先进的对象检测模型进行了实验，发现所提出的数据集具有挑战性，并且将其与其他公共数据集结合使用有助于总体上获得更高的结果。我们将公开我们的代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2402.05349</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:45 GMT</pubDate>
    </item>
    <item>
      <title>BIKED++：包含 140 万张自行车图像和参数化 CAD 设计的多模式数据集</title>
      <link>https://arxiv.org/abs/2402.05301</link>
      <description><![CDATA[本文介绍了一个包含 140 万个程序生成的自行车设计的公共数据集，这些自行车设计以参数化方式表示为 JSON 文件和光栅化图像。该数据集是通过使用渲染引擎创建的，该渲染引擎利用 BikeCAD 软件从参数化设计生成矢量图形。该渲染引擎在论文中进行了讨论，并与数据集一起公开发布。尽管该数据集有许多应用，但主要动机是需要在参数化和基于图像的设计表示之间训练跨模式预测模型。例如，我们证明可以训练预测模型来直接根据参数表示准确估计对比语言图像预训练（CLIP）嵌入。这允许在参数自行车设计和文本字符串或参考图像之间建立相似关系。经过训练的预测模型也被公开。该数据集加入了 BIKED 数据集系列，其中包括数千个混合表示的人类设计的自行车模型和多个量化设计性能的数据集。代码和数据集可以在以下位置找到：https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main]]></description>
      <guid>https://arxiv.org/abs/2402.05301</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>基于跨模型半监督学习的道路检测知识蒸馏</title>
      <link>https://arxiv.org/abs/2402.05305</link>
      <description><![CDATA[知识蒸馏的进步在实现知识从较大的教师模型转移到更小、更高效的学生模型方面发挥了至关重要的作用，并且对于在线和资源受限的应用程序尤其有利。学生模型的有效性在很大程度上取决于从教师那里获得的知识的质量。鉴于未标记遥感数据的可访问性，半监督学习已成为增强模型性能的流行策略。然而，由于特征提取能力有限，仅依靠较小模型的半监督学习可能是不够的。这种限制限制了他们利用训练数据的能力。为了解决这个问题，我们提出了一种结合知识蒸馏和半监督学习方法的综合方法。这种混合方法利用大型模型的强大功能来有效利用大量未标记数据，同时随后为小型学生模型提供丰富且信息丰富的功能以进行增强。所提出的基于半监督学习的知识蒸馏（SSLKD）方法在道路分割的应用中证明了学生模型的性能显着提高，超越了传统半监督学习方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.05305</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>驾驶模拟器中基于 Kinect 和 Oculus 的注视区域估计方法的比较分析</title>
      <link>https://arxiv.org/abs/2402.05248</link>
      <description><![CDATA[驾驶员的注视信息在驾驶研究中至关重要，因为它与驾驶员注意力相关。特别是，在驾驶模拟器中纳入注视数据扩大了研究范围，因为它们可以将驾驶员的注视模式与其特征和表现联系起来。在本文中，我们提出了集成在驾驶模拟器中的两个注视区域估计模块。一种使用 3D Kinect 设备，另一种使用虚拟现实 Oculus Rift 设备。这些模块能够在驾驶场景划分的七个区域中检测驾驶员在每个路线处理帧中注视的区域。实施并比较了四种用于注视估计的方法，这些方法了解注视位移和头部运动之间的关系。其中两个更简单，基于尝试捕获这种关系的点，另外两个基于 MLP 和 SVM 等分类器。实验由 12 位用户在同一场景中驾驶两次，每次使用不同的可视化显示，首先使用大屏幕，后来使用 Oculus Rift。总体而言，Oculus Rift 的表现优于 Kinect，成为凝视估计的最佳硬件。性能最高的基于Oculus的注视区域估计方法达到了97.94%的准确率。 Oculus Rift模块提供的信息丰富了驾驶模拟器数据，除了通过Oculus提供的虚拟现实体验获得沉浸感和真实感之外，还可以进行多模态驾驶性能分析。]]></description>
      <guid>https://arxiv.org/abs/2402.05248</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:43 GMT</pubDate>
    </item>
    <item>
      <title>通过残差学习对水下图像进行物理知情和数据驱动的模拟</title>
      <link>https://arxiv.org/abs/2402.05281</link>
      <description><![CDATA[一般来说，水下图像会出现颜色失真和对比度低的问题，因为光在水中传播时会衰减和反向散射（具体取决于波长和水体的特性）。现有的简单退化模型（类似于大气图像“雾化”效应）虽然有帮助，但不足以正确表示水下图像退化，因为存在无法解释和不可测量的因素，例如由于水的浑浊度、浑浊介质的反射特性等导致的光散射。我们提出了一种基于深度学习的架构来自动模拟水下效果，其中网络只知道类似去雾的图像形成方程，并且由于如果以数据驱动的方式推断出其他未知因素。我们只使用RGB图像（因为在实时场景中深度图像不可用）来估计深度图像。为了进行测试，我们提出了（由于缺乏真实的水下图像数据集）一个复杂的图像形成模型/方程来手动生成类似于真实水下图像（用作地面实况）的图像。然而，只有经典的图像形成方程（用于图像去雾的方程）被告知网络。这模仿了这样一个事实：在真实场景中，物理学永远不会完全已知，只有简化的模型是已知的。与其他纯数据驱动的方法相比，由于由复杂的图像形成方程生成的基本事实，我们可以成功地对所提出的技术进行定性和定量评估]]></description>
      <guid>https://arxiv.org/abs/2402.05281</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:43 GMT</pubDate>
    </item>
    <item>
      <title>$\lambda$-ECLIPSE：利用 CLIP 潜在空间的多概念个性化文本到图像扩散模型</title>
      <link>https://arxiv.org/abs/2402.05195</link>
      <description><![CDATA[尽管个性化文本到图像 (P-T2I) 生成模型最近取得了进展，但主题驱动的 T2I 仍然具有挑战性。主要瓶颈包括 1) 密集的培训资源需求，2) 超参数敏感性导致输出不一致，以及 3) 平衡新颖的视觉概念和构图对齐的复杂性。我们首先重申 T2I 扩散模型的核心理念，以解决上述局限性。当代主题驱动的 T2I 方法主要依赖于潜在扩散模型 (LDM)，该模型通过交叉注意层促进 T2I 映射。虽然 LDM 具有明显的优势，但 P-T2I 方法对这些扩散模型的潜在空间的依赖显着增加了资源需求，导致结果不一致，并且需要对单个所需图像进行多次迭代。最近，ECLIPSE 展示了一种更资源有效的途径来训练基于 UnCLIP 的 T2I 模型，避免了扩散文本到图像先验的需要。在此基础上，我们引入了 $\lambda$-ECLIPSE。我们的方法说明有效的 P-T2I 不一定依赖于扩散模型的潜在空间。 $\lambda$-ECLIPSE 只需 34M 个参数即可实现单主体、多主体和边缘引导的 T2I 个性化，并使用 160 万图像文本交错数据在仅 74 个 GPU 小时上进行训练。通过大量的实验，我们还确定 $\lambda$-ECLIPSE 在组合对齐方面超越了现有基线，同时保留了概念对齐性能，即使资源利用率显着降低。]]></description>
      <guid>https://arxiv.org/abs/2402.05195</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:42 GMT</pubDate>
    </item>
    <item>
      <title>SPAD：空间感知多视图扩散器</title>
      <link>https://arxiv.org/abs/2402.05235</link>
      <description><![CDATA[我们提出了 SPAD，这是一种从文本提示或单个图像创建一致的多视图图像的新颖方法。为了实现多视图生成，我们通过跨视图交互扩展其自注意力层来重新利用预训练的 2D 扩散模型，并在 Objaverse 的高质量子集上对其进行微调。我们发现，先前工作（例如 MVDream）中提出的自注意力的天真扩展会导致视图之间的内容复制。因此，我们明确地约束基于极几何的交叉视图注意力。为了进一步增强 3D 一致性，我们利用从相机光线导出的 Plucker 坐标并将它们作为位置编码注入。这使得 SPAD 能够很好地推理 3D 空间邻近度。与最近只能在固定方位角和仰角生成视图的作品相比，SPAD 提供了完整的相机控制，并在 Objaverse 和 Google Scanned Objects 数据集中未见过的物体上实现了最先进的视图合成结果。最后，我们证明使用 SPAD 生成文本到 3D 可以防止多面 Janus 问题。请访问我们的网页查看更多详细信息：https://yashkant.github.io/spad]]></description>
      <guid>https://arxiv.org/abs/2402.05235</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:42 GMT</pubDate>
    </item>
    <item>
      <title>通过针对不同文档类型的专业模型和先进技术增强孟加拉语 OCR</title>
      <link>https://arxiv.org/abs/2402.05158</link>
      <description><![CDATA[本研究论文提出了一种具有某些功能的独特孟加拉语 OCR 系统。该系统擅长重建文档布局，同时保留结构、对齐方式和图像。它结合了先进的图像和签名检测功能，可实现准确提取。专门的分词模型可满足不同的文档类型，包括计算机撰写、凸版印刷、打字机和手写文档。该系统处理静态和动态手写输入，识别各种书写风格。此外，它还能够识别孟加拉语的复合字符。广泛的数据收集工作提供了多样化的语料库，而先进的技术组件则优化了字符和单词识别。其他贡献包括图像、徽标、签名和表格识别、透视校正、布局重建以及用于高效和可扩展处理的排队模块。该系统在高效、准确的文本提取和分析方面表现出了出色的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.05158</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    </channel>
</rss>