<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 28 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>基于视频的个性化手部分类法：适用于脊髓损伤患者</title>
      <link>https://arxiv.org/abs/2403.18094</link>
      <description><![CDATA[arXiv:2403.18094v1 公告类型：新
摘要：手部功能对于我们的互动和生活质量至关重要。脊髓损伤 (SCI) 会损害手部功能，降低独立性。对家庭和社区环境中的功能进行全面评估需要对手部功能受损的个体进行手部抓握分类。由于标准分类法中没有代表性的掌握类型、不同损伤级别的数据分布不均匀以及数据有限，开发这样的分类法具有挑战性。本研究旨在使用语义聚类自动识别以自我为中心的视频中占主导地位的不同手部抓握。使用在 19 名颈椎 SCI 患者家中收集的以自我为中心的视频记录，对具有语义意义的抓取动作进行聚类。采用集成姿势和外观数据的深度学习模型来创建个性化的手部分类。定量分析显示簇纯度为 67.6% ± 24.2%，冗余度为 18.0% ± 21.8%。定性评估揭示了视频内容中有意义的集群。该方法提供了一种灵活有效的策略来分析野外手部功能。它为研究人员和临床医生提供了评估手部功能、帮助敏感评估和定制干预计划的有效工具。]]></description>
      <guid>https://arxiv.org/abs/2403.18094</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>全范围头部姿势估计的数学基础和修正</title>
      <link>https://arxiv.org/abs/2403.18104</link>
      <description><![CDATA[arXiv:2403.18104v1 公告类型：新
摘要：有关头部姿势估计（HPE）的许多工作提供了算法或提出的基于神经网络的方法，用于从面部关键点或直接从头部区域的图像中提取欧拉角。然而，许多著作未能提供所使用的坐标系和欧拉或泰特-布莱恩角阶的明确定义。众所周知，旋转矩阵依赖于坐标系，并且偏航角、滚动角和俯仰角对其应用顺序敏感。如果没有精确的定义，验证先前作品中使用的输出头部姿势和绘图例程的正确性就变得具有挑战性。在本文中，我们彻底检查了 300W-LP 数据集中定义的欧拉角、3DDFA-v2、6D-RepNet、WHENet 等头部姿势估计及其欧拉角绘制例程的有效性。必要时，我们从提供的代码中推断出它们的坐标系以及偏航、横滚、俯仰的顺序。本文介绍了（1）用于从提供的源代码推断坐标系的代码和算法、欧拉角应用顺序的代码以及提取精确旋转矩阵和欧拉角的代码和算法，（2）用于将姿态从一种旋转系统转换为另一种旋转系统的代码和算法， (3) 旋转矩阵二维增强的新颖公式，以及 (4) 旋转矩阵和姿势的正确绘图例程的推导和代码。本文还讨论了在维基百科和 SciPy 中使用右手坐标系定义旋转的可行性，这使得欧拉角提取更容易用于全范围头部姿势研究。]]></description>
      <guid>https://arxiv.org/abs/2403.18104</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>细分任何扩展的医疗模型</title>
      <link>https://arxiv.org/abs/2403.18114</link>
      <description><![CDATA[arXiv:2403.18114v1 公告类型：新
摘要：分段任意模型（SAM）因其通用性而引起了医学图像分割研究人员的极大关注。然而，研究人员发现，与最先进的非基础模型相比，SAM 在医学图像上的性能可能有限。无论如何，社区看到了扩展、微调、修改和评估 SAM 用于医学成像分析的潜力。越来越多的著作集中在上述四个方向上，并提出了 SAM 的变体。为此，统一平台有助于突破医学图像基础模型的边界，促进 SAM 及其变体在医学图像分割中的使用、修改和验证。在这项工作中，我们引入了 SAMM Extended (SAMME)，这是一个集成新的 SAM 变体模型、采用更快的通信协议、适应新的交互模式并允许对模型子组件进行微调的平台。这些功能可以扩展 SAM 等基础模型的潜力，并且结果可以转化为图像引导治疗、混合现实交互、机器人导航和数据增强等应用。]]></description>
      <guid>https://arxiv.org/abs/2403.18114</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>EgoPoseFormer：以自我为中心的 3D 人体姿势估计的简单基线</title>
      <link>https://arxiv.org/abs/2403.18080</link>
      <description><![CDATA[arXiv:2403.18080v1 公告类型：新
摘要：我们提出了 EgoPoseFormer，这是一种简单而有效的基于 Transformer 的模型，用于立体自我中心人体姿势估计。以自我为中心的姿态估计的主要挑战是克服关节不可见性，这是​​由头戴式摄像头的自遮挡或有限视场（FOV）引起的。我们的方法通过结合两阶段姿态估计范例克服了这一挑战：在第一阶段，我们的模型利用全局信息来估计每个关节的粗略位置，然后在第二阶段，它采用 DETR 样式转换器来细化粗略位置通过利用细粒度的立体视觉特征。此外，我们提出了一种可变形立体操作，使我们的 Transformer 能够有效处理多视图特征，从而能够准确定位 3D 世界中的每个关节。我们在立体 UnrealEgo 数据集上评估了我们的方法，结果表明它在计算效率方面显着优于以前的方法：与现有技术相比，它仅将 MPJPE 提高了 27.4mm（提高了 45%），模型参数仅为 7.9%，FLOP 为 13.1% -艺术。令人惊讶的是，通过适当的训练技术，我们发现即使是我们的第一阶段姿势提议网络也可以实现比以前的艺术更出色的性能。我们还表明，我们的方法可以无缝扩展到单目设置，这在 SceneEgo 数据集上实现了最先进的性能，与模型仅 60.7% 的现有最佳方法相比，MPJPE 提高了 25.5mm（提高了 21%）参数和 36.4% 的 FLOPs。]]></description>
      <guid>https://arxiv.org/abs/2403.18080</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>OCAI：通过遮挡和一致性感知插值改进光流估计</title>
      <link>https://arxiv.org/abs/2403.18092</link>
      <description><![CDATA[arXiv:2403.18092v1 公告类型：新
摘要：地面实况标签的稀缺对开发具有通用性和鲁棒性的光流估计模型提出了一项重大挑战。虽然当前的方法依赖于数据增强，但它们尚未充分利用标记视频序列中可用的丰富信息。我们提出了 OCAI，这是一种通过生成中间视频帧以及其间的光流来支持鲁棒帧插值的方法。 OCAI 利用前向扭曲方法，利用遮挡感知来解决像素值的模糊性，并利用光流的前后向一致性来填充缺失值。此外，我们在插值帧之上引入了师生式半监督学习方法。使用一对未标记的帧和教师模型的预测光流，我们生成插值帧和光流来训练学生模型。使用学生的指数移动平均来维持教师的权重。我们的评估表明，在 Sintel 和 KITTI 等既定基准上，我们具有感知上卓越的插值质量和增强的光流精度。]]></description>
      <guid>https://arxiv.org/abs/2403.18092</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>深度学习在跟踪和检测海洋垃圾方面的最先进应用：一项调查</title>
      <link>https://arxiv.org/abs/2403.18067</link>
      <description><![CDATA[arXiv:2403.18067v1 公告类型：新
摘要：深度学习技术在海洋垃圾问题中的探索已有大约 20 年，但大多数研究在过去五年中发展迅速。我们对海洋垃圾深度学习的 28 项最新且重要的贡献进行了深入、最新的总结和分析。从交叉引用研究论文结果来看，YOLO 系列明显优于所有其他目标检测方法，但该领域有许多受人尊敬的贡献者明确同意，目前还没有用于机器学习的水下碎片综合数据库。使用我们整理和标记的小数据集，我们在二元分类任务上测试了 YOLOv5，发现准确率较低且误报率较高；强调了综合数据库的重要性。我们在本次调查中提出了 40 多项未来研究建议和开放挑战。]]></description>
      <guid>https://arxiv.org/abs/2403.18067</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>每个镜头都很重要：使用示例在视频中进行重复计数</title>
      <link>https://arxiv.org/abs/2403.18074</link>
      <description><![CDATA[arXiv:2403.18074v1 公告类型：新
摘要：视频重复计数是指视频中重复动作或运动的重复次数。我们提出了一种基于样本的方法，可以发现目标视频中重复的视频样本的视觉对应关系。我们提出的“Every Shot Counts”（ESCounts）模型是一种基于注意力的编码器-解码器，可对不同长度的视频以及来自相同和不同视频的样本进行编码。在训练中，ESCounts 会回归与视频中的样本高度对应的位置。同时，我们的方法学习了一个编码一般重复运动表示的潜在变量，我们将其用于无样本、零样本推理。对常用数据集（RepCount、Countix 和 UCFRep）进行的广泛实验展示了 ESCount 在所有三个数据集上获得了最先进的性能。在 RepCount 上，ESCounts 将偏差从 0.39 增加到 0.56，并将平均绝对误差从 0.38 减少到 0.21。详细的消融进一步证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.18074</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>用于大型转型的全球点云注册网络</title>
      <link>https://arxiv.org/abs/2403.18040</link>
      <description><![CDATA[arXiv:2403.18040v1 公告类型：新
摘要：三维数据配准是一个既定但具有挑战性的问题，在许多不同的应用中都至关重要，例如为自动驾驶车辆绘制环境图，以及为创建头像而对对象和人物进行建模等。配准是指通过匹配对应关系和变换估计将多个数据映射到同一坐标系的过程。为此，新颖的提案利用了深度学习架构的优势，因为它们学习数据的最佳特征，提供更好的匹配，从而提供更好的结果。然而，现有技术通常集中于相对较小的变换的情况，尽管在某些应用中以及在真实和实际的环境中，大的变换是非常常见的。在本文中，我们提出了 ReLaTo（大型变换注册），这是一种面对发生大型变换的情况同时保持局部变换良好性能的架构。该提案使用新颖的 Softmax 池化层以双边共识的方式查找两个点集之间的对应关系，对最置信的匹配进行采样。这些匹配用于使用加权奇异值分解 (SVD) 来估计粗略的全局配准。然后将目标引导的去噪步骤应用于获得的匹配和潜在特征，并考虑局部几何形状来估计最终的精细配准。所有这些步骤都是按照端到端方法进行的，该方法已被证明可以改进该任务常用的两个数据集（ModelNet40 和 KITTI）中的 10 种最先进的配准方法，特别是在大转变。]]></description>
      <guid>https://arxiv.org/abs/2403.18040</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:20 GMT</pubDate>
    </item>
    <item>
      <title>谱卷积变换器：协调视觉变换器的实数与复杂多视图谱算子</title>
      <link>https://arxiv.org/abs/2403.18063</link>
      <description><![CDATA[arXiv:2403.18063v1 公告类型：新
摘要：已经通过不同的架构 - ViT、PVT 和 Swin 对视觉中使用的 Transformer 进行了研究。这些努力改善了注意力机制并使其更加高效。不同的是，人们认为需要包含本地信息，从而导致在 CPVT 和 CvT 等转换器中合并卷积。使用复杂的傅立叶基捕获全局信息，通过各种方法（例如 AFNO、GFNet 和 Spectformer）实现全局令牌混合。我们主张结合三种不同的数据观点——本地、全局和长期依赖。我们还仅使用通过哈特利变换获得的实域谱表示来研究最简单的全局表示。我们在初始层中使用卷积算子来捕获局部信息。通过这两项贡献，我们能够优化并获得频谱卷积变换器（SCT），该变换器在减少参数数量的同时，比最先进的方法提供更高的性能。通过大量实验，我们表明 SCT-C-small 在 ImageNet 数据集上提供了最先进的性能，达到 84.5% top-1 准确率，而 SCT-C-Large 达到 85.9%，SCT-C-巨大达到86.4%。我们在 CIFAR-10、CIFAR-100、Oxford Flower 和 Stanley Car 等数据集上评估 SCT 在迁移学习方面的表现。我们还评估了下游任务上的 SCT，即 MSCOCO 数据集上的实例分割。该项目页面可在此网页上找到。\url{https://github.com/badripatro/sct}]]></description>
      <guid>https://arxiv.org/abs/2403.18063</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:20 GMT</pubDate>
    </item>
    <item>
      <title>说走就走，能互动就说：具有场景可供性的语言引导人体动作生成</title>
      <link>https://arxiv.org/abs/2403.18036</link>
      <description><![CDATA[arXiv:2403.18036v1 公告类型：新
摘要：尽管文本到动作合成取得了重大进展，但在 3D 环境中生成语言引导的人体动作仍面临巨大的挑战。这些挑战主要源于 (i) 缺乏能够对自然语言、3D 场景和人体运动进行联合建模的强大生成模型，以及 (ii) 生成模型的密集数据需求与缺乏全面、高质量、语言场景运动数据集。为了解决这些问题，我们引入了一种新颖的两阶段框架，该框架采用场景可供性作为中间表示，有效地将 3D 场景基础和条件运动生成联系起来。我们的框架包括用于预测显式可供性图的可供性扩散模型（ADM）和用于生成合理的人体运动的可供性运动扩散模型（AMDM）。通过利用场景可供性图，我们的方法克服了在多模态条件信号下生成人体运动的困难，特别是在使用缺乏广泛语言场景运动对的有限数据进行训练时。我们广泛的实验表明，我们的方法始终优于既定基准上的所有基准，包括 HumanML3D 和 HUMANISE。此外，我们还在专门策划的评估集上验证了我们的模型卓越的泛化能力，该评估集具有以前未见过的描述和场景。]]></description>
      <guid>https://arxiv.org/abs/2403.18036</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>TGGLinesPlus：一种强大的拓扑图引导计算机视觉算法，用于图像中的线条检测</title>
      <link>https://arxiv.org/abs/2403.18038</link>
      <description><![CDATA[arXiv:2403.18038v1 公告类型：新
摘要： 线检测是图像处理、计算机视觉和机器智能中的一个经典且重要的问题。线检测有许多重要的应用，包括图像矢量化（例如文档识别和艺术设计）、室内测绘和重要的社会挑战（例如从卫星图像中提取海冰断裂线）。已经开发了许多线条检测算法和方法，但仍然缺乏鲁棒且直观的方法。在本文中，我们提出并实现了一种拓扑图引导算法，名为 TGGLinesPlus，用于线条检测。我们对来自广泛领域的图像进行的实验证明了我们的 TGGLinesPlus 算法的灵活性。我们还使用五种经典且最先进的线条检测方法对我们的算法进行了基准测试，结果证明了 TGGLinesPlus 的稳健性。我们希望 TGGLinesPlus 的开源实现能够激发空间科学重要的许多应用并为其铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2403.18038</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>文本是质量：文本视频检索的随机嵌入建模</title>
      <link>https://arxiv.org/abs/2403.17998</link>
      <description><![CDATA[arXiv:2403.17998v1 公告类型：新
摘要：视频剪辑的日益流行引发了人们对文本视频检索日益增长的兴趣。最近的进展集中在为文本和视频建立联合嵌入空间，依靠一致的嵌入表示来计算相似性。然而，现有数据集中的文本内容普遍简短，难以完整描述视频的冗余语义。相应地，单个文本嵌入可能缺乏表达能力来捕获视频嵌入并增强检索能力。在本研究中，我们提出了一种新的随机文本建模方法 T-MASS，即将文本建模为随机嵌入，以灵活且有弹性的语义范围丰富文本嵌入，从而产生文本质量。具体来说，我们引入了一个相似性感知半径模块，以根据给定的文本-视频对调整文本质量的比例。另外，我们设计和开发了支持文本正则化，以进一步控制训练期间的文本质量。推理管道也经过定制，可以充分利用文本量进行准确检索。经验证据表明，T-MASS 不仅有效地吸引相关的文本视频对，同时远离不相关的文本视频对，而且还能够确定相关对的精确文本嵌入。我们的实验结果表明 T-MASS 较基线有显着改善（R@1 为 3% 至 6.3%）。此外，T-MASS 在五个基准数据集上实现了最先进的性能，包括 MSRVTT、LSMDC、DiDeMo、VATEX 和 Charades。]]></description>
      <guid>https://arxiv.org/abs/2403.17998</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>SpectralWaste 数据集：废物分类自动化的多模式数据</title>
      <link>https://arxiv.org/abs/2403.18033</link>
      <description><![CDATA[arXiv:2403.18033v1 公告类型：新
【摘要】：不可生物降解废物的增加引起了全世界的关注。回收设施发挥着至关重要的作用，但其自动化受到废物回收线复杂特征（如杂乱或物体变形）的阻碍。此外，缺乏针对这些环境的公开可用标记数据使得开发强大的感知系统具有挑战性。我们的工作探索了多模态感知在实际废物管理场景中对对象分割的好处。首先，我们介绍 SpectralWaste，这是从运行中的塑料废物分类设施收集的第一个数据集，该设施提供同步的高光谱和传统 RGB 图像。该数据集包含几类物体的标签，这些物体通常出现在分拣厂中，出于多种原因（例如管理线的安全或重复使用）需要检测这些物体并将其与主要垃圾流分离。此外，我们提出了一个采用不同对象分割架构的管道，并评估我们数据集上的替代方案，对多模式和单模式替代方案进行了广泛的分析。我们的评估特别关注实时处理的效率和适用性，并展示了 HSI 如何在这些现实的工业环境中增强仅 RGB 的感知，而无需太多的计算开销。]]></description>
      <guid>https://arxiv.org/abs/2403.18033</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>考虑 Wasserstein 图匹配的半监督图像描述</title>
      <link>https://arxiv.org/abs/2403.17995</link>
      <description><![CDATA[arXiv:2403.17995v1 公告类型：新
摘要：图像字幕可以自动为给定图像生成字幕，其关键挑战是学习从视觉特征到自然语言特征的映射函数。现有的方法大多是有监督的方法，即每个图像在训练集中都有一个对应的句子。然而，考虑到描述图像总是需要大量的人力，在现实应用中我们通常拥有有限数量的描述图像（即图像文本对）和大量未描述的图像。因此，一个困境就是“半监督图像字幕”。为了解决这个问题，我们提出了一种考虑 Wasserstein 图匹配的半监督图像描述方法（SSIC-WGM），该方法采用原始图像输入来监督生成的句子。与传统的单模态半监督方法不同，半监督跨模态学习的难点在于构建异构模态之间的中间可比较信息。本文中，SSIC-WGM采用成功的场景图作为中间信息，并从两个方面约束生成的句子：1）模态间一致性。 SSIC-WGM 分别构建原始图像和生成句子的场景图，然后利用 wasserstein 距离更好地测量不同图的区域嵌入之间的相似性。 2）模式内一致性。 SSIC-WGM 对原始图像采用数据增强技术，然后约束增强图像和生成句子之间的一致性。因此，SSIC-WGM结合了跨模态伪监督和结构不变测量，以有效地使用未描述的图像，并学习更合理的映射函数。]]></description>
      <guid>https://arxiv.org/abs/2403.17995</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>ICCV 2023年第一届感知测试挑战赛点跟踪任务解决方案</title>
      <link>https://arxiv.org/abs/2403.17994</link>
      <description><![CDATA[arXiv:2403.17994v1 公告类型：新
摘要：本报告提出了一种跟踪任意点（TAP）任务的改进方法，该方法通过视频跟踪任何物理表面。一些现有的方法已经通过考虑时间关系来探索 TAP 以获得平滑的点运动轨迹，然而，它们仍然受到时间预测引起的累积误差的影响。为了解决这个问题，我们提出了一种简单而有效的方法，称为具有置信静态点的 TAP (TAPIR+)，其重点是纠正静态摄像机拍摄的视频中静态点的跟踪。需要澄清的是，我们的方法包含两个关键组成部分：（1）多粒度摄像机运动检测，它可以通过静态摄像机镜头识别视频序列。 (2) 基于 CMR 的点轨迹预测，采用一种运动对象分割方法将静态点与运动对象隔离。我们的方法在最终测试中排名第一，得分为 0.46。]]></description>
      <guid>https://arxiv.org/abs/2403.17994</guid>
      <pubDate>Thu, 28 Mar 2024 06:17:16 GMT</pubDate>
    </item>
    </channel>
</rss>