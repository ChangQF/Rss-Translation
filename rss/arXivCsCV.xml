<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 01 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>迈向热成像长期 SLAM</title>
      <link>https://arxiv.org/abs/2403.19885</link>
      <description><![CDATA[arXiv:2403.19885v1 公告类型：新
摘要：具有热图像的视觉 SLAM 以及其他低对比度视觉退化环境（例如水下或冰雪覆盖的区域）对于许多最先进的 (SOTA) 算法来说仍然是一个难题。除了对前端数据关联提出挑战之外，热图像还为长期重新定位和地图重用带来了额外的困难。热图像中物体的相对温度从白天到晚上发生巨大变化。通常用于 SLAM 中重新定位的特征描述符无法保持这些昼夜变化的一致性。我们表明，学习到的特征描述符可以在现有的基于词袋的定位方案中使用，以显着提高热图像中大时间间隙的位置识别。为了证明我们训练的词汇的有效性，我们开发了一个基线 SLAM 系统，将学习到的特征和匹配集成到经典 SLAM 算法中。我们的系统在具有挑战性的热图像上展示了良好的局部跟踪，并且重新定位克服了昼夜热外观的剧烈变化。我们的代码和数据集可以在这里找到：https://github.com/neufieldrobotics/IRSLAM_Baseline]]></description>
      <guid>https://arxiv.org/abs/2403.19885</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>PLoc：基于物理位置的自动驾驶数据集新评估标准</title>
      <link>https://arxiv.org/abs/2403.19893</link>
      <description><![CDATA[arXiv:2403.19893v1 公告类型：新
摘要：自动驾驶作为人工智能领域的一个关键研究领域，受到了广泛的关注。在自动驾驶场景中，物体不同的物理位置对应于不同的危险级别。然而，传统的自动驾驶物体检测评估标准往往忽视了物体物理位置这一关键方面，导致评估结果可能无法准确反映物体对自动驾驶车辆构成的真正威胁。为了提高自动驾驶的安全性，本文引入了一种基于物理位置信息的新型评估标准，称为PLoc。该标准超越了传统标准的局限性，承认自动驾驶场景中行人的物理位置可以提供有价值的安全相关信息。此外，本文提出了一个源自 ApolloScape 的新重新注释的数据集（ApolloScape-R）。 ApolloScape-R 涉及根据行人物理位置的重要性重新标记行人。该数据集用于评估各种目标检测模型在所提出的 PLoc 标准下的性能。实验结果表明，所有物体检测模型识别位于自动驾驶车辆行驶车道上的人的平均准确度低于识别人行道上的人的平均准确度。该数据集可在 https://github.com/lnyrlyed/ApolloScape-R.git 上公开获取]]></description>
      <guid>https://arxiv.org/abs/2403.19893</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>理清种族表型：细粒度控制与种族相关的面部表型特征</title>
      <link>https://arxiv.org/abs/2403.19897</link>
      <description><![CDATA[arXiv:2403.19897v1 公告类型：新
摘要：由于常见 2D 面部特征编码空间的高度复杂性和纠缠性，在 2D 面部图像上实现有效的细粒度外观变化，同时保留面部身份，是一项具有挑战性的任务。尽管存在这些挑战，这种通过解开的方式进行的细粒度控制是跨多个自动面部分析任务的数据驱动的种族偏见缓解策略的关键推动者，因为它允许分析、表征和综合人类面部多样性。在本文中，我们提出了一种新颖的 GAN 框架，可以对面部图像的个体种族相关表型属性进行细粒度控制。我们的框架将潜在（特征）空间分解为与种族相关的面部表型表征相对应的元素，从而分离表型方面（例如皮肤、头发颜色、鼻子、眼睛、嘴巴形状），而这些方面在现实中很难进行稳健注释。世界面部数据。同时，我们还引入了从 CelebA-HQ 中提取的高质量增强多样化 2D 人脸图像数据集，用于 GAN 训练。与之前的工作不同，我们的框架仅依赖于 2D 图像和相关参数来实现对种族相关表型属性的最先进的个体控制，并具有改进的照片真实感输出。]]></description>
      <guid>https://arxiv.org/abs/2403.19897</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>用于自动驾驶问答的多框架、轻量级和高效的视觉语言模型</title>
      <link>https://arxiv.org/abs/2403.19838</link>
      <description><![CDATA[arXiv:2403.19838v1 公告类型：新
摘要：视觉语言模型（VLM）和多模态语言模型（MMLM）在自动驾驶研究中已变得很重要，因为这些模型可以使用交通场景图像为端到端自动驾驶安全任务提供可解释的文本推理和响应和其他数据模式。然而，目前这些系统的方法使用昂贵的大语言模型（LLM）主干和图像编码器，使得此类系统不适合存在严格内存限制且需要快速推理时间的实时自动驾驶系统。为了解决之前的这些问题，我们开发了 EM-VLM4AD，这是一种高效、轻量级、多帧视觉语言模型，可为自动驾驶执行视觉问答。与以前的方法相比，EM-VLM4AD 所需的内存和浮点运算至少减少了 10 倍，同时与 DriveLM 数据集上的现有基准相比，还获得了更高的 BLEU-4、METEOR、CIDEr 和 ROGUE 分数。 EM-VLM4AD还表现出从与提示相关的交通视图中提取相关信息的能力，并可以回答各种自动驾驶子任务的问题。我们在 https://github.com/akshaygopalkr/EM-VLM4AD 发布了用于训练和评估模型的代码。]]></description>
      <guid>https://arxiv.org/abs/2403.19838</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>合成图像对迁移学习有用吗？对数据生成、数量和利用的调查</title>
      <link>https://arxiv.org/abs/2403.19866</link>
      <description><![CDATA[arXiv:2403.19866v1 公告类型：新
摘要：合成图像数据生成代表了训练深度学习模型的一种有前途的途径，特别是在迁移学习领域，由于隐私和知识产权的考虑，获取特定领域内的真实图像可能非常昂贵。这项工作深入研究了从文本到图像生成模型派生的合成图像的生成和利用，以促进迁移学习范例。尽管生成的图像具有高视觉保真度，但我们观察到，由于合成图像和真实图像之间固有的分布差距，将它们简单地合并到现有的真实图像数据集中并不能始终如一地增强模型性能。为了解决这个问题，我们引入了一种称为桥接传输的新颖的两阶段框架，该框架最初使用合成图像来微调预训练模型以提高其可传输性，然后使用真实数据进行快速适应。除此之外，我们提出了数据集风格反转策略，以改善合成图像和真实图像之间的风格对齐。我们提出的方法在 10 个不同的数据集和 5 个不同的模型中进行了评估，展示了一致的改进，分类任务的准确率提高了 30%。有趣的是，我们注意到增强功能尚未饱和，这表明随着合成数据量的扩大，好处可能会进一步增加。]]></description>
      <guid>https://arxiv.org/abs/2403.19866</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>使用神经场进行高效 3D 实例映射和定位</title>
      <link>https://arxiv.org/abs/2403.19797</link>
      <description><![CDATA[arXiv:2403.19797v1 公告类型：新
摘要：我们解决了从一系列 RGB 图像中学习用于 3D 实例分割的隐式场景表示的问题。为此，我们引入了 3DIML，这是一种新颖的框架，可以有效地学习可以从新颖的视点渲染的标签字段，以生成视图一致的实例分割掩模。 3DIML 显着改进了现有基于隐式场景表示的方法的训练和推理运行时间。与以自我监督方式优化神经场的现有技术不同，现有技术需要复杂的训练过程和损失函数设计，3DIML 利用两阶段过程。第一阶段 InstanceMap 将前端实例分割模型生成的图像序列的 2D 分割掩模作为输入，并将图像上的相应掩模与 3D 标签相关联。这些几乎视图一致的伪标签掩码随后在第二阶段 InstanceLift 中使用，以监督神经标签字段的训练，该字段对 InstanceMap 遗漏的区域进行插值并解决歧义。此外，我们还引入了 InstanceLoc，它可以通过融合两者的输出，在给定训练有素的标签字段和现成的图像分割模型的情况下实现实例掩模的近实时本地化。我们在 Replica 和 ScanNet 数据集的序列上评估 3DIML，并在图像序列的温和假设下证明 3DIML 的有效性。与质量相当的现有隐式场景表示方法相比，我们实现了 14-24 倍的加速，展示了其促进更快、更有效的 3D 场景理解的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.19797</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>X-MIC：用于自我中心动作泛化的跨模式实例调节</title>
      <link>https://arxiv.org/abs/2403.19811</link>
      <description><![CDATA[arXiv:2403.19811v1 公告类型：新
摘要：最近，由于视觉语言模型（VLM）在零样本识别方面的成功，人们对将视觉语言模型（VLM）应用于图像和第三人称视频分类越来越感兴趣。然而，这些模型对以自我为中心的视频的适应在很大程度上尚未被探索。为了解决这一差距，我们提出了一个简单而有效的跨模式适应框架，我们称之为 X-MIC。使用视频适配器，我们的管道学习直接在共享嵌入空间中将冻结文本嵌入与每个以自我为中心的视频对齐。我们新颖的适配器架构通过解开可学习的时间模型和冻结的视觉编码器，保留并提高了预训练 VLM 的泛化能力。这可以增强文本嵌入与每个以自我为中心的视频的对齐，从而显着提高跨数据集泛化能力。我们在 Epic-Kitchens、Ego4D 和 EGTEA 数据集上评估了我们的方法，以实现细粒度的跨数据集动作泛化，证明了我们方法的有效性。代码可在 https://github.com/annusha/xmic 获取]]></description>
      <guid>https://arxiv.org/abs/2403.19811</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>ENet-21：用于车道检测的优化光 CNN 结构</title>
      <link>https://arxiv.org/abs/2403.19782</link>
      <description><![CDATA[arXiv:2403.19782v1 公告类型：新
摘要：自动驾驶车辆的车道检测是一个重要的概念，但它是现代车辆驾驶辅助系统中的一个具有挑战性的问题。深度学习的出现导致自动驾驶汽车取得重大进展。传统的基于深度学习的方法将车道检测问题作为二元分割任务处理，并确定像素是否属于一条线。这些方法依赖于车道数量固定的假设，但这并不总是有效。本研究旨在开发车道检测问题的最佳结构，通过利用由二进制分割和亲和力场组成的机器学习方法，可以管理不同数量的车道和变道场景，为现代车辆的驾驶员辅助功能提供有前景的解决方案。在该方法中，选择卷积神经网络（CNN）作为特征提取器，并通过语义分割和亲和力场输出的聚类获得最终输出。我们的方法使用比 exi 更简单的 CNN 架构]]></description>
      <guid>https://arxiv.org/abs/2403.19782</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>用于手术手势识别的基于零样本提示的视频编码器</title>
      <link>https://arxiv.org/abs/2403.19786</link>
      <description><![CDATA[arXiv:2403.19786v1 公告类型：新
摘要：目的：手术视频是手势识别的重要数据流。因此，针对这些数据流的鲁棒视觉编码器同样重要。方法：利用 Bridge-Prompt 框架，我们微调预训练的视觉文本模型 (CLIP)，用于手术视频中的手势识别。这可以利用大量的外部视频数据（例如文本），还可以利用标签元数据和弱监督对比损失。结果：我们的实验表明，基于提示的视频编码器在手术手势识别任务中优于标准编码器。值得注意的是，它在零样本场景中表现出强大的性能，其中编码器训练阶段未提供的手势/任务包含在预测阶段。此外，我们还衡量了在特征提取器训练模式中包含文本描述的好处。结论：Bridge-Prompt 和类似的预训练+微调视频编码器模型为手术机器人提供了重要的视觉表示，特别是在手势识别任务中。考虑到手术任务（手势）的多样性，这些模型无需进行任何任务（手势）特定的再训练即可进行零镜头转移的能力使其变得非常宝贵。]]></description>
      <guid>https://arxiv.org/abs/2403.19786</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>JIST：用于顺序视觉位置识别的联合图像和序列训练</title>
      <link>https://arxiv.org/abs/2403.19787</link>
      <description><![CDATA[arXiv:2403.19787v1 公告类型：新
摘要：视觉地点识别旨在依靠视觉线索识别之前访问过的地点，并用于机器人应用中的 SLAM 和定位。由于移动机器人通常可以访问连续的帧流，因此该任务自然会被视为序列到序列的定位问题。然而，获取标记数据序列比收集孤立的图像要昂贵得多，这可以在几乎没有监督的情况下以自动化方式完成。为了缓解这个问题，我们提出了一种新颖的联合图像和序列训练协议（JIST），该协议通过多任务学习框架利用大量未经整理的图像集。通过 JIST，我们还引入了 SeqGeM，这是一个聚合层，它重新审视流行的 GeM 池化，从一系列单帧嵌入中生成单个稳健且紧凑的嵌入。我们表明，我们的模型能够超越以前的最先进技术，同时速度更快，使用小 8 倍的描述符，具有更轻的架构并允许处理各种长度的序列。代码可在 https://github.com/ga1i13o/JIST 获取]]></description>
      <guid>https://arxiv.org/abs/2403.19787</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>CLoRA：构建多个 LoRA 模型的对比方法</title>
      <link>https://arxiv.org/abs/2403.19776</link>
      <description><![CDATA[arXiv:2403.19776v1 公告类型：新
摘要：低秩适应（LoRA）已成为图像生成领域中一种强大且流行的技术，它提供了一种高效的方法来适应和完善针对特定任务的预训练深度学习模型，而无需进行全面的再训练。通过采用预先训练的 LoRA 模型（例如代表特定猫和特定狗的模型），目标是生成忠实体现 LoRA 定义的这两种动物的图像。然而，无缝混合多个概念 LoRA 以在一张图像中捕获各种概念的任务被证明是一项重大挑战。常见的方法常常存在不足，主要是因为不同 LoRA 模型中的注意力机制重叠，导致一个概念可能被完全忽略（例如，忽略狗）或概念被错误组合（例如，生成两只猫的图像）的情况一只猫和一只狗）。为了克服这些问题，CLoRA 通过更新多个 LoRA 模型的注意力图并利用它们创建促进潜在表示融合的语义掩码来解决这些问题。我们的方法能够创建真正反映每个 LoRA 特征的合成图像，成功融合多个概念或风格。我们的定性和定量综合评估表明，我们的方法优于现有方法，标志着 LoRA 图像生成领域的重大进步。此外，我们还分享源代码、基准数据集和经过训练的 LoRA 模型，以促进对该主题的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2403.19776</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>通过事件和帧减轻神经辐射场中的运动模糊</title>
      <link>https://arxiv.org/abs/2403.19780</link>
      <description><![CDATA[arXiv:2403.19780v1 公告类型：新
摘要：神经辐射场（NeRF）在新颖的视图合成中显示出巨大的潜力。然而，当用于训练的数据受到运动模糊的影响时，它们很难渲染清晰的图像。另一方面，事件相机在动态场景中表现出色，因为它们以微秒分辨率测量亮度变化，因此仅受模糊影响很小。最近的方法试图通过融合帧和事件来增强相机运动下的 NeRF 重建。然而，他们在恢复准确的颜色内容或将 NeRF 限制为一组预定义的相机姿势方面面临挑战，从而在具有挑战性的条件下损害重建质量。本文提出了一种利用基于模型和基于学习的模块来解决这些问题的新颖方案。我们明确地对模糊形成过程进行建模，利用事件二重积分作为附加的基于模型的先验。此外，我们使用端到端可学习响应函数对事件像素响应进行建模，使我们的方法能够适应真实事件相机传感器中的非理想情况。我们在合成数据和真实数据上表明，所提出的方法比仅使用帧的现有去模糊 NeRF 以及结合帧和事件的去模糊 NeRF 分别优于 +6.13dB 和 +2.48dB。]]></description>
      <guid>https://arxiv.org/abs/2403.19780</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>MIST：通过文本到图像扩散模型中的解开交叉注意力编辑来减轻交叉偏差</title>
      <link>https://arxiv.org/abs/2403.19738</link>
      <description><![CDATA[arXiv:2403.19738v1 公告类型：新
摘要：基于扩散的文本到图像模型因其能够从文本描述生成详细且真实的图像而迅速流行。然而，这些模型通常反映了训练数据中存在的偏差，尤其是对边缘群体的影响。虽然之前消除语言模型偏见的努力主要集中在解决特定偏见，例如种族或性别偏见，但解决交叉偏见的努力却很有限。交叉偏见是指个人在多种社会身份的交叉点所经历的独特形式的偏见。解决交叉偏见至关重要，因为它放大了基于种族、性别和其他身份的歧视的负面影响。在本文中，我们介绍了一种通过以解开的方式修改交叉注意力图来解决基于扩散的文本到图像模型中的交叉偏差的方法。我们的方法利用预先训练的稳定扩散模型，无需额外的参考图像集，并保留未改变概念的原始质量。综合实验表明，我们的方法在减轻各种属性的单一偏差和交叉偏差方面优于现有方法。我们提供各种属性的源代码和去偏模型，以鼓励生成模型的公平性并支持进一步的研究。]]></description>
      <guid>https://arxiv.org/abs/2403.19738</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习提高虚拟现实中眼动追踪的稳健性、准确性和精度</title>
      <link>https://arxiv.org/abs/2403.19768</link>
      <description><![CDATA[arXiv:2403.19768v1 公告类型：新
摘要：用于从基于移动和视频的眼动仪估计注视方向的算法通常涉及跟踪在眼睛摄像机图像中移动的眼睛特征，其方式与移动注视方向协变，例如眼睛的中心或边界。学生。由于部分遮挡和环境反射，使用传统计算机视觉技术跟踪这些特征可能很困难。尽管最近使用机器学习 (ML) 进行瞳孔跟踪的努力在使用分割性能的标准衡量标准进行评估时表现出了优异的结果，但人们对这些网络如何影响最终注视估计的质量知之甚少。当使用基于特征或基于模型的方法生成后续注视估计时，这项工作对几种基于机器学习的当代眼睛特征跟踪方法的影响进行了客观评估。指标包括注视估计的准确性和精确度，以及退出率。]]></description>
      <guid>https://arxiv.org/abs/2403.19768</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>ShapeFusion：用于局部形状编辑的 3D 扩散模型</title>
      <link>https://arxiv.org/abs/2403.19773</link>
      <description><![CDATA[arXiv:2403.19773v1 公告类型：新
摘要：在 3D 计算机视觉领域，参数化模型已成为创建逼真且富有表现力的 3D 化身的突破性方法。传统上，它们依赖于主成分分析 (PCA)，因为它能够将数据分解到最大限度地捕获形状变化的正交空间。然而，由于正交性约束和 PCA 分解的全局性质，这些模型难以执行 3D 形状的局部和解缠结编辑，这严重影响了它们在需要精细控制的应用（例如面部雕刻）中的使用。在本文中，我们利用扩散模型来实现对 3D 网格的多样化且完全本地化的编辑，同时完全保留未编辑的区域。我们提出了一种有效的扩散掩蔽训练策略，通过设计，该策略有助于对任何形状区域进行局部操作，而不受限于预定义区域或预定义控制顶点的稀疏集。按照我们的框架，用户可以显式地设置他们选择的操作区域，并将任意一组顶点定义为编辑 3D 网格的句柄。与当前最先进的技术相比，我们的方法比依赖潜在代码状态的方法具有更多可解释的形状操作、更大的本地化和生成多样性，同时比基于优化的方法提供更快的推理。项目页面：https://rolpotamias.github.io/Shapefusion/]]></description>
      <guid>https://arxiv.org/abs/2403.19773</guid>
      <pubDate>Mon, 01 Apr 2024 06:17:47 GMT</pubDate>
    </item>
    </channel>
</rss>