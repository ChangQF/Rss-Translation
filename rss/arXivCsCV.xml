<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 07 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于接触变形预测的物理编码图神经网络</title>
      <link>https://arxiv.org/abs/2402.03466</link>
      <description><![CDATA[在机器人技术中，了解触觉交互过程中的物体变形至关重要。对变形的精确理解可以提升机器人模拟水平，并对不同行业产生广泛影响。我们引入了一种使用物理编码图神经网络（GNN）进行此类预测的方法。与机器人抓取和操作场景类似，我们专注于对外力作用下刚性网格与可变形网格接触之间的动力学进行建模。我们的方法表示图结构中的软体和刚体，其中节点保存网格的物理状态。我们还结合了交叉注意机制来捕获对象之间的相互作用。通过共同学习几何和物理，我们的模型重建了一致且详细的变形。我们公开了我们的代码和数据集，以推进机器人模拟和抓取的研究。]]></description>
      <guid>https://arxiv.org/abs/2402.03466</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:39 GMT</pubDate>
    </item>
    <item>
      <title>用于服装和背景更换的修复管道</title>
      <link>https://arxiv.org/abs/2402.03501</link>
      <description><![CDATA[近年来，生成人工智能（GenAI）的突破性进展引发了变革性的范式转变，对各个领域产生了重大影响。在这项工作中，我们专门探索了一种集成方法，利用 GenAI 和计算机视觉中的先进技术，强调图像处理。该方法通过几个阶段展开，包括深度估计、基于深度信息创建修复掩模、利用稳定扩散结合潜在一致性模型（LCM）生成和替换背景，以及随后的衣服替换和美学应用通过修复管道进行更改。本研究中进行的实验强调了该方法的有效性，强调了其产生视觉上吸引人的内容的潜力。这些先进技术的融合使用户可以输入个人照片，并根据特定提示操​​纵他们修改服装和背景，而无需手动输入修复蒙版，有效地将主题置于创意想象的广阔景观中。]]></description>
      <guid>https://arxiv.org/abs/2402.03501</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:39 GMT</pubDate>
    </item>
    <item>
      <title>使用 CNN-LSTM-MLP 混合融合模型的基于计算机视觉的跟踪检测方法</title>
      <link>https://arxiv.org/abs/2402.03417</link>
      <description><![CDATA[犯罪和可疑活动检测已成为近年来的热门研究课题。计算机视觉技术的快速发展对解决这一问题产生了至关重要的影响。然而，尽管现代技术不断发展，物理跟踪检测仍然是一个较少探索的领域。如今，公共场所的跟踪行为已成为常见现象，其中女性受到的影响最大。跟踪是一种明显的行为，通常发生在任何犯罪活动开始之前，因为跟踪者在实施任何犯罪活动（例如袭击、绑架、强奸等）之前就开始跟踪、徘徊并盯着受害者。因此，检测跟踪行为已成为必要，因为所有这些犯罪活动都可以通过跟踪检测来制止。在这项研究中，我们提出了一种新颖的基于深度学习的混合融合模型，用于从具有最少帧数的单个视频中检测潜在的跟踪者。我们从视频帧中提取多个相关特征，例如面部标志、头部姿势估计和相对距离，作为数值。该数据被输入多层感知器 (MLP)，以执行跟踪和非跟踪场景之间的分类任务。同时，视频帧被输入卷积和 LSTM 模型的组合中以提取时空特征。我们融合这些数值和时空特征来构建分类器来检测跟踪事件。此外，我们引入了一个由从各种故事片和电视剧中收集的跟踪和非跟踪视频组成的数据集，该数据集也用于训练模型。实验结果显示了我们提出的跟踪者检测系统的效率和活力，达到了 89.58% 的测试精度，与最先进的方法相比有了显着的提高。]]></description>
      <guid>https://arxiv.org/abs/2402.03417</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:38 GMT</pubDate>
    </item>
    <item>
      <title>通过基于图像的渲染进行去噪扩散</title>
      <link>https://arxiv.org/abs/2402.03445</link>
      <description><![CDATA[生成 3D 场景是一个具有挑战性的开放问题，需要合成在 3D 空间中完全一致的合理内容。虽然最近的方法（例如神经辐射场）在视图合成和 3D 重建方面表现出色，但由于缺乏生成能力，它们无法在未观察到的区域合成合理的细节。相反，现有的生成方法通常无法在野外重建详细的大规模场景，因为它们使用容量有限的 3D 场景表示，需要对齐相机姿势，或依赖额外的正则化器。在这项工作中，我们引入了第一个能够快速、详细地重建和生成真实世界 3D 场景的扩散模型。为了实现这一目标，我们做出了三项贡献。首先，我们引入了一种新的神经场景表示，即 IB 平面，它可以高效、准确地表示大型 3D 场景，根据需要动态分配更多容量以捕获每个图像中可见的细节。其次，我们提出了一个去噪扩散框架来学习这种新颖的 3D 场景表示的先验，仅使用 2D 图像，而不需要任何额外的监督信号，例如掩模或深度。这支持统一架构中的 3D 重建和生成。第三，我们开发了一种原则性方法，通过删除一些图像的表示，在将基于图像的渲染与扩散模型集成时避免琐碎的 3D 解决方案。我们在几个具有挑战性的真实图像和合成图像数据集上评估了模型，并在生成、新视图合成和 3D 重建方面展示了卓越的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.03445</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:38 GMT</pubDate>
    </item>
    <item>
      <title>自监督对比学习的约束多视图表示</title>
      <link>https://arxiv.org/abs/2402.03456</link>
      <description><![CDATA[表示学习构成了当代深度学习范式的关键基石，为阐明潜在空间中的独特特征和解释深度模型提供了渠道。然而，医学图像分割中解剖模式固有的复杂性和病变分布的随机性对表征的解开和显着特征的理解提出了重大挑战。以互信息最大化为指导的方法，特别是在对比学习的框架内，在解耦紧密交织的表示方面表现出了显着的成功和优越性。然而，对比学习的有效性很大程度上取决于正负样本对的质量，即多视图之间未经选择的平均互信息会阻碍学习策略，因此视图的选择至关重要。在这项工作中，我们引入了一种基于表示距离的互信息（MI）最大化的新方法，用于测量不同视图的重要性，旨在进行更有效的对比学习和表示解开。此外，我们引入了用于表示选择的 MI 重新排序策略，有利于连续 MI 估计和表示显着性距离测量。具体来说，我们利用从频域提取的多视图表示，根据不同频率的互信息重新评估它们的重要性，从而促进多方面的对比学习方法来增强语义理解。五个指标下的统计结果表明，我们提出的框架熟练地限制了 MI 最大化驱动的表示选择并引导多视图对比学习过程。]]></description>
      <guid>https://arxiv.org/abs/2402.03456</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:38 GMT</pubDate>
    </item>
    <item>
      <title>尊重模型：共享比分解的细粒度和鲁棒解释</title>
      <link>https://arxiv.org/abs/2402.03348</link>
      <description><![CDATA[现有解释方法在真实阐明底层模型决策过程方面的真实性受到质疑。现有方法偏离了忠实地表示模型，因此容易受到对抗性攻击。为了解决这个问题，我们提出了一种新的可解释人工智能（XAI）方法，称为 SRD（共享比率分解），它真实地反映了模型的推理过程，从而显着增强了我们解释的稳健性。与传统强调神经元级别不同，我们采用向量视角来考虑滤波器之间复杂的非线性相互作用。我们还引入了一个有趣的观察，称为仅激活模式预测（APOP），让我们强调不活动神经元的重要性，并重新定义封装所有相关信息（包括活动和不活动神经元）的相关性。我们的方法 SRD 允许对逐点特征向量 (PFV) 进行递归分解，从而在任何层提供高分辨率的有效感受野 (ERF)。]]></description>
      <guid>https://arxiv.org/abs/2402.03348</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:37 GMT</pubDate>
    </item>
    <item>
      <title>使用迁移学习预测神经胶质瘤的存活率和分级</title>
      <link>https://arxiv.org/abs/2402.03384</link>
      <description><![CDATA[胶质母细胞瘤是一种高度恶性的脑肿瘤，如果不治疗，预期寿命仅为3至6个月。准确检测和预测其生存和等级至关重要。这项研究介绍了一种使用迁移学习技术的新方法。通过详尽的优化来测试各种预训练网络，包括 EfficientNet、ResNet、VGG16 和 Inception，以确定最合适的架构。应用迁移学习在胶质母细胞瘤图像数据集上对这些模型进行微调，旨在实现两个目标：生存期和肿瘤分级预测。实验结果显示生存期预测准确率为 65%，将患者分为短、中、长生存类别。此外，肿瘤分级的预测准确率达到97%，准确区分低级别胶质瘤（LGG）和高级别胶质瘤（HGG）。该方法的成功归功于迁移学习的有效性，超越了当前最先进的方法。总之，这项研究提出了一种预测胶质母细胞瘤生存和分级的有前景的方法。迁移学习展示了其增强预测模型的潜力，特别是在大型数据集有限的场景中。这些发现有望改善胶质母细胞瘤患者的诊断和治疗方法。]]></description>
      <guid>https://arxiv.org/abs/2402.03384</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:37 GMT</pubDate>
    </item>
    <item>
      <title>大规模生成式人工智能模型缺乏视觉数字感</title>
      <link>https://arxiv.org/abs/2402.03328</link>
      <description><![CDATA[人类即使不需要数数，也可以轻松判断视觉场景中物体的数量，并且这种技能已在各种动物物种和语言发展和正规教育之前的婴儿中得到记录。对于较小的集合，数值判断是没有错误的，而对于较大的集合，响应变得近似，并且可变性与目标数量成比例地增加。尽管物体特征（例如颜色或形状）存在差异，但各种物品都观察到这种反应模式，这表明我们的视觉数字感依赖于数字的抽象表示。在这里，我们研究了基于大规模 Transformer 架构的生成人工智能 (AI) 模型是否可以可靠地命名简单视觉刺激中的对象数量，或生成包含 1-10 范围内的目标项目数量的图像。令人惊讶的是，没有一个基础模型被认为是以类似人类的方式执行的：即使数量很少，它们也会犯下惊人的错误，响应变异性通常不会以系统的方式增加，并且错误的模式随对象类别的不同而变化。我们的研究结果表明，先进的人工智能系统仍然缺乏支持对数字的直观理解的基本能力，而这对人类来说是计算和数学发展的基础。]]></description>
      <guid>https://arxiv.org/abs/2402.03328</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:36 GMT</pubDate>
    </item>
    <item>
      <title>用于数据高效强化学习的无监督显着补丁选择</title>
      <link>https://arxiv.org/abs/2402.03329</link>
      <description><![CDATA[为了提高基于视觉的深度强化学习（RL）的样本效率，我们提出了一种称为 SPIRL 的新方法，可以自动从输入图像中提取重要的补丁。继 Masked Auto-Encoders 之后，SPIRL 基于以自监督方式预训练的 Vision Transformer 模型，以从随机采样的补丁中重建图像。然后可以利用这些预先训练的模型来检测和选择显着的补丁，这些补丁被定义为难以从相邻补丁重建。在 RL 中，SPIRL 代理通过注意力模块处理选定的显着补丁。我们在 Atari 游戏上实证验证 SPIRL，以根据相关最先进的方法（包括一些传统的基于模型的方法和基于关键点的模型）测试其数据效率。此外，我们还分析了模型的可解释能力。]]></description>
      <guid>https://arxiv.org/abs/2402.03329</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 Densenet201 架构模型进行马铃薯叶病分类的迁移学习</title>
      <link>https://arxiv.org/abs/2402.03347</link>
      <description><![CDATA[马铃薯植物是对人类有益的植物。与其他植物一样，马铃薯也有病害。如果不立即治疗这种疾病，粮食产量将会大幅下降。因此，必须快速、准确地检测疾病，以便有效、高效地进行疾病控制。可直接对马铃薯叶部病害进行分类。尽管如此，这些症状并不能总能解释侵袭马铃薯叶子的疾病类型，因为有许多类型的疾病具有看起来相同的症状。人类对马铃薯叶病鉴定结果的判断也存在缺陷，因此有时个体之间的鉴定结果可能会有所不同。因此，利用深度学习进行马铃薯叶部病害的分类过程有望缩短时间并具有较高的分类精度。本研究使用具有 DenseNet201 架构的深度学习方法。本研究选择使用DenseNet201算法是因为该模型可以识别马铃薯叶片的重要特征并识别新出现疾病的早期迹象。本研究旨在评估与传统分类方法相比，采用 DenseNet201 架构的迁移学习方法在提高马铃薯叶病分类准确性方面的有效性。本研究使用两种场景，即比较 dropout 数量和比较三种优化器。该测试使用 dropout 0.1 和 Adam 优化器生成最佳模型，训练准确率为 99.5%，验证准确率为 95.2%，混淆矩阵准确率为 96%。在这项研究中，使用数据测试，将多达 40 张图像测试到已构建的模型中。该模型的测试结果得出了马铃薯叶病分类的新准确率，即 92.5%。]]></description>
      <guid>https://arxiv.org/abs/2402.03347</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:36 GMT</pubDate>
    </item>
    <item>
      <title>槽结构世界模型</title>
      <link>https://arxiv.org/abs/2402.03326</link>
      <description><![CDATA[感知和推理单个对象及其交互的能力是构建智能人工系统要实现的目标。最先进的方法使用前馈编码器来提取对象嵌入，并使用潜在图神经网络来对这些对象嵌入之间的交互进行建模。然而，前馈编码器无法提取{\以对象为中心}的表示，也无法解开具有相似外观的多个对象。为了解决这些问题，我们引入了{\it Slot Structured World Models}（SSWM），一类世界模型，它将{\it以对象为中心的}编码器（基于Slot Attention）与基于潜在图的动态模型相结合。我们使用简单的物理交互规则在 Spriteworld 基准测试中评估我们的方法，其中槽结构世界模型在一系列具有动作条件对象交互的（多步骤）预测任务上始终优于基线。重现纸质实验的所有代码均可从 \url{https://github.com/JonathanCollu/Slot-Structured-World-Models} 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.03326</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:35 GMT</pubDate>
    </item>
    <item>
      <title>Uni3D-LLM：使用大型语言模型统一点云感知、生成和编辑</title>
      <link>https://arxiv.org/abs/2402.03327</link>
      <description><![CDATA[在本文中，我们介绍了 Uni3D-LLM，这是一个利用大型语言模型 (LLM) 将 3D 感知、生成和编辑任务集成到点云场景中的统一框架。该框架使用户能够在自然语言描述的多功能性的指导下，轻松地生成和修改场景中指定位置的对象。 Uni3D-LLM利用自然语言的表达能力，可以精确指挥3D对象的生成和编辑，从而显着增强操作的灵活性和可控性。通过将点云映射到统一的表示空间，Uni3D-LLM 实现了跨应用程序功能，从而能够无缝执行各种任务，从 3D 对象的精确实例化到交互式设计的多样化要求。通过一整套严​​格的实验，Uni3D-LLM在点云理解、生成和编辑方面的功效得到了验证。此外，我们还评估了集成点云感知模块对生成和编辑过程的影响，证实了我们的方法在实际应用中的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.03327</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:35 GMT</pubDate>
    </item>
    <item>
      <title>SpecFormer：通过最大奇异值惩罚保护 Vision Transformer 的鲁棒性</title>
      <link>https://arxiv.org/abs/2402.03317</link>
      <description><![CDATA[视觉变压器 (ViT) 因其卓越的性能而成为各种计算机视觉任务的首选。然而，它们的广泛采用引起了人们对面临恶意攻击时的安全性的担忧。现有方法大多依赖训练过程中的经验调整，缺乏明确的理论基础。在本研究中，我们通过引入 SpecFormer 来解决这一差距，该 SpecFormer 专门设计用于增强 ViT 抵御对抗性攻击的能力，并得到精心推导的理论保证的支持。我们为自注意力层建立了局部 Lipschitz 边界，并引入了一种新颖的方法，即最大奇异值惩罚（MSVP），以实现对这些边界的精确控制。我们将 MSVP 无缝集成到 ViT 的注意力层中，使用幂迭代方法来提高计算效率。修改后的模型 SpecFormer 有效降低了注意力权重矩阵的谱范数，从而增强了网络局部 Lipschitzness。这反过来又提高了训练效率和稳健性。对 CIFAR 和 ImageNet 数据集的大量实验证实了 SpecFormer 在防御对抗性攻击方面的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.03317</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:34 GMT</pubDate>
    </item>
    <item>
      <title>稍后连接：通过有针对性的增强来提高稳健性的微调</title>
      <link>https://arxiv.org/abs/2402.03325</link>
      <description><![CDATA[在标记源域（例如，来自野生动物相机陷阱的标记图像）上训练的模型在部署到分布外（OOD）目标域（例如，来自新相机陷阱位置的图像）时通常泛化性很差。在可获得未标记目标数据的领域适应设置中，自监督预训练（例如，屏蔽自动编码或对比学习）是缓解这种性能下降的有前途的方法。当使用通用数据增强（例如，屏蔽或裁剪）连接源域和目标域（它们在输入空间中可能相距很远）时，预训练可以改善 OOD 错误。在本文中，我们在现实世界的任务中表明，与在标记源数据上从头开始训练相比，预训练后的标准微调并不能始终如一地改善 OOD 错误。为了更好地利用分布变化的预训练，我们建议稍后连接：在使用通用增强进行预训练后，使用根据分布变化的知识设计的有针对性的增强进行微调。预训练学习源域和目标域内的良好表示，而有针对性的增强在微调过程中更好地连接域。 Connect Later 通过对 4 个真实世界数据集进行有针对性的增强，改进了标准微调和监督学习的平均 OOD 误差：Connect Later 在天文时间序列分类 (AstroClassification) 、野生动物物种方面实现了最先进的 2.5%与 ResNet-50 的识别 (iWildCam-WILDS) 提高 0.9%，与 DenseNet121 的肿瘤识别 (Camelyon17-WILDS) 提高 1.1%；以及在天文时间序列红移预测 (Redshifts) 的新数据集上实现 0.03 RMSE（相对 11%）的最佳性能。代码和数据集可在 https://github.com/helenqu/connect-later 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.03325</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:34 GMT</pubDate>
    </item>
    <item>
      <title>RTHDet：旋转工作台区域和图像中的头部检测</title>
      <link>https://arxiv.org/abs/2402.03315</link>
      <description><![CDATA[传统模型专注于水平表格检测，但在旋转环境中表现不佳，限制了表格识别的进展。本文介绍了一项新任务：检测表格区域并定位旋转场景中的头尾部分。我们提出了相应的数据集、评估指标和方法。我们的新颖方法“自适应有界旋转”解决了检测旋转表及其头尾部分时的数据集稀缺问题。我们基于“ICDAR2019MTD”制作了包含表头和表尾语义信息的数据集“TRR360D”。新指标“R360 AP”可测量检测旋转区域和定位头尾部分的精度。我们的基准，即高速且准确的“RTMDet-S”，是经过广泛审查和测试后选择的。我们引入了“RTHDet”，通过“r360”旋转矩形角度表示和“角度损失”分支增强基线，从而改进头尾定位。通过应用迁移学习和自适应边界旋转增强，RTHDet 的 AP50 (T&lt;90) 与基线相比从 23.7% 提高到 88.7%。这证明了 RTHDet 在检测旋转台区域和准确定位头部和尾部部分方面的有效性。RTHDet 已集成到广泛使用的开源 MMRotate 工具包中：https://github.com/open-mmlab/mmrotate/tree/dev-1 .x/项目/RR360。]]></description>
      <guid>https://arxiv.org/abs/2402.03315</guid>
      <pubDate>Wed, 07 Feb 2024 09:13:33 GMT</pubDate>
    </item>
    </channel>
</rss>