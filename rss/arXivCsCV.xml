<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 19 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于全幻灯片图像分类多实例学习的紧凑且去偏的负实例嵌入</title>
      <link>https://arxiv.org/abs/2402.10595</link>
      <description><![CDATA[arXiv:2402.10595v1 公告类型：新
摘要：全幻灯片图像 (WSI) 分类是一项具有挑战性的任务，因为 1) WSI 的补丁缺乏注释，2) WSI 具有不必要的可变性，例如染色协议。最近，多实例学习 (MIL) 取得了重大进展，允许基于幻灯片级别而不是补丁级别注释进行分类。然而，现有的 MIL 方法忽略了正常幻灯片的所有补丁都是正常的。使用这个免费注释，我们引入了半监督信号来消除载玻片间变异性的偏差并捕获正常斑块内变异的常见因素。由于我们的方法与 MIL 算法正交，因此我们在最近提出的 MIL 算法的基础上评估我们的方法，并将其性能与其他半监督方法进行比较。我们在包括 Camelyon-16 和 TCGA 肺癌在内的两个公共 WSI 数据集上评估我们的方法，并证明我们的方法显着提高了现有 MIL 算法的预测性能，并且优于其他半监督算法。我们在 https://github.com/AITRICS/pathology_mil 发布了我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2402.10595</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>PEGASUS：具有可组合属性的个性化生成 3D 化身</title>
      <link>https://arxiv.org/abs/2402.10636</link>
      <description><![CDATA[arXiv:2402.10636v1 公告类型：新
摘要：我们提出了 PEGASUS，一种从单眼视频源构建个性化生成 3D 面部头像的方法。作为一个组合生成模型，我们的模型可以通过分离控制来选择性地改变目标个体的面部属性（例如头发或鼻子），同时保留身份。我们提出了实现这一目标的两种关键方法。首先，我们提出了一种通过构建具有不同面部属性的目标身份的合成视频集合来构建特定于人的生成 3D 头像的方法，其中视频是通过借用其他单眼视频中不同个体的部分来合成的。通过多次实验，我们通过生成高度真实的看不见的属性来证明我们的方法的卓越性能。随后，我们引入了一种零样本方法，通过利用先前构建的个性化生成模型来更有效地实现相同的生成建模。]]></description>
      <guid>https://arxiv.org/abs/2402.10636</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>共同使用左右脑：迈向视觉和语言规划</title>
      <link>https://arxiv.org/abs/2402.10534</link>
      <description><![CDATA[arXiv:2402.10534v1 公告类型：新
摘要：大型语言模型（LLM）和大型多模态模型（LMM）在各种任务上表现出了卓越的决策屏蔽能力。但他们本质上是在语言空间内进行规划，缺乏视野和空间想象能力。相比之下，人类在思维过程中利用大脑的左半球和右半球进行语言和视觉规划。因此，我们在这项工作中引入了一种新颖的视觉语言规划框架，以对具有任何形式输入的任务执行并发视觉和语言规划。我们的框架结合了视觉规划来捕捉复杂的环境细节，而语言规划则增强了整个系统的逻辑连贯性。我们评估了我们的框架在视觉语言任务、仅视觉任务和仅语言任务中的有效性。结果证明了我们的方法的卓越性能，表明视觉和语言规划的集成可以产生更好的上下文感知任务执行。]]></description>
      <guid>https://arxiv.org/abs/2402.10534</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>联合语义分割和单目深度估计的高效多任务不确定性</title>
      <link>https://arxiv.org/abs/2402.10580</link>
      <description><![CDATA[arXiv:2402.10580v1 公告类型：新
摘要：量化预测不确定性成为解决诸如过度自信或深度神经网络缺乏可解释性和鲁棒性等常见挑战的一种可能解决方案，尽管这种方法通常计算成本很高。许多现实世界的应用本质上是多模式的，因此受益于多任务学习。例如，在自动驾驶领域，语义分割和单目深度估计的联合解决方案已被证明是有价值的。在这项工作中，我们首先将不同的不确定性量化方法与联合语义分割和单目深度估计结合起来，并评估它们之间的性能比较。此外，我们还揭示了与单独解决两个任务相比，多任务学习在不确定性质量方面的优势。基于这些见解，我们引入了 EMUFormer，这是一种新颖的师生蒸馏方法，用于联合语义分割和单目深度估计以及高效的多任务不确定性量化。通过隐式利用教师的预测不确定性，EMUFormer 在 Cityscapes 和 NYUv2 上取得了新的最先进的结果，并且还估计了这两项任务的高质量预测不确定性，尽管它们是一个顺序，但与 Deep Ensemble 相当或优于 Deep Ensemble。幅度更有效。]]></description>
      <guid>https://arxiv.org/abs/2402.10580</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>CodaMal：低成本显微镜中疟疾检测的对比域适应</title>
      <link>https://arxiv.org/abs/2402.10478</link>
      <description><![CDATA[arXiv:2402.10478v1 公告类型：新
摘要：疟疾是世界范围内的一个主要健康问题，其诊断需要可扩展的解决方案，能够与低成本显微镜（LCM）有效配合。基于深度学习的方法在显微图像的计算机辅助诊断方面取得了成功。然而，这些方法需要带注释的图像来显示受疟疾寄生虫影响的细胞及其生命阶段。与注释来自高成本显微镜 (HCM) 的图像相比，注释来自 LCM 的图像显着增加了医学专家的负担。因此，一个实用的解决方案将在 HCM 图像上进行训练，该解决方案在测试过程中应该能够很好地推广到 LCM 图像上。虽然早期的方法采用多阶段学习过程，但它们没有提供端到端的方法。在这项工作中，我们提出了一个端到端学习框架，名为 CodaMal（疟疾对比域适应）。为了弥合 HCM（训练）和 LCM（测试）之间的差距，我们提出了域自适应对比损失。它通过促进 HCM 表示及其相应的 LCM 图像之间的相似性来减少域偏移，而不会造成额外的注释负担。此外，训练目标包括经过精心设计的增强功能的物体检测目标，确保准确检测疟疾寄生虫。在公开的大规模 M5 数据集上，我们提出的方法在平均精度指标 (mAP) 方面比最先进的方法显着提高了 16%，在推理过程中提供了 21 倍的加速，并且只需要之前方法一半的可学习参数。我们的代码是公开的。]]></description>
      <guid>https://arxiv.org/abs/2402.10478</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>进行廉价的缩放：用于更高分辨率适应的自级联扩散模型</title>
      <link>https://arxiv.org/abs/2402.10491</link>
      <description><![CDATA[arXiv:2402.10491v1 公告类型：新
摘要：扩散模型已被证明在图像和视频生成方面非常有效；然而，由于单尺度训练数据，它们在生成不同尺寸的图像时仍然面临构图挑战。使大型预训练扩散模型适应更高分辨率需要大量的计算和优化资源，但实现与低分辨率模型相当的生成能力仍然难以实现。本文提出了一种新颖的自级联扩散模型，该模型利用从训练有素的低分辨率模型中获得的丰富知识来快速适应更高分辨率的图像和视频生成，采用免调整或廉价的上采样器调整范例。自级联扩散模型集成了一系列多尺度上采样器模块，可以有效地适应更高分辨率，同时保留原始组成和生成能力。我们进一步提出了一种枢轴引导的噪声重新调度策略，以加快推理过程并改善局部结构细节。与完全微调相比，我们的方法实现了 5 倍的训练加速，并且只需要额外的 0.002M 调整参数。大量实验表明，我们的方法可以通过仅 10k 步骤的微调来快速适应更高分辨率的图像和视频合成，几乎没有额外的推理时间。]]></description>
      <guid>https://arxiv.org/abs/2402.10491</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>基于实时模型的定量超声和雷达</title>
      <link>https://arxiv.org/abs/2402.10520</link>
      <description><![CDATA[arXiv:2402.10520v1 公告类型：新
摘要：超声波和雷达信号由于它们是非侵入性和非电离性的，对于医学成像非常有益。传统成像技术在对比度和物理解释方面存在局限性。定量医学成像可以显示各种物理特性，例如声速、密度、电导率和相对介电常数。这使得它具有更广泛的应用，包括改善癌症检测、诊断脂肪肝和快速中风成像。然而，当前从接收信号估计物理特性的定量成像技术（例如全波形反演）非常耗时，并且往往会收敛到局部最小值，使得它们不适合医学成像。为了解决这些挑战，我们提出了一种基于波传播物理模型的神经网络，它定义了接收信号和物理特性之间的关系。我们的网络可以仅使用八个元素的数据，在不到一秒的时间内针对复杂和现实的场景重建多个物理属性。我们展示了我们的方法对于雷达和超声波信号的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.10520</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>通过多模态数据和辅助任务集成优化皮肤病变分类</title>
      <link>https://arxiv.org/abs/2402.10454</link>
      <description><![CDATA[arXiv:2402.10454v1 公告类型：新
摘要：全球皮肤病患病率不断上升，其中一些如果不及时诊断和治疗可能会升级至危及生命的阶段，这对医疗保健提出了重大挑战。这个问题在偏远地区尤为严重，这些地区获得医疗服务的机会有限，常常导致治疗延迟，从而使皮肤病发展到更严重的阶段。诊断皮肤病的主要挑战之一是其类间差异较小，因为许多皮肤病表现出相似的视觉特征，这使得准确分类具有挑战性。这项研究引入了一种新颖的多模式方法来对皮肤病变进行分类，将智能手机捕获的图像与基本的临床和人口统计信息相结合。这种方法模仿了医疗专业人员所采用的诊断过程。该方法的一个独特之处是集成了专注于超分辨率图像预测的辅助任务。该组件在细化视觉细节和增强特征提取方面发挥着至关重要的作用，从而提高了类之间的区分度，从而提高了模型的整体有效性。实验评估是使用 PAD-UFES20 数据集进行的，应用了各种深度学习架构。这些实验的结果不仅证明了所提出方法的有效性，而且证明了其在资源贫乏的医疗保健环境中的潜在适用性。]]></description>
      <guid>https://arxiv.org/abs/2402.10454</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>Spike-EVPR：具有交叉表示聚合的深度尖峰残差网络，用于基于事件的视觉位置识别</title>
      <link>https://arxiv.org/abs/2402.10476</link>
      <description><![CDATA[arXiv:2402.10476v1 公告类型：新
摘要：近年来，事件摄像机已通过深度人工神经网络（ANN）成功应用于视觉地点识别（VPR）任务。然而，先前提出的深度 ANN 架构通常无法利用事件流中呈现的丰富时间信息。相比之下，深度尖峰网络表现出更复杂的时空动态，并且本质上非常适合处理稀疏异步事件流。不幸的是，直接将时间密集事件量输入尖峰网络会引入过多的时间步长，导致大规模 VPR 任务的训练成本过高。为了解决上述问题，我们提出了一种名为 Spike-EVPR 的新型深度尖峰网络架构，用于基于事件的 VPR 任务。首先，我们引入了两种专为 SNN 定制的新颖事件表示形式，以充分利用事件流中的时空信息，并尽可能减少训练期间的视频内存占用。然后，为了充分利用这两种表示的潜力，我们构建了一个具有强大表示能力的分叉尖峰残差编码器（BSR-Encoder），以更好地从两个事件表示中提取高级特征。接下来，我们介绍共享和特定描述符提取器（SSD-Extractor）。该模块旨在提取两种表示之间共享的特征以及每种表示所特有的特征。最后，我们提出了一个跨描述符聚合模块（CDA-Module），它融合了上述三个特征，生成一个精致的、鲁棒的场景全局描述符。我们的实验结果表明，与 Brisbane-Event-VPR 和 DDD20 数据集上的几个现有 EVPR 管道相比，我们的 Spike-EVPR 具有优越的性能，平均 Recall@1 在 Brisbane 上增加了 7.61%，在 DDD20 上增加了 13.20%。]]></description>
      <guid>https://arxiv.org/abs/2402.10476</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>深度光谱网格：使用图神经网络进行多频面部网格处理</title>
      <link>https://arxiv.org/abs/2402.10365</link>
      <description><![CDATA[arXiv:2402.10365v1 公告类型：新
摘要：随着虚拟世界的日益普及，数据驱动的 3D 网格参数化模型的重要性迅速增长。许多应用程序，例如计算机视觉、程序生成和网格编辑，很大程度上依赖于这些模型。然而，当前的方法不允许对不同频率级别的变形进行独立编辑。它们也没有受益于用专用表示来表示不同频率的变形，这将更好地暴露它们的属性并提高生成的网格的几何和感知质量。在这项工作中，引入了频谱网格作为将网格变形分解为低频和高频变形的方法。这些低频和高频变形的特征用于图卷积网络的表示学习。 3D 面部网格合成的参数模型基于所提出的框架构建，公开了控制解开的高频和低频变形的用户参数。独立控制不同频率下的变形和生成合理的合成示例是相互排斥的目标。引入条件因素来利用这些目标。我们的模型通过用不同的、更合适的表示来表示不同的频率水平，进一步利用了频谱划分的优势。低频用标准化欧几里得坐标表示，高频用标准化变形表示 (DR) 表示。本文研究了我们提出的方法在网格重建、网格插值和多频率编辑中的应用。事实证明，在考虑 $L_1$ 范数和感知二面角网格误差 (DAME) 指标时，我们的方法提高了大多数数据集生成网格的整体质量。]]></description>
      <guid>https://arxiv.org/abs/2402.10365</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>通过可视化分析解释生成扩散模型以实现可解释的决策过程</title>
      <link>https://arxiv.org/abs/2402.10404</link>
      <description><![CDATA[arXiv:2402.10404v1 公告类型：新
摘要：扩散模型在生成任务中表现出了卓越的性能。然而，解释扩散过程仍然具有挑战性，因为它是一系列去噪噪声图像，专家很难解释。为了解决这个问题，我们提出了三个研究问题，从模型生成的视觉概念和模型在每个时间步骤中参与的区域的角度来解释扩散过程。我们设计了用于可视化扩散过程并回答上述研究问题的工具，以使扩散过程易于人类理解。我们通过使用工具进行各种视觉分析的实验结果解释去噪水平并强调每个时间步骤与基本视觉概念的关系，从而展示如何在扩散过程中逐步生成输出。在扩散模型的整个训练过程中，模型学习了每个时间步对应的不同视觉概念，使模型能够预测不同阶段不同级别的视觉概念。我们使用覆盖面积 (AUC) 评分、相关性量化和交叉注意力映射来证实我们的工具。我们的研究结果提供了对扩散过程的见解，并为进一步研究可解释的扩散机制铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2402.10404</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>用于被遮挡人员重新识别的动态补丁感知丰富变压器</title>
      <link>https://arxiv.org/abs/2402.10435</link>
      <description><![CDATA[arXiv:2402.10435v1 公告类型：新
摘要：行人重新识别（re-ID）仍然是一个重大挑战，特别是在涉及遮挡的场景中。先前旨在解决遮挡问题的方法主要集中于通过利用外部语义线索来对齐身体特征。然而，这些方法往往很复杂并且容易受到噪声的影响。为了解决上述挑战，我们提出了一种创新的端到端解决方案，称为动态补丁感知丰富变压器（DPEFormer）。该模型自动动态地有效区分人体信息和遮挡，无需外部探测器或精确的图像对齐。具体来说，我们引入了动态补丁令牌选择模块（DPSM）。 DPSM 利用标签引导的代理令牌作为中介来识别信息丰富的无遮挡令牌。然后选择这些标记来导出后续的局部特征。为了促进全局分类特征与 DPSM 选择的精细局部特征的无缝集成，我们引入了一种新颖的特征混合模块（FBM）。 FBM 通过信息的互补性和零件多样性的利用来增强特征表示。此外，为了确保 DPSM 和整个 DPEFormer 能够仅使用身份标签进行有效学习，我们还提出了现实遮挡增强（ROA）策略。该策略利用了分段任意模型 (SAM) 的最新进展。因此，它生成的遮挡图像与现实世界的遮挡非常相似，极大地增强了后续的对比学习过程。对遮挡和整体 re-ID 基准的实验表明 DPEFormer 相对于现有最先进的方法取得了实质性进步。该代码将公开。]]></description>
      <guid>https://arxiv.org/abs/2402.10435</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>HI-GAN：具有辅助输入的分层修复 GAN，用于组合 RGB 和深度修复</title>
      <link>https://arxiv.org/abs/2402.10334</link>
      <description><![CDATA[arXiv:2402.10334v1 公告类型：新
摘要：修复涉及填充图像中缺失的像素或区域，这是混合现实环境中各种应用中采用的一项关键技术，特别是在内容从用户视觉环境中删除的缩小现实 (DR) 中。现有方法依赖于数字替换技术，这需要多个摄像机并且成本高昂。 AR 设备和智能手机使用 ToF 深度传感器来捕获与 RGB 图像对齐的场景深度图。尽管速度快且价格实惠，但 ToF 相机创建的深度图不完美且像素缺失。为了解决上述挑战，我们提出了分层修复 GAN（HI-GAN），这是一种以分层方式包含三个 GAN 的新颖方法，用于 RGBD 修复。 EdgeGAN 和 LabelGAN 分别修复掩模边缘和分割标签图像，而 CombinedRGBD-GAN 结合其潜在表示输出并执行 RGB 和深度修复。边缘图像，特别是分割标签图像作为辅助输入，通过互补上下文和分层优化显着增强修复性能。我们相信我们首次尝试将标签图像合并到修复过程中。与之前需要多个顺序模型和单独输出的方法不同，我们的工作以端到端的方式运行，同时分层地训练所有三个模型。具体来说，EdgeGAN 和 LabelGAN 首先分别进行优化，然后在 CombinedRGBD-GAN 中进一步优化，以提高修复质量。实验表明，与现有方法相比，HI-GAN 可以无缝工作并实现整体优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.10334</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>评估田间条件下 3D 植物几何重建的 NeRF</title>
      <link>https://arxiv.org/abs/2402.10344</link>
      <description><![CDATA[arXiv:2402.10344v1 公告类型：新
摘要：我们评估了不同的神经辐射场 (NeRF) 技术，用于在不同环境（从室内环境到室外环境）中重建 (3D) 植物。传统技术常常难以捕捉植物的复杂细节，这对于植物学和农业的理解至关重要。我们评估了复杂性不断增加的三种场景，并将结果与​​使用 LiDAR 作为地面实况数据获得的点云进行比较。在最真实的现场场景中，NeRF 模型在 GPU 上训练 30 分钟即可获得 74.65% 的 F1 分数，凸显了 NeRF 在充满挑战的环境中的效率和准确性。这些发现不仅证明了 NeRF 在详细而真实的 3D 植物建模方面的潜力，而且还提出了提高 3D 重建过程的速度和效率的实用方法。]]></description>
      <guid>https://arxiv.org/abs/2402.10344</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>GaussianObject：只需拍摄四张图像即可通过高斯泼溅获得高质量的 3D 对象</title>
      <link>https://arxiv.org/abs/2402.10259</link>
      <description><![CDATA[arXiv:2402.10259v1 公告类型：新
摘要：从高度稀疏的视图中重建和渲染 3D 对象对于促进 3D 视觉技术的应用和改善用户体验至关重要。然而，稀疏视图的图像仅包含非常有限的 3D 信息，这导致了两个重大挑战：1）由于用于匹配的图像太少，难以建立多视图一致性； 2）由于视图覆盖不足，部分省略或高度压缩对象信息。为了应对这些挑战，我们提出了 GaussianObject，一个用高斯泼溅表示和渲染 3D 对象的框架，只需 4 个输入图像即可实现高渲染质量。我们首先介绍视觉船体和漂浮物消除技术，这些技术将结构先验明确地注入到初始优化过程中，以帮助构建多视图一致性，从而产生粗糙的 3D 高斯表示。然后，我们基于扩散模型构建高斯修复模型来补充遗漏的对象信息，其中高斯被进一步细化。我们设计了一种自生成策略来获取图像对来训练修复模型。我们的 GaussianObject 在几个具有挑战性的数据集上进行了评估，包括 MipNeRF360、OmniObject3D 和 OpenIllumination，仅通过 4 个视图即可实现强大的重建结果，并且显着优于以前的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2402.10259</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:02 GMT</pubDate>
    </item>
    </channel>
</rss>