<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 19 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>IMPRINT：通过学习身份保留表示来生成对象合成</title>
      <link>https://arxiv.org/abs/2403.10701</link>
      <description><![CDATA[arXiv:2403.10701v1 公告类型：新
摘要：生成对象合成成为合成图像编辑的一种有前途的新途径。然而，对象身份保存的要求提出了重大挑战，限制了大多数现有方法的实际使用。为此，本文介绍了 IMPRINT，这是一种新颖的基于扩散的生成模型，采用两阶段学习框架进行训练，将身份保存的学习与合成的学习分离。第一阶段的目标是对对象编码器进行上下文无关、身份保留的预训练，使编码器能够学习既具有视图不变性又有利于增强细节保留的嵌入。后续阶段利用这种表示来学习与背景合成的对象的无缝协调。此外，IMPRINT 还采用了形状引导机制，为合成过程提供用户导向的控制。大量实验表明，IMPRINT 在身份保存和成分质量方面显着优于现有方法和各种基线。]]></description>
      <guid>https://arxiv.org/abs/2403.10701</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 Faster R-CNN 检测大麻种子变异</title>
      <link>https://arxiv.org/abs/2403.10722</link>
      <description><![CDATA[arXiv:2403.10722v1 公告类型：新
摘要：分析和检测大麻种子变异对于农业至关重要。它可以实现精准育种，使栽培者能够选择性地增强所需的性状。准确识别种子变体还可以确保法规遵从性，促进具有明确特征的特定大麻品种的种植，最终提高农业生产力并满足多样化的市场需求。本文介绍了采用最先进的对象检测模型 Faster R-CNN 进行大麻种子变异检测的研究。这项研究在泰国本地采购的大麻种子数据集上实施了该模型，其中包括 17 个不同的类别。我们通过比较各种指标的性能来评估六个 Faster R-CNN 模型，并获得 94.08% 的 mAP 分数和 95.66% 的 F1 分数。本文介绍了深度神经网络对象检测模型在视觉识别大麻种子类型的新任务中的第一个已知应用。]]></description>
      <guid>https://arxiv.org/abs/2403.10722</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>GS-Pose：基于通用分割的 6D 物体姿态估计的级联框架</title>
      <link>https://arxiv.org/abs/2403.10683</link>
      <description><![CDATA[arXiv:2403.10683v1 公告类型：新
摘要：本文介绍了 GS-Pose，一种用于定位和估计物体 6D 位姿的端到端框架。 GS-Pose 从一组以前未见过的物体的 RGB 图像开始，并构建存储在数据库中的三个不同的表示。在推理时，GS-Pose 通过在输入图像中定位对象、使用检索方法估计其初始 6D 姿势并使用渲染和比较方法细化姿势来按顺序进行操作。关键的见解是在过程的每个阶段应用适当的对象表示。特别是，对于细化步骤，我们利用 3D 高斯泼溅，这是一种新颖的可微分渲染技术，可提供较高的渲染速度和相对较短的优化时间。现成的工具链和商用硬件（例如移动电话）可用于捕获要添加到数据库中的新对象。对 LINEMOD 和 OnePose-LowTexture 数据集的广泛评估展示了出色的性能，确立了新的最先进水平。项目页面：https://dingdingcai.github.io/gs-pose。]]></description>
      <guid>https://arxiv.org/abs/2403.10683</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>论[V]-曼巴的低射可转移性</title>
      <link>https://arxiv.org/abs/2403.10696</link>
      <description><![CDATA[arXiv:2403.10696v1 公告类型：新
摘要：现代大规模神经网络的优势在于它们能够有效地适应示例较少的新任务。尽管广泛的研究已经调查了视觉变压器（ViT）在不同约束下向各种下游任务的可迁移性，但本研究将重点转移到探索 [V]-Mamba 的迁移学习潜力。我们将其性能与不同的少样本数据预算和高效传输方法的 ViT 进行比较。我们的分析得出了 [V]-Mamba 的少样本迁移性能的三个关键见解：(a) 在利用线性探测 (LP) 进行迁移时，[V]-Mamba 表现出比 ViT 更优越或相当的少样本学习能力，(b) ）相反，当采用视觉提示（VP）作为迁移方法时，[V]-Mamba 与 ViT 相比表现出更弱或相似的少样本学习性能，并且（c）我们观察到通过 LP 迁移的性能差距之间存在弱正相关性VP 和 [V]-Mamba 模型的规模。这一初步分析为更全面的研究奠定了基础，旨在进一步了解 [V]-Mamba 变体的功能及其与 ViT 的区别。]]></description>
      <guid>https://arxiv.org/abs/2403.10696</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>针对嘈杂的脑 MRI 的基于影响的鲁棒训练方法</title>
      <link>https://arxiv.org/abs/2403.10698</link>
      <description><![CDATA[arXiv:2403.10698v1 公告类型：新
摘要：正确分类脑肿瘤对于患者的及时、准确治疗至关重要。虽然已经提出了几种基于经典图像处理或深度学习方法的分类算法来快速对 MR 图像中的肿瘤进行分类，但大多数算法都假设无噪声训练数据的不切实际的设置。在这项工作中，我们研究了一种困难但现实的环境，即在嘈杂的 MR 图像上训练深度学习模型以对脑肿瘤进行分类。我们提出了两种对噪声 MRI 训练数据具有鲁棒性的训练方法：基于影响的样本重新称重（ISR）和基于影响的样本扰动（ISP），它们基于鲁棒统计的影响函数。使用影响函数，在 ISR 中，我们根据训练示例对训练过程的帮助/有害程度自适应地重新权衡训练示例，而在 ISP 中，我们精心设计并注入与影响分数成比例的有用扰动。 ISR 和 ISP 都针对噪声训练数据强化了分类模型，而不会显着影响模型对测试数据的泛化能力。我们对常见的脑肿瘤数据集进行实证评估，并将 ISR 和 ISP 与三个基线进行比较。我们的实证结果表明，ISR 和 ISP 可以有效地训练对噪声训练数据具有鲁棒性的深度学习模型。]]></description>
      <guid>https://arxiv.org/abs/2403.10698</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>PALM：推动自适应学习率机制以实现持续的测试时间适应</title>
      <link>https://arxiv.org/abs/2403.10650</link>
      <description><![CDATA[arXiv:2403.10650v1 公告类型：新
摘要：动态环境中的现实世界视觉模型面临域分布的快速变化，导致识别性能下降。持续测试时间适应（CTTA）使用测试数据直接调整预先训练的源判别模型以适应这些变化的领域。高效的 CTTA 方法涉及应用分层自适应学习率，并有选择地适应预训练层。然而，它对域转移的估计很差并且伪标签引起的不准确。在这项工作中，我们的目标是通过量化模型预测不确定性来识别层，而不依赖伪标签，从而克服这些限制。我们利用梯度的大小作为度量，通过反向传播 softmax 输出和均匀分布之间的 KL 散度来计算，以选择用于进一步适应的层。随后，对于仅属于这些选定层的参数，其余参数被冻结，我们评估它们的敏感性以近似域移位，然后相应地调整它们的学习率。总的来说，这种方法比以前的方法带来了更稳健和稳定的优化。我们在 CIFAR-10C、CIFAR-100C 和 ImageNet-C 上进行了广泛的图像分类实验，并证明了我们的方法相对于标准基准和先前方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.10650</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>InterLUDE：标记和未标记数据之间的交互以增强半监督学习</title>
      <link>https://arxiv.org/abs/2403.10658</link>
      <description><![CDATA[arXiv:2403.10658v1 公告类型：新
摘要：半监督学习（SSL）旨在通过对标记和未标记数据进行训练来提高任务性能。主流 SSL 图像分类方法主要优化损失，将监督分类目标与仅源自未标记数据的正则化项相加。这种表述忽略了标记图像和未标记图像之间相互作用的可能性。在本文中，我们介绍了 InterLUDE，这是一种增强 SSL 的新方法，由两部分组成，每个部分都受益于标记-未标记交互。第一部分是嵌入融合，在标记和未标记嵌入之间进行插值以改进表示学习。第二部分是基于一致性正则化原则的新损失，旨在最大限度地减少标记输入与未标记输入之间模型预测的差异。对标准封闭集 SSL 基准测试和使用未经策划的未标记集的医学 SSL 任务进行的实验表明，我们的方法具有明显的优势。在只有 40 个标签的 STL-10 数据集上，InterLUDE 实现了 3.2% 的错误率，而之前最好的方法报告为 14.9%。]]></description>
      <guid>https://arxiv.org/abs/2403.10658</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>SwinMTL：用于单目相机图像同步深度估计和语义分割的共享架构</title>
      <link>https://arxiv.org/abs/2403.10662</link>
      <description><![CDATA[arXiv:2403.10662v1 公告类型：新
摘要：本文提出了一种创新的多任务学习框架，允许使用单个相机同时进行深度估计和语义分割。所提出的方法基于共享的编码器-解码器架构，该架构集成了各种技术来提高深度估计和语义分割任务的准确性，而不影响计算效率。此外，本文还结合了对抗性训练组件，采用 Wasserstein GAN 框架和批评者网络来完善模型的预测。该框架在两个数据集（室外 Cityscapes 数据集和室内 NYU Depth V2 数据集）上进行了彻底评估，并且在分割和深度估计任务方面都优于现有的最先进方法。我们还进行了消融研究来分析不同组件的贡献，包括预训练策略、评论家的加入、对数深度缩放的使用和高级图像增强，以更好地理解所提出的框架。随附的源代码可在 \url{https://github.com/PardisTaghavi/SwinMTL} 访问。]]></description>
      <guid>https://arxiv.org/abs/2403.10662</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>LightIt：漫射模型的照明建模和控制</title>
      <link>https://arxiv.org/abs/2403.10615</link>
      <description><![CDATA[arXiv:2403.10615v1 公告类型：新
摘要：我们介绍 LightIt，一种用于图像生成的显式照明控制方法。最近的生成方法缺乏照明控制，这对于图像生成的许多艺术方面（例如设置整体情绪或电影外观）至关重要。为了克服这些限制，我们建议以着色和法线贴图为生成条件。我们使用单次反射着色对光照进行建模，其中包括投射阴影。我们首先训练一个阴影估计模块来生成真实世界图像和阴影对的数据集。然后，我们使用估计的阴影和法线作为输入来训练控制网络。我们的方法在许多场景中展示了高质量的图像生成和照明控制。此外，我们使用生成的数据集来训练以图像和目标着色为条件的身份保留重新照明模型。我们的方法是第一个能够使用可控、一致的照明生成图像的方法，并且其性能与专门的重新照明最先进的方法相当。]]></description>
      <guid>https://arxiv.org/abs/2403.10615</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>利用 CLIP 推断敏感信息并提高模型公平性</title>
      <link>https://arxiv.org/abs/2403.10624</link>
      <description><![CDATA[arXiv:2403.10624v1 公告类型：新
摘要：众所周知，基于深度学习的视觉识别模型中存在不同子群体的性能差异，但之前的工作在很大程度上解决了假设了解敏感属性标签的这种公平性问题。为了克服这种依赖，以前的策略涉及单独的学习结构来暴露和调整差异。在这项工作中，我们探索了一种不需要敏感属性标签的新范式，并通过利用视觉语言模型 CLIP 作为丰富的知识源来推断敏感信息，从而避免了额外的训练。我们提出基于图像和属性指定语言嵌入的相似性的样本聚类，并评估它们与真实属性分布的对应关系。我们通过重新采样和增强表现不佳的集群来训练目标模型。对多个基准偏差数据集的大量实验表明，该模型相对于现有基线有明显的公平性增益，这表明 CLIP 可以提取由语言提示的歧视性敏感信息，并用于促进模型公平性。]]></description>
      <guid>https://arxiv.org/abs/2403.10624</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>MeDSLIP：用于细粒度对齐的医学双流语言图像预训练</title>
      <link>https://arxiv.org/abs/2403.10635</link>
      <description><![CDATA[arXiv:2403.10635v1 公告类型：新
摘要：视觉语言预训练（VLP）模型在医学领域取得了显着的进步。然而，大多数 VLP 模型在非常粗略的水平上将原始报告与图像对齐，而没有对报告中概述的解剖和病理概念与图像中相应的语义对应项之间的细粒度关系进行建模。为了解决这个问题，我们提出了医学双流语言图像预训练（MeDSLIP）框架。具体来说，MeDSLIP 通过将视觉和文本表示分解为解剖学相关和病理学相关的流来建立视觉语言细粒度对齐。此外，MeDSLIP 采用了一种新颖的视觉语言原型对比学习（ProtoCL）方法来增强解剖学和病理学流中的对齐。 MeDSLIP 进一步采用跨流图像内对比学习 (ICL)，以确保成对的解剖学和病理学概念在同一图像内一致共存。这种跨流正则化鼓励模型利用两个流之间的同步性来进行更全面的表示学习。 MeDSLIP 在三个公共数据集的零样本和监督微调设置下进行评估：NIH CXR14、RSNA Pneumonia 和 SIIM-ACR Pneumothorax。在这些设置下，MeDSLIP 在分类、基础和分割任务方面优于六种领先的基于 CNN 的模型。]]></description>
      <guid>https://arxiv.org/abs/2403.10635</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>针对模型反转攻击的隐私保护人脸识别的自适应混合屏蔽策略</title>
      <link>https://arxiv.org/abs/2403.10558</link>
      <description><![CDATA[arXiv:2403.10558v1 公告类型：新
摘要：在训练人脸识别（FR）模型中使用个人敏感数据会带来严重的隐私问题，因为对手可以利用模型反转攻击（MIA）来推断原始训练数据。现有的防御方法，例如数据增强和差异隐私，已被用来缓解这个问题。然而，这些方法往往无法在隐私和准确性之间取得最佳平衡。为了解决这个限制，本文引入了一种针对 MIA 的自适应混合掩码算法。具体来说，使用自适应 MixUp 策略在频域中屏蔽人脸图像。与主要用于数据增强的传统 MixUp 算法不同，我们的修改方法结合了频域混合。之前的研究表明，增加 MixUp 中混合的图像数量可以增强隐私保护，但代价是降低人脸识别准确率。为了克服这种权衡，我们开发了一种基于强化学习的增强型自适应混合策略，这使我们能够混合大量图像，同时保持令人满意的识别精度。为了优化隐私保护，我们建议在策略网络的训练过程中最大化奖励函数（即FR系统的损失函数）。而FR网络的损失函数在训练FR网络的阶段被最小化。策略网络和人脸识别网络在训练过程中可以被视为对抗实体，最终达到更平衡的权衡。实验结果表明，我们提出的混合屏蔽方案在隐私保护和针对 MIA 的识别精度方面优于现有的防御算法。]]></description>
      <guid>https://arxiv.org/abs/2403.10558</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 Spatio-TemporalTransformers 进行自适应跟踪的自回归查询</title>
      <link>https://arxiv.org/abs/2403.10574</link>
      <description><![CDATA[arXiv:2403.10574v1 公告类型：新
摘要：丰富的时空信息对于捕获视觉跟踪中复杂的目标外观变化至关重要。然而，大多数性能最佳的跟踪算法依赖于许多手工制作的组件来进行时空信息聚合。因此，时空信息还远未得到充分探索。为了缓解这个问题，我们提出了一种带有时空转换器的自适应跟踪器（名为 AQATrack），它采用简单的自回归查询来有效地学习时空信息，而无需许多手工设计的组件。首先，我们引入一组可学习的自回归查询，以滑动窗口方式捕获瞬时目标外观变化。然后，我们设计了一种新颖的注意机制，用于现有查询的交互，以在当前帧中生成新查询。最后，基于初始目标模板和学习的自回归查询，设计时空信息融合模块（STM），用于时空形成聚合以定位目标对象。受益于STM，我们可以有效地结合静态外观和瞬时变化来指导鲁棒跟踪。大量实验表明，我们的方法显着提高了跟踪器在六种流行跟踪基准上的性能：LaSOT、LaSOText、TrackingNet、GOT-10k、TNL2K 和 UAV123。]]></description>
      <guid>https://arxiv.org/abs/2403.10574</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>SurvRNC：使用 Rank-N-Contrast 学习生存预测的有序表示</title>
      <link>https://arxiv.org/abs/2403.10603</link>
      <description><![CDATA[arXiv:2403.10603v1 公告类型：新
摘要：预测生存可能性对于诊断出癌症的个体至关重要，因为它提供了有关早期预后的宝贵信息。这些知识有助于制定有效的治疗计划，从而改善患者的治疗效果。在过去的几年中，深度学习模型为评估医学图像、电子健康记录和基因组数据以估计癌症风险评分提供了可行的解决方案。然而，这些模型往往无法发挥其潜力，因为它们很难学习回归感知的特征表示。在本研究中，我们提出了生存排名-N 对比（SurvRNC）方法，该方法引入损失函数作为正则化器，以获得基于生存时间的有序表示。该函数可以处理审查数据，并且可以合并到任何生存模型中，以确保学习到的表示是有序的。该模型在 HEad \&amp; NeCK TumOR (HECKTOR) 分割和结果预测任务数据集上进行了广泛评估。我们证明，使用 SurvRNC 方法进行训练可以在不同的深度生存模型上获得更高的性能。此外，它在一致性指数上比最先进的方法高出 3.6%。该代码可在 https://github.com/numanai/SurvRNC 上公开获取]]></description>
      <guid>https://arxiv.org/abs/2403.10603</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>VIREAS：复杂的视觉推理与无法回答的问题</title>
      <link>https://arxiv.org/abs/2403.10534</link>
      <description><![CDATA[arXiv:2403.10534v1 公告类型：新
摘要：在回答问题之前验证问题的有效性在现实应用中至关重要，因为用户可能会提供不完美的指令。在这种情况下，理想的模型应该解决查询中的差异并将其传达给用户，而不是生成最佳答案。为了满足这一要求，我们引入了一种新的组合视觉问答数据集 VISREAS，它由可回答和不可回答的视觉查询组成，这些查询是通过遍历和扰乱对象、属性和关系之间的共性和差异而制定的。 VISREAS 包含使用 Visual Genome 场景图自动生成的 207 万个语义多样的查询。该任务的独特之处在于，在回答之前验证图像的问题可回答性，以及最先进模型的较差性能，激发了新的模块化基线 LOGIC2VISION 的设计，该基线通过生成和执行伪代码而无需任何操作来进行推理。外部模块来生成答案。 LOGIC2VISION 的性能优于 VISREAS 中的生成模型（比 LLaVA-1.5 + 4.82%；比 InstructBLIP + 12.23%），并且相对于分类模型在性能上取得了显着提升。]]></description>
      <guid>https://arxiv.org/abs/2403.10534</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:52 GMT</pubDate>
    </item>
    </channel>
</rss>