<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 03 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>LOTUS：通过稀疏剪枝和数据彩票提高变压器效率</title>
      <link>https://arxiv.org/abs/2405.00906</link>
      <description><![CDATA[arXiv:2405.00906v1 公告类型：新
摘要：视觉转换器彻底改变了计算机视觉，但其计算需求给训练和部署带来了挑战。本文介绍了 LOTUS（具有超稀疏性的 LOttery Transformers），这是一种利用数据彩票选择和稀疏修剪来加速视觉变换器训练并同时保持准确性的新颖方法。我们的方法侧重于识别和利用信息最丰富的数据子集，并消除冗余模型参数以优化训练过程。通过大量的实验，我们证明了 LOTUS 在实现快速收敛和高精度方面的有效性，同时显着降低了计算要求。这项工作凸显了将数据选择和稀疏技术相结合进行高效视觉变换训练的潜力，为该领域的进一步研究和开发打开了大门。]]></description>
      <guid>https://arxiv.org/abs/2405.00906</guid>
      <pubDate>Fri, 03 May 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>基于变压器的自我监督学习对缺血性中风血栓起源的组织病理学分类</title>
      <link>https://arxiv.org/abs/2405.00908</link>
      <description><![CDATA[arXiv:2405.00908v1 公告类型：新
摘要： 背景与目的：识别缺血性脑卒中的血栓栓塞来源对于治疗和二级预防至关重要，但往往尚未确定。这项研究描述了栓塞数字病理学中的一种自监督深度学习方法，用于根据组织病理学图像对缺血性中风血栓起源进行分类。方法：数据集包括来自 STRIP AI Kaggle 挑战的完整幻灯片图像 (WSI)，其中包括机械取栓后从缺血性中风患者身上回收的血栓。基于 Transformer 的深度学习模型是使用迁移学习和自监督预训练开发的，用于对 WSI 进行分类。定制包括注意力池层、加权损失函数和阈值优化。测试和比较了各种模型架构，并且主要使用加权对数损失来评估模型性能。结果：该模型在交叉验证中的对数损失分数为 0.662，在测试集上的对数损失分数为 0.659。比较不同的模型主干，swin_large_patch4_window12_384 显示出更高的性能。采用凝块起源分类的阈值技术来平衡假阳性和阴性。结论：该研究证明了基于 Transformer 的深度学习模型在从组织病理学图像中识别缺血性中风血栓起源方面的有效性，并强调需要专门适用于血栓 WSI 的精细建模技术。需要进一步的研究来提高模型性能、可解释性，验证其有效性。未来的增强可能包括整合更大的患者群体、先进的预处理策略以及探索整体多模式方法以提高诊断准确性。]]></description>
      <guid>https://arxiv.org/abs/2405.00908</guid>
      <pubDate>Fri, 03 May 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>EchoScene：通过场景图扩散的信息回声生成室内场景</title>
      <link>https://arxiv.org/abs/2405.00915</link>
      <description><![CDATA[arXiv:2405.00915v1 公告类型：新
摘要：我们提出了 EchoScene，一种交互式且可控的生成模型，可以在场景图上生成 3D 室内场景。 EchoScene 利用双分支扩散模型来动态适应场景图。由于节点数量不同、多个边组合以及操纵器引起的节点边操作，现有方法很难处理场景图。 EchoScene 通过将每个节点与去噪过程相关联并实现协作信息交换来克服这一问题，从而增强了解全局约束的可控且一致的生成。这是通过形状和布局分支中的信息回显方案来实现的。在每个去噪步骤中，所有进程都与信息交换单元共享其去噪数据，该信息交换单元使用图卷积组合这些更新。该方案确保去噪过程受到对场景图的整体理解的影响，从而促进全局连贯场景的生成。可以在推理过程中通过编辑输入场景图并对扩散模型中的噪声进行采样来操纵生成的场景。大量的实验验证了我们的方法，该方法保持了场景的可控性，并在生成保真度方面超越了以前的方法。此外，生成的场景具有高质量，因此与现成的纹理生成直接兼容。代码和训练模型都是开源的。]]></description>
      <guid>https://arxiv.org/abs/2405.00915</guid>
      <pubDate>Fri, 03 May 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>Wake Vision：用于 TinyML 人员检测的大规模、多样化数据集和基准套件</title>
      <link>https://arxiv.org/abs/2405.00892</link>
      <description><![CDATA[arXiv:2405.00892v1 公告类型：新
摘要：极低功耗设备上的机器学习应用程序（通常称为微型机器学习（TinyML））有望打造一个更智能、更互联的世界。然而，当前 TinyML 研究的进展受到相关数据集的有限大小和质量的阻碍。为了应对这一挑战，我们引入了 Wake Vision，这是一个为人体检测量身定制的大规模、多样化的数据集，这是 TinyML 视觉传感的典型任务。 Wake Vision 包含超过 600 万张图像，比之前的标准增加了百倍，并且经过了彻底的质量过滤。与既定基准相比，使用唤醒视觉进行训练的准确度提高了 2.41%。除了数据集之外，我们还提供了五个详细基准集的集合，用于评估测试数据特定部分的模型性能，例如不同的照明条件、距相机的距离以及受试者的人口统计特征。这些新颖的细粒度基准有助于在具有挑战性的现实场景中评估模型质量，而在仅关注整体准确性时常常会忽略这些场景。通过在基准上对 MobileNetV2 TinyML 模型进行评估，我们发现，在检测远处主体时，输入分辨率比模型宽度发挥着更重要的作用，并且由于数据集的质量，量化对模型鲁棒性的影响很小。这些发现强调了详细评估以确定模型开发的基本因素的重要性。数据集、基准套件、代码和模型在 CC-BY 4.0 许可证下公开可用，使其可用于商业用例。]]></description>
      <guid>https://arxiv.org/abs/2405.00892</guid>
      <pubDate>Fri, 03 May 2024 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>DiL-NeRF：深入研究激光雷达在街景上的神经辐射场</title>
      <link>https://arxiv.org/abs/2405.00900</link>
      <description><![CDATA[arXiv:2405.00900v1 公告类型：新
摘要：真实感模拟在自动驾驶等应用中发挥着至关重要的作用，其中神经辐射场 (NeRF) 的进步可以通过自动创建数字 3D 资产实现更好的可扩展性。然而，由于很大程度上共线的相机运动和高速下的稀疏采样，街道场景的重建质量受到影响。另一方面，应用程序通常需要从偏离输入的摄像机视图进行渲染，以准确模拟变道等行为。在本文中，我们提出了一些见解，可以更好地利用激光雷达数据来提高街道场景的 NeRF 质量。首先，我们的框架从激光雷达学习几何场景表示，该表示与用于辐射解码的隐式基于网格的表示融合，从而提供显式点云提供的更强的几何信息。其次，我们提出了一种强大的遮挡感知深度监督方案，该方案允许通过积累来利用致密的激光雷达点。第三，我们从激光雷达点生成增强的训练视图以进一步改进。我们的见解转化为在真实驾驶场景下大大改进的新颖视图合成。]]></description>
      <guid>https://arxiv.org/abs/2405.00900</guid>
      <pubDate>Fri, 03 May 2024 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>超越人类视觉：大视觉语言模型在显微镜图像分析中的作用</title>
      <link>https://arxiv.org/abs/2405.00876</link>
      <description><![CDATA[arXiv:2405.00876v1 公告类型：新
摘要：视觉语言模型（VLM）最近出现并因其理解图像和文本数据的双重模态的能力而受到关注。 LLaVA、ChatGPT-4 和 Gemini 等 VLM 最近在自然图像字幕、视觉问答 (VQA) 和空间推理等任务上表现出了令人印象深刻的性能。此外，Meta AI 的通用分割模型 Segment Anything Model (SAM) 在从不可预见的图像中分离对象方面表现出了前所未有的性能。由于医学专家、生物学家和材料科学家经常结合标题、文献或报告形式的文本信息检查显微镜或医学图像，并得出非常重要和有价值的结论，因此测试 VLM 的性能无疑是必要的以及这些图像上的基础模型，例如 SAM。在这项研究中，我们让 ChatGPT、LLaVA、Gemini 和 SAM 对各种显微图像执行分类、分割、计数和 VQA 任务。我们观察到，ChatGPT 和 Gemini 能够理解显微图像中的视觉特征，而 SAM 则非常有能力隔离一般意义上的伪影。然而，其性能与领域专家的性能并不接近——模型很容易受到图像中存在的杂质、缺陷、伪影重叠和多样性的影响。]]></description>
      <guid>https://arxiv.org/abs/2405.00876</guid>
      <pubDate>Fri, 03 May 2024 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>SonicDiffusion：使用预训练扩散模型进行音频驱动图像生成和编辑</title>
      <link>https://arxiv.org/abs/2405.00878</link>
      <description><![CDATA[arXiv:2405.00878v1 公告类型：新
摘要：随着大规模文本到图像生成方法最近的成功，我们正在见证条件图像合成的一场革命。这一成功还为使用多模式输入控制生成和编辑过程开辟了新的机会。虽然使用深度、草图和其他图像等线索进行空间控制吸引了大量研究，但我们认为另一种同样有效的方式是音频，因为声音和视觉是人类感知的两个主要组成部分。因此，我们提出了一种在大规模图像扩散模型中启用音频调节的方法。我们的方法首先将从音频剪辑获得的特征映射到可以以类似于文本标记的方式注入到扩散模型中的标记。我们引入了额外的音频图像交叉注意层，我们在冻结扩散模型原始层的权重的同时对其进行微调。除了音频条件图像生成之外，我们的方法还可以与基于扩散的编辑方法结合使用，以实现音频条件图像编辑。我们在广泛的音频和图像数据集上演示了我们的方法。我们与最近的方法进行了广泛的比较，并显示出良好的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.00878</guid>
      <pubDate>Fri, 03 May 2024 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>通过 Triplane Fusion 进行连贯 3D 肖像视频重建</title>
      <link>https://arxiv.org/abs/2405.00794</link>
      <description><![CDATA[arXiv:2405.00794v1 公告类型：新
摘要：最近在单图像 3D 肖像重建方面取得的突破使得远程呈现系统能够从单个摄像机实时传输 3D 肖像视频，从而可能使远程呈现民主化。然而，每帧 3D 重建表现出时间不一致，并且会忘记用户的外观。另一方面，自我重演方法可以通过驱动个性化 3D 先验来渲染连贯的 3D 肖像，但无法忠实地重建用户的每帧外观（例如面部表情和灯光）。在这项工作中，我们认识到需要保持连贯的身份和动态的每帧外观，以实现最佳的真实感。为此，我们提出了一种新的基于融合的方法，该方法将个性化 3D 主题先验与每帧信息融合，生成时间稳定的 3D 视频，并忠实重建用户的每帧外观。我们基于编码器的方法仅使用由表达条件 3D GAN 生成的合成数据进行训练，在工作室和野外数据集上实现了最先进的 3D 重建精度和时间一致性。]]></description>
      <guid>https://arxiv.org/abs/2405.00794</guid>
      <pubDate>Fri, 03 May 2024 06:18:40 GMT</pubDate>
    </item>
    <item>
      <title>Brighteye：基于 Vision Transformer 的彩色眼底照片青光眼筛查</title>
      <link>https://arxiv.org/abs/2405.00857</link>
      <description><![CDATA[arXiv:2405.00857v1 公告类型：新
摘要：图像质量、照明条件和患者人口统计数据的差异对彩色眼底摄影的自动青光眼检测提出了挑战。 Brighteye是一种基于Vision Transformer的方法，被提出用于青光眼检测和青光眼特征分类。 Brighteye 使用自注意力机制来学习大型眼底图像中像素之间的远程关系。在输入 Brighteye 之前，使用 YOLOv8 对视盘进行定位，并裁剪视盘中心周围的感兴趣区域 (ROI)，以确保与临床实践保持一致。视盘检测将青光眼检测的 95% 特异性灵敏度从 79.20% 提高到 85.70%，将青光眼特征分类的汉明距离从 0.2470 提高到 0.1250。在 AI 青光眼筛查合理推荐 (JustRAIGS) 挑战赛的发展阶段，总体结果在 226 项参赛作品中排名第五。]]></description>
      <guid>https://arxiv.org/abs/2405.00857</guid>
      <pubDate>Fri, 03 May 2024 06:18:40 GMT</pubDate>
    </item>
    <item>
      <title>引导条件扩散分类器 (ConDiff) 用于增强糖尿病足溃疡感染的预测</title>
      <link>https://arxiv.org/abs/2405.00858</link>
      <description><![CDATA[arXiv:2405.00858v1 公告类型：新
摘要：从照片中检测糖尿病足溃疡（DFU）的感染伤口，防止严重并发症和截肢。方法：本文提出了引导条件扩散分类器（ConDiff），这是一种新颖的深度学习感染检测模型，它将引导图像合成与去噪扩散模型和基于距离的分类相结合。该过程包括（1）通过向引导图像注入高斯噪声来生成引导条件合成图像，然后通过反向扩散过程对受噪声扰动的图像进行去噪，以感染状态为条件；（2）根据最小欧几里德距离对感染进行分类嵌入空间中合成图像和原始引导图像之间的关系。结果：ConDiff 表现出卓越的性能，准确率为 83%，F1 分数为 0.858，比最先进的模型至少高出 3%。使用三元组损失函数可以减少基于距离的分类器中的过度拟合。结论：ConDiff 不仅提高了 DFU 感染的诊断准确性，而且开创性地使用生成判别模型进行详细的医学图像分析，为改善患者治疗结果提供了一种有前景的方法。]]></description>
      <guid>https://arxiv.org/abs/2405.00858</guid>
      <pubDate>Fri, 03 May 2024 06:18:40 GMT</pubDate>
    </item>
    <item>
      <title>用于调整文本到图像扩散模型的深度奖励监督</title>
      <link>https://arxiv.org/abs/2405.00760</link>
      <description><![CDATA[arXiv:2405.00760v1 公告类型：新
摘要：使用给定的奖励函数优化文本到图像的扩散模型是一个重要但尚未充分探索的研究领域。在本研究中，我们提出了深度奖励调整（DRTune），这是一种直接监督文本到图像扩散模型的最终输出图像并通过迭代采样过程反向传播到输入噪声的算法。我们发现，训练采样过程中的早期步骤对于低水平奖励至关重要，并且可以通过停止去噪网络输入的梯度来高效且有效地实现深度监督。 DRTune 在各种奖励模型上进行了广泛的评估。它始终优于其他算法，特别是对于低级控制信号，所有浅层监督方法都失败了。此外，我们通过 DRTune 微调 Stable Diffusion XL 1.0 (SDXL 1.0) 模型，以优化 Human Preference Score v2.1，从而产生有利的 Diffusion XL 1.0 (FDXL 1.0) 模型。与 SDXL 1.0 相比，FDXL 1.0 显着提高了图像质量，并达到了与 Midjourney v5.2 相当的质量。]]></description>
      <guid>https://arxiv.org/abs/2405.00760</guid>
      <pubDate>Fri, 03 May 2024 06:18:39 GMT</pubDate>
    </item>
    <item>
      <title>获得多对象生成的有利布局</title>
      <link>https://arxiv.org/abs/2405.00791</link>
      <description><![CDATA[arXiv:2405.00791v1 公告类型：新
摘要：可以根据文本提示生成高质量和多样化图像的大型文本到图像模型已经取得了显着的成功。这些模型的最终目标是创建复杂的场景，而解决多主体生成的挑战是实现这一目标的关键一步。然而，现有最先进的扩散模型在生成涉及多个主题的图像时面临困难。当出现包含多个主题的提示时，这些模型可能会省略一些主题或将它们合并在一起。为了应对这一挑战，我们提出了一种基于指导原则的新方法。我们允许扩散模型最初提出布局，然后重新排列布局网格。这是通过强制交叉注意图（XAM）遵守建议的掩模并将像素从潜在图迁移到我们确定的新位置来实现的。我们引入了新的损失项，旨在减少 XAM 熵，以更清晰地定义主体的空间，减少 XAM 之间的重叠，并确保 XAM 与其各自的掩模对齐。我们将我们的方法与几种替代方法进行了对比，并表明它更忠实地捕捉了各种文本提示中所需的概念。]]></description>
      <guid>https://arxiv.org/abs/2405.00791</guid>
      <pubDate>Fri, 03 May 2024 06:18:39 GMT</pubDate>
    </item>
    <item>
      <title>越多越好：多源深度域适应</title>
      <link>https://arxiv.org/abs/2405.00749</link>
      <description><![CDATA[arXiv:2405.00749v1 公告类型：新
摘要：在许多实际应用中，获取大规模标记数据来训练最先进的深度神经网络通常是困难且昂贵的。因此，将学到的知识从单独的、标记的源域转移到未标记或稀疏标记的目标域成为一种有吸引力的替代方案。然而，直接转移通常会因域转移而导致性能显着下降。域适应（DA）旨在通过对齐源域和目标域之间的分布来解决这个问题。多源域适应（MDA）是一种强大且实用的扩展，其中标记数据可以从具有不同分布的多个源收集。在本次调查中，我们首先定义了各种 MDA 策略。然后我们从不同角度系统地总结和比较了深度学习时代的现代 MDA 方法，并提供了常用的数据集和简短的基准测试。最后，我们讨论了 MDA 未来值得研究的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2405.00749</guid>
      <pubDate>Fri, 03 May 2024 06:18:38 GMT</pubDate>
    </item>
    <item>
      <title>CLIPArTT：CLIP 在测试时轻量级适应新领域</title>
      <link>https://arxiv.org/abs/2405.00754</link>
      <description><![CDATA[arXiv:2405.00754v1 公告类型：新
摘要：以 CLIP 为代表的预训练视觉语言模型 (VLM) 在无需额外训练的情况下在零样本分类任务中表现出卓越的适应性。然而，它们的性能在域转移的情况下会降低。在本研究中，我们介绍了 CLIP 测试时适应 (CLIPArTT)，这是一种 CLIP 的完全测试时适应 (TTA) 方法，其中涉及在推理过程中自动构建文本提示，以用作文本监督。我们的方法采用独特的、微创的文本提示调整过程，其中多个预测类别被聚合到一个新的文本提示中，用作伪标签以转导方式对输入重新分类。此外，我们还率先在 VLM 领域实现了 TTA 基准（例如 TENT）的标准化。我们的研究结果表明，无需额外的转换或新的可训练模块，CLIPArTT 即可动态增强 CIFAR-10 等未损坏数据集、CIFAR-10-C 和 CIFAR-10.1 等损坏数据集以及 VisDA-C 等合成数据集的性能。这项研究强调了通过新颖的测试时策略提高 VLM 适应性的潜力，为跨不同数据集和环境的稳健性能提供见解。代码可以在以下位置找到：https://github.com/dosowiechi/CLIPArTT.git]]></description>
      <guid>https://arxiv.org/abs/2405.00754</guid>
      <pubDate>Fri, 03 May 2024 06:18:38 GMT</pubDate>
    </item>
    <item>
      <title>对比视觉语言预训练中的字幕多样性建模</title>
      <link>https://arxiv.org/abs/2405.00740</link>
      <description><![CDATA[arXiv:2405.00740v1 公告类型：新
摘要：为图像添加标题的方法有一千种。另一方面，对比语言预训练 (CLIP) 的工作原理是将图像及其标题映射到单个向量 - 限制类 CLIP 模型表示描述图像的不同方式的能力。在这项工作中，我们介绍了 Llip，潜在语言图像预训练，它对可以匹配图像的字幕的多样性进行建模。 Llip 的视觉编码器输出一组视觉特征，通过对从文本中导出的信息进行调节，将这些特征混合成最终表示。我们证明，即使使用大规模编码器，Llip 在各种任务上也优于 CLIP 和 SigLIP 等非上下文化基线。 Llip 使用 ViT-G/14 编码器将零样本分类平均提高了 2.9%。具体来说，Llip 在 ImageNet 上获得了 83.5% 的零样本 top-1 准确率，比类似大小的 CLIP 的准确率高出 1.4%。我们还证明 MS-COCO 上的零样本检索提高了 6.0%。我们对该方法引入的组件进行了全面分析，并证明 Llip 可以带来更丰富的视觉表示。]]></description>
      <guid>https://arxiv.org/abs/2405.00740</guid>
      <pubDate>Fri, 03 May 2024 06:18:37 GMT</pubDate>
    </item>
    </channel>
</rss>