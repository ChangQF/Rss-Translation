<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 25 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>学习眼睛注视预测的高斯表示</title>
      <link>https://arxiv.org/abs/2403.14821</link>
      <description><![CDATA[arXiv:2403.14821v1 公告类型：新
摘要：现有的眼睛注视预测方法执行从输入图像到从原始注视点生成的相应密集注视图的映射。然而，由于人类注视的随机性，生成的密集注视图可能不是人类注视的理想表示。为了提供稳健的注视模型，我们引入了高斯表示法来进行眼睛注视建模。具体来说，我们建议将眼睛注视图建模为概率分布的混合，即高斯混合模型。在这个新的表示中，我们使用几个高斯分布分量作为所提供的注视图的替代，这使得模型对注视的随机性更加鲁棒。同时，我们在一些轻量级主干上设计了我们的框架，以实现实时注视预测。三个公共注视预测数据集（SALICON、MIT1003、TORONTO）的实验结果表明我们的方法快速有效。]]></description>
      <guid>https://arxiv.org/abs/2403.14821</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>用于保留属性的图像匿名化的潜在扩散模型</title>
      <link>https://arxiv.org/abs/2403.14790</link>
      <description><![CDATA[arXiv:2403.14790v1 公告类型：新
摘要：图像匿名化生成技术具有巨大的潜力，可以生成保护图像中描绘的隐私的数据集，同时实现高数据保真度和实用性。现有方法广泛关注保留面部属性，但未能采用更全面的视角，将场景和背景考虑到匿名化过程中。据我们所知，本文提出了第一种基于潜在扩散模型（LDM）的图像匿名化方法。场景中的每个元素都保持传达相同的含义，但其操纵方式却使重新识别变得困难。为此，我们提出了两种 LDM：CAMOUFLAGE-Base 结合了预先训练的 ControlNet 和旨在增加真实图像与匿名图像之间距离的新控制机制。 CAMOFULAGE-Light 基于适配器技术，并结合旨在有效表示场景中不同人的属性的编码。前一种解决方案在大多数指标和基准测试中实现了卓越的性能，而后者则以微调轻量级模块为代价将推理时间缩短了一半。我们通过广泛的实验比较表明，所提出的方法在身份混淆方面与最先进的方法具有竞争力，同时更好地保留了图像的原始内容并解决了当前解决方案无法解决的未解决的挑战。]]></description>
      <guid>https://arxiv.org/abs/2403.14790</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>在连续检测中通过记忆网络防止灾难性遗忘</title>
      <link>https://arxiv.org/abs/2403.14797</link>
      <description><![CDATA[arXiv:2403.14797v1 公告类型：新
摘要：现代预训练架构在对新任务进行持续微调的同时，很难保留以前的信息。尽管连续分类取得了显着进展，但为检测或分割等复杂视觉任务设计的系统仍然难以获得令人满意的性能。在这项工作中，我们引入了一种基于内存的检测变压器架构，使预训练的 DETR 型检测器适应新任务，同时保留以前任务的知识。我们提出了一种新颖的本地化查询函数，用于从记忆单元中高效地检索信息，旨在最大限度地减少遗忘。此外，我们还发现了连续检测中的一个基本挑战，称为背景降级。当早期任务中的对象类别重新出现在未来任务中时（可能没有标签），就会出现这种情况，导致它们被隐式地视为背景。这是连续检测或分割中不可避免的问题。引入的持续优化技术有效地解决了这一挑战。最后，我们评估了我们提出的系统在连续检测基准上的性能，并证明我们的方法超越了现有最先进的性能，从而在以下任务上比 MS-COCO 和 PASCAL-VOC 提高了 5-7%持续检测。]]></description>
      <guid>https://arxiv.org/abs/2403.14797</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>多智能体 VQA：探索零样本视觉问答中的多智能体基础模型</title>
      <link>https://arxiv.org/abs/2403.14783</link>
      <description><![CDATA[arXiv:2403.14783v1 公告类型：新
摘要：这项工作探讨了视觉问答（VQA）任务中基础模型的零样本能力。我们提出了一种名为 Multi-Agent VQA 的自适应多智能体系统，通过使用专门的智能体作为工具来克服基础模型在目标检测和计数方面的局限性。与现有方法不同，我们的研究重点关注系统的性能，而不是在特定的 VQA 数据集上对其进行微调，从而使其在开放世界中更加实用和稳健。我们展示了零样本场景下的初步实验结果，并重点介绍了一些失败案例，为未来的研究提供了新的方向。]]></description>
      <guid>https://arxiv.org/abs/2403.14783</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>关于 DCT 统计在裁剪检测器中的应用</title>
      <link>https://arxiv.org/abs/2403.14789</link>
      <description><![CDATA[arXiv:2403.14789v1 公告类型：新
摘要：{对离散余弦变换 (DCT) 得出的频率分量的研究已广泛应用于图像分析。近年来，人们发现可以从它们中推断出有关图像生命周期的重要信息，但没有研究集中于它们与图像源分辨率之间的分析。在这项工作中，我们研究了一种新颖的图像分辨率分类器，它采用 DCT 统计，旨在检测图像的原始分辨率；特别是利用这种洞察力来解决识别裁剪图像的挑战。在整个图像（未裁剪）上训练机器学习 (ML) 分类器，生成的模型可以利用此信息来检测裁剪。结果证明了分类器在区分裁剪图像和未裁剪图像方面的可靠性，提供了对其原始分辨率的可靠估计。这一进步通过提供检测图像处理和增强定性图像评估的新工具，对图像处理应用产生重大影响，包括数字安全、真实性验证和视觉质量分析。这项工作开辟了该领域的新视角，有可能改变多个领域的图像分析和使用。}]]></description>
      <guid>https://arxiv.org/abs/2403.14789</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言模型的少样本对抗性即时学习</title>
      <link>https://arxiv.org/abs/2403.14774</link>
      <description><![CDATA[arXiv:2403.14774v1 公告类型：新
摘要：深度神经网络对难以察觉的对抗性扰动的脆弱性引起了广泛的关注。受视觉语言基础模型成功的启发，之前的工作通过将对抗性视觉特征与文本监督结合起来，实现了零样本对抗鲁棒性。然而，在实践中，由于适应成本高、文本监督不理想以及自然泛化能力不受控制等问题，它们仍然不能令人满意。在本文中，为了解决这些问题，我们提出了一种少量对抗性提示框架，其中使用有限数据调整输入序列可以显着提高对抗性鲁棒性。具体来说，我们通过提供从对抗性示例中端到端学习的对抗性相关文本监督来实现这一目标。我们还提出了一种新颖的训练目标，该目标增强多模态特征的一致性，同时鼓励自然示例和对抗性示例之间区分单模态特征。所提出的框架提供了学习对抗性文本监督的机会，它提供了卓越的跨模式对抗性对齐，并仅用 1% 的训练数据就可以达到最先进的零样本对抗性鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2403.14774</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>扩散攻击：利用稳定扩散进行自然图像攻击</title>
      <link>https://arxiv.org/abs/2403.14778</link>
      <description><![CDATA[arXiv:2403.14778v1 公告类型：新
摘要：在虚拟现实（VR）中，对抗性攻击仍然是一个重大的安全威胁。大多数基于深度学习的物理和数字对抗攻击方法都侧重于通过制作包含大量可打印扭曲的对抗示例来增强攻击性能，这些扭曲易于人类观察者识别。然而，攻击者很少对生成的攻击图像的外观自然度和舒适度施加限制，从而导致明显且不自然的攻击。为了应对这一挑战，我们提出了一个框架，将风格迁移融入到自然风格的对抗性输入中，表现出最小的可检测性和最大的自然外观，同时保持卓越的攻击能力。]]></description>
      <guid>https://arxiv.org/abs/2403.14778</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>Champ：具有 3D 参数化指导的可控且一致的人体图像动画</title>
      <link>https://arxiv.org/abs/2403.14781</link>
      <description><![CDATA[arXiv:2403.14781v1 公告类型：新
摘要：在这项研究中，我们引入了一种人类图像动画方法，通过在潜在扩散框架内利用 3D 人体参数模型来增强当前人类生成技术中的形状对齐和运动引导。该方法利用SMPL（Skinned Multi-Person Linear）模型作为3D人体参数模型来建立身体形状和姿势的统一表示。这有助于从源视频中准确捕捉复杂的人体几何形状和运动特征。具体来说，我们结合了从 SMPL 序列获得的渲染深度图像、法线图和语义图，以及基于骨架的运动指导，以丰富具有全面 3D 形状和详细姿势属性的潜在扩散模型的条件。采用集成自注意力机制的多层运动融合模块来融合空间域中的形状和运动潜在表示。通过将 3D 人体参数化模型表示为运动指导，我们可以在参考图像和源视频运动之间进行人体参数化形状对齐。对基准数据集进行的实验评估表明，该方法具有生成高质量人体动画的卓越能力，可以准确捕获姿势和形状变化。此外，我们的方法还在所提出的野生数据集上表现出卓越的泛化能力。项目页面：https://fudan-generative-vision.github.io/champ。]]></description>
      <guid>https://arxiv.org/abs/2403.14781</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>通过稀疏编码架构提高模型反转攻击的鲁棒性</title>
      <link>https://arxiv.org/abs/2403.14772</link>
      <description><![CDATA[arXiv:2403.14772v1 公告类型：新
摘要：最近的模型反转攻击算法允许对手仅通过重复查询网络并检查其输出来重建神经网络的私有训练数据。在这项工作中，我们开发了一种新颖的网络架构，它利用稀疏编码层来获得针对此类攻击的卓越鲁棒性。三十年的计算机科学研究已经在图像去噪、对象识别和对抗性错误分类设置的背景下研究了稀疏编码，但据我们所知，它与最先进的隐私漏洞的联系仍未得到研究。然而，稀疏编码架构提出了一种防御模型反转攻击的有利方法，因为它们允许我们以一种可以在训练期间有效计算的方式控制网络中间表示中编码的不相关私有信息的数量，并且已知这种方式几乎没有什么作用。对分类准确率的影响。具体来说，与使用各种最先进的防御方法训练的网络相比，我们的稀疏编码架构保持了相当或更高的分类精度，同时将最先进的训练数据重建性能降低了 1.1 到 18.3 倍。各种重建质量指标（PSNR、SSIM、FID）。这种性能优势适用于从 CelebA 人脸到医学图像和 CIFAR-10 的 5 个数据集，以及各种最先进的基于 SGD 和基于 GAN 的反转攻击，包括即插即用攻击。我们提供集群就绪的 PyTorch 代码库来促进研究和标准化防御评估。]]></description>
      <guid>https://arxiv.org/abs/2403.14772</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>StreamingT2V：从文本生成一致、动态且可扩展的长视频</title>
      <link>https://arxiv.org/abs/2403.14773</link>
      <description><![CDATA[arXiv:2403.14773v1 公告类型：新
摘要：文本到视频的扩散模型可以生成遵循文本指令的高质量视频，从而可以轻松创建多样化和个性化的内容。然而，现有的方法主要关注高质量的短视频生成（通常为 16 或 24 帧），当天真地扩展到长视频合成的情况时，最终会导致硬剪切。为了克服这些限制，我们引入了 StreamingT2V，这是一种自回归方法，用于生成具有平滑过渡的 80、240、600、1200 或更多帧的长视频。关键组件是：（i）称为条件注意模块（CAM）的短期记忆块，它通过注意机制根据从前一个块中提取的特征来调节当前一代，从而导致一致的块转换，（ii）称为外观保留模块的长期记忆块，它从第一个视频块中提取高级场景和对象特征，以防止模型忘记初始场景，以及（iii）一种随机混合方法，可以自回归地应用视频增强器对于无限长的视频，块之间没有不一致。实验表明StreamingT2V产生高运动量。相比之下，当以自回归方式简单应用时，所有竞争的图像到视频方法都容易出现视频停滞。因此，我们通过 StreamingT2V 提出了一种高质量的无缝文本到长视频生成器，其在一致性和运动方面优于竞争对手。我们的代码将在以下位置提供：https://github.com/Picsart-AI-Research/StreamingT2V]]></description>
      <guid>https://arxiv.org/abs/2403.14773</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>VURF：视频理解的通用推理和自我改进框架</title>
      <link>https://arxiv.org/abs/2403.14743</link>
      <description><![CDATA[arXiv:2403.14743v1 公告类型：新
摘要：最近的研究证明了大型语言模型（LLM）作为推理模块的有效性，可以将复杂的任务解构为更易于管理的子任务，特别是当应用于图像的视觉推理任务时。相比之下，本文介绍了一种基于法学硕士推理能力的视频理解和推理框架（VURF）。我们的方法是一种新颖的方法，可以在视频任务的背景下扩展法学硕士的效用，利用其在上下文框架内从最小的输入和输出演示中进行概括的能力。通过向法学硕士提供成对的指令及其相应的高级程序，我们利用他们的上下文学习能力来生成用于视频理解的可执行视觉程序。为了提高程序的准确性和稳健性，我们实施了两项重要策略。首先，我们采用由 GPT-3.5 提供支持的反馈生成方法来纠正使用不支持的功能的程序中的错误。其次，从最近关于法学硕士输出自我完善的工作中汲取灵感，我们引入了一种迭代程序，通过将初始输出与法学硕士不受约束时本应生成的输出对齐来提高上下文示例的质量。上下文示例的结构。我们在多个视频特定任务（包括视觉 QA、视频预期、姿态估计和多视频 QA）上的结果说明了这些增强功能在提高视频任务可视化编程方法性能方面的功效。我们的准则和数据将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2403.14743</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>3D视觉语言模型能否真正理解自然语言？</title>
      <link>https://arxiv.org/abs/2403.14760</link>
      <description><![CDATA[arXiv:2403.14760v1 公告类型：新
摘要：3D 视觉语言 (3D-VL) 任务的快速发展为人类使用自然语言与实体代理或机器人交互开辟了新途径。尽管取得了这些进展，我们还是发现了一个显着的局限性：现有的 3D-VL 模型对语言输入的风格表现出敏感性，难以理解具有相同语义但以不同变体编写的句子。这一观察结果提出了一个关键问题：3D 视觉语言模型能否真正理解自然语言？为了测试 3D-VL 模型的语言可理解性，我们首先提出了一种语言鲁棒性任务，用于系统地评估各种任务中的 3D-VL 模型，并在呈现不同语言风格变体时对其性能进行基准测试。重要的是，考虑到人类语言的多样性和不可预测性，这些变体通常在需要与人类直接交互的应用中遇到，例如实体机器人技术。我们提出了一个基于人类语言特征设计的3D语言鲁棒性数据集，以促进鲁棒性的系统研究。我们的综合评估发现，所有现有模型在各种 3D-VL 任务中的性能均显着下降。即使是最先进的 3D-LLM 也无法理解同一句子的某些变体。进一步深入分析表明，现有模型存在脆弱且有偏差的融合模块，这源于现有数据集的低多样性。最后，我们提出了一个由 LLM 驱动的免训练模块，它提高了语言的鲁棒性。数据集和代码将在 github 上提供。]]></description>
      <guid>https://arxiv.org/abs/2403.14760</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>具有时间关系知识的深度生成域适应用于跨用户活动识别</title>
      <link>https://arxiv.org/abs/2403.14682</link>
      <description><![CDATA[arXiv:2403.14682v1 公告类型：新
摘要：在人类活动识别（HAR）中，训练和测试数据独立且同分布（i.i.d.）的假设经常失败，特别是在数据分布变化很大的跨用户场景中。这种差异凸显了 HAR 中传统域适应方法的局限性，这些方法通常忽略了时间序列数据中固有的时间关系。为了弥补这一差距，我们的研究引入了具有通用序列映射的条件变分自动编码器（CVAE-USM）方法，该方法通过放宽独立同分布来解决 HAR 中时间序列域适应的独特挑战。假设并利用时间关系来有效地调整不同用户之间的数据分布。该方法结合了变分自动编码器 (VAE) 和通用序列映射 (USM) 的优势，捕获并利用用户之间的常见时间模式来改进活动识别。我们的结果在两个公共 HAR 数据集（OPPT 和 PAMAP2）上进行评估，表明 CVAE-USM 优于现有的最先进方法，为跨用户活动识别提供了更准确和通用的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2403.14682</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>FOCIL：通过训练随机修剪的稀疏专家实现在线课堂增量学习的微调和冻结</title>
      <link>https://arxiv.org/abs/2403.14684</link>
      <description><![CDATA[arXiv:2403.14684v1 公告类型：新
摘要：在线持续学习环境中的类增量学习（CIL）致力于从数据流中获取一系列新类的知识，每个数据点仅使用一次进行训练。与离线模式相比，这更加现实，离线模式假设来自新类的所有数据都是容易获得的。当前的在线 CIL 方法存储先前数据的子集，这在内存和计算以及隐私问题方面造成了沉重的开销成本。在本文中，我们提出了一种新的在线 CIL 方法，称为 FOCIL。它通过为每个任务训练随机修剪的稀疏子网络来不断微调主要架构。然后，它会冻结经过训练的连接以防止遗忘。 FOCIL 还自适应地确定每个任务的稀疏级别和学习率，并确保所有任务的（几乎）零遗忘，而不存储任何重播数据。 10 任务 CIFAR100、20 任务 CIFAR100 和 100 任务 TinyImagenet 上的实验结果表明，我们的方法大幅优于 SOTA。该代码可在 https://github.com/muratonuryildirim/FOCIL 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2403.14684</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>自动训练一次：控制器网络引导的从头开始的自动网络修剪</title>
      <link>https://arxiv.org/abs/2403.14729</link>
      <description><![CDATA[arXiv:2403.14729v1 公告类型：新
摘要：当前的深度神经网络（DNN）修剪技术通常涉及复杂的多步骤过程，需要特定领域的专业知识，这使得它们的广泛采用具有挑战性。为了解决这一限制，提出了 Only-Train-Once (OTO) 和 OTOv2，通过从头开始直接训练和压缩通用 DNN 来消除额外微调步骤的需要。然而，优化器的静态设计（在 OTO 中）可能会导致局部最优的收敛问题。在本文中，我们提出了 Auto-Train-Once (ATO)，这是一种创新的网络修剪算法，旨在自动降低 DNN 的计算和存储成本。在模型训练阶段，我们的方法不仅训练目标模型，还利用控制器网络作为架构生成器来指导目标模型权重的学习。此外，我们开发了一种新颖的随机梯度算法，可以增强模型训练和控制器网络训练之间的协调，从而提高剪枝性能。我们提供了全面的收敛分析以及广泛的实验，结果表明我们的方法在标准基准数据集上的各种模型架构（包括 ResNet18、ResNet34、ResNet50、ResNet56 和 MobileNetv2）上实现了最先进的性能（ CIFAR-10、CIFAR-100 和 ImageNet）。]]></description>
      <guid>https://arxiv.org/abs/2403.14729</guid>
      <pubDate>Mon, 25 Mar 2024 06:18:05 GMT</pubDate>
    </item>
    </channel>
</rss>