<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 29 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>人体形状和服装估计</title>
      <link>https://arxiv.org/abs/2402.18032</link>
      <description><![CDATA[arXiv:2402.18032v1 公告类型：新
摘要：人体形状和服装估计在在线购物、时尚零售、增强现实 (AR)、虚拟现实 (VR) 和游戏等各个领域都获得了显着的重视。近年来，人体形状和服装的视觉表示已成为计算机视觉研究人员关注的焦点。本文对该领域的主要工作进行了全面的综述，重点关注四个关键方面：人体形状估计、时尚生成、地标检测和属性识别。对于每一项任务，调查论文都检查了最近的进展，讨论了它们的优点和局限性，以及方法和结果的质量差异。通过探索人体形状和服装估计的最新发展，本次调查旨在提供对该领域的全面了解，并启发该快速发展领域的未来研究。]]></description>
      <guid>https://arxiv.org/abs/2402.18032</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>表示 3D 稀疏地图点和线以进行相机重新定位</title>
      <link>https://arxiv.org/abs/2402.18011</link>
      <description><![CDATA[arXiv:2402.18011v1 公告类型：新
摘要：视觉定位和地图绘制的最新进展在集成点和线特征方面取得了相当大的成功。然而，扩展本地化框架以包含额外的映射组件经常会导致对专用于匹配任务的内存和计算资源的需求增加。在这项研究中，我们展示了轻量级神经网络如何学习表示 3D 点和线特征，并通过利用多个学习映射的力量来展示领先的姿势准确性。具体来说，我们利用单个转换器块来编码线特征，有效地将它们转换为独特的点状描述符。随后，我们将这些点和线描述符集视为不同但相互关联的特征集。通过在多个图层中集成自注意力和交叉注意力，我们的方法在使用两个简单的 MLP 回归 3D 地图之前有效地细化每个特征。在综合实验中，我们的室内定位结果在基于点和线辅助配置方面均超过了 Hloc 和 Limap。此外，在户外场景中，我们的方法取得了显着的领先优势，标志着与最先进的基于学习的方法相比的最显着的增强。这项工作的源代码和演示视频已公开：https://thpjp.github.io/pl2map/]]></description>
      <guid>https://arxiv.org/abs/2402.18011</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>打破黑匣子：针对分布偏移的置信引导模型反转攻击</title>
      <link>https://arxiv.org/abs/2402.18027</link>
      <description><![CDATA[arXiv:2402.18027v1 公告类型：新
摘要：模型反转攻击（MIA）试图通过查询模型生成反映目标类特征的合成图像来推断目标分类器的私有训练数据。然而，先前的研究依赖于对目标模型的完全访问，这在现实场景中并不实用。此外，现有的黑盒 MIA 假设图像先验和目标模型遵循相同的分布。然而，当面对不同的数据分布设置时，这些方法可能会导致攻击性能不佳。为了解决这些限制，本文提出了一种名为 CG-MI 的 \textbf{C}onfidence-\textbf{G}uided \textbf{M}odel \textbf{I}nversion 攻击方法，该方法利用预编码的潜在空间经过训练的公开可用的生成对抗网络（GAN）作为先验信息和无梯度优化器，在黑盒设置中实现跨不同数据分布的高分辨率 MIA。我们的实验表明，我们的方法显着\textbf{在不同的分布设置中，Celeba 的性能优于 SOTA 黑盒 MIA 超过 49\%，Facescrub 的性能优于 58\%}。此外，我们的方法展示了生成高质量图像\textbf{可与白盒攻击生成的图像相媲美}的能力。我们的方法为黑盒模型反转攻击提供了实用且有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2402.18027</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>OpenMEDLab：医学多模态基础模型的开源平台</title>
      <link>https://arxiv.org/abs/2402.18028</link>
      <description><![CDATA[arXiv:2402.18028v1 公告类型：新
摘要：GPTv4 和 Gemini 等通用人工智能的新兴趋势重塑了机器学习和许多其他研究领域的研究（学术界和工业界）格局。然而，此类基础模型的特定领域应用（例如在医学中）仍然没有受到影响或通常处于非常早期的阶段。它将需要一套单独的迁移学习和模型适应技术，通过进一步扩展和注入领域知识和数据这些模型。如果将数据、算法和预先训练的基础模型集中在一起并以有组织的方式开源，则可以大大加速此类技术的发展。在这项工作中，我们提出了 OpenMEDLab，一个用于多模态基础模型的开源平台。它不仅囊括了为一线临床和生物信息学应用提示和微调大型语言和视觉模型的开创性尝试的解决方案，而且还利用大规模多模态医疗数据构建特定领域的基础模型。重要的是，它为各种医学图像模式、临床文本、蛋白质工程等提供了一组预先训练的基础模型。在下游任务的各种基准中，还为每个收集的方法和模型展示了鼓舞人心且有竞争力的结果。我们欢迎医疗人工智能领域的研究人员不断为OpenMEDLab贡献前沿的方法和模型，可以通过https://github.com/openmedlab访问。]]></description>
      <guid>https://arxiv.org/abs/2402.18028</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>PolyOculus：基于图像的同时多视图新颖视图合成</title>
      <link>https://arxiv.org/abs/2402.17986</link>
      <description><![CDATA[arXiv:2402.17986v1 公告类型：新
摘要：本文考虑了生成新颖视图合成（GNVS）的问题，即在给定有限数量的已知视图的情况下生成新颖的、合理的场景视图。在这里，我们提出了一种基于集合的生成模型，它可以同时生成多个、自洽的新视图，以任意数量的已知视图为条件。我们的方法不限于一次生成单个图像，并且可以以零个、一个或多个视图为条件。因此，当生成大量视图时，我们的方法不限于低阶自回归生成方法，并且能够更好地在大量图像上保持生成的图像质量。我们在标准 NVS 数据集上评估了所提出的模型，并表明它优于最先进的基于图像的 GNVS 基线。此外，我们表明该模型能够生成没有自然顺序排序的相机视图集，例如循环和双目轨迹，并且在此类任务上显着优于其他方法。]]></description>
      <guid>https://arxiv.org/abs/2402.17986</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>快速且可解释的 2D 单应性分解：相似性-核-相似性和仿射-核心-仿射变换</title>
      <link>https://arxiv.org/abs/2402.18008</link>
      <description><![CDATA[arXiv:2402.18008v1 公告类型：新
摘要：在本文中，我们提出了两种快速且可解释的二维单应性分解方法，分别称为相似性-核相似性（SKS）和仿射-核心-仿射（ACA）变换。在最小$4$点配置下，SKS中的第一个和最后一个相似变换分别由目标平面和源平面上的两个锚点计算。然后，可以利用其他两点对应关系来计算仅具有四个参数的中间核变换。此外，ACA 使用三个锚点来计算第一个和最后一个仿射变换，然后利用另一个点对应来计算中间核心变换。 ACA 只需 85 美元的浮点运算 (FLOP) 即可计算出最大规模的单应性，甚至不需要任何除法运算。因此，作为一个插件模块，ACA 促进了传统的基于特征的随机样本共识 (RANSAC) 管道，以及估计 4 美元点偏移的深度单应性管道。除了几何参数化和计算效率的优点外，SKS和ACA还可以通过输入坐标的多项式（$7$次到$9$次）来表达单应性的每个元素，扩展了现有本质的相似性仿射射影（SAP ）统一分解并计算二维仿射变换。源代码发布在 https://github.com/cscvlab/SKS-Homography。]]></description>
      <guid>https://arxiv.org/abs/2402.18008</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>从泛化到精确：探索 SAM 在手术环境中的工具分割</title>
      <link>https://arxiv.org/abs/2402.17972</link>
      <description><![CDATA[arXiv:2402.17972v1 公告类型：新
摘要：目的：准确的工具分割在计算机辅助程序中至关重要。然而，由于医疗场景中伪影的存在和有限的训练数据，这项任务带来了挑战。推广到看不见的数据的方法代表了一个有趣的场所，其中零样本分割提供了解决数据限制的选项。分段任意模型 (SAM) 的初步探索性工作表明，基于边界框的提示呈现出显着的零短泛化能力。然而，基于点的提示会导致性能下降，在图像损坏的情况下进一步恶化。我们认为，SAM 严重过度分割具有高损坏级别的图像，导致仅考虑单个分割掩模时性能下降，而与感兴趣对象重叠的掩模组合会生成准确的预测。方法：我们使用 SAM 生成内窥镜帧的过分割预测。然后，我们使用 ground-truth 工具 mask 来分析当选择最佳单个 mask 作为预测时以及当与感兴趣对象重叠的所有单独 mask 组合以获得最终预测 mask 时 SAM 的结果。我们使用各种强度的合成损坏和具有反事实创建的真实世界损坏的内部数据集来分析 Endovis18 和 Endovis17 仪器分割数据集。结果：结合过度分割的掩模有助于改善 IoU。此外，选择最佳的单一分割可以为干净图像提供有竞争力的 IoU 分数。结论：组合 SAM 预测在一定的损坏水平下呈现出改进的结果和鲁棒性。然而，适当的提示策略是在医学领域实施这些模型的基础。]]></description>
      <guid>https://arxiv.org/abs/2402.17972</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>通过辅助对抗防御网络增强跟踪鲁棒性</title>
      <link>https://arxiv.org/abs/2402.17976</link>
      <description><![CDATA[arXiv:2402.17976v1 公告类型：新
摘要：视觉对象跟踪中的对抗性攻击通过在图像中引入难以察觉的扰动，显着降低了高级跟踪器的性能。近年来，这些攻击方法引起了研究人员的广泛关注。然而，目前仍然缺乏专门针对视觉目标跟踪设计对抗性防御方法的研究。为了解决这些问题，我们提出了一种名为 DuaLossDef 的有效附加预处理网络，它可以消除跟踪过程中的对抗性扰动。 DuaLossDef 部署在跟踪器的搜索分支或模板分支之前，以将防御变换应用于输入图像。此外，它可以作为即插即用模块与其他视觉跟踪器无缝集成，无需任何参数调整。我们使用对抗性训练来训练 DuaLossDef，特别是使用 Dua-Loss 来生成对抗性样本，同时攻击跟踪器的分类和回归分支。在 OTB100、LaSOT 和 VOT2018 基准上进行的大量实验表明，DuaLossDef 在自适应和非自适应攻击场景中都针对对抗性攻击方法保持了出色的防御鲁棒性。而且，当将防御网络转移到其他跟踪器时，它表现出可靠的可转移性。最后，DuaLossDef 的处理时间高达 5ms/帧，允许与现有高速跟踪器无缝集成，而不会引入大量计算开销。我们将很快公开我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2402.17976</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>利用稀疏数据进行快速高光谱光热中红外光谱成像，用于妇科癌症组织亚型分析</title>
      <link>https://arxiv.org/abs/2402.17960</link>
      <description><![CDATA[arXiv:2402.17960v1 公告类型：新
摘要：卵巢癌的检测传统上依赖于多步骤过程，包括由经验丰富的病理学家进行的活检、组织染色和形态学分析。虽然这种传统方法被广泛应用，但存在几个缺点：它是定性的、耗时的，并且严重依赖于染色的质量。中红外（MIR）高光谱光热成像是一种无标记的生化定量技术，与机器学习算法相结合，可以消除染色的需要，并提供与传统组织学相当的定量结果。然而，这项技术进展缓慢。这项工作提出了一种新的中红外光热成像方法，将其速度提高了一个数量级。我们的方法通过捕获高分辨率和交错的低分辨率红外波段图像的组合并应用数据插值计算技术，显着加速了数据收集。我们通过利用稀疏数据采集和采用基于曲线的重建算法，有效地最小化数据收集要求。该方法能够从欠采样数据集重建高质量、高分辨率图像，并将数据采集时间缩短 10 倍。我们使用各种定量指标评估稀疏成像方法的性能，包括均方误差 (MSE)、结构相似性指数 (SSIM) 和组织亚型分类精度，同时采用随机森林和卷积神经网络 (CNN) 模型，并附有ROC曲线。我们基于 100 个卵巢癌患者样本和超过 6500 万个数据点的数据进行统计稳健分析，证明该方法能够产生卓越的图像质量并准确区分不同的妇科组织类型，分割精度超过 95%。]]></description>
      <guid>https://arxiv.org/abs/2402.17960</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>基于视觉语言模型的利用视觉上下文提取的字幕评估方法</title>
      <link>https://arxiv.org/abs/2402.17969</link>
      <description><![CDATA[arXiv:2402.17969v1 公告类型：新
摘要：鉴于视觉和语言建模的不断进步，对机器生成的图像描述的准确评估仍然至关重要。为了更接近人类偏好地评估字幕，指标需要区分不同质量和内容的字幕。然而，传统的指标无法超越单词的表面匹配或嵌入的相似性进行比较。因此，他们仍然需要改进。本文提出了VisCE$^2$，一种基于视觉语言模型的字幕评估方法。我们的方法侧重于视觉上下文，它指的是图像的详细内容，包括对象、属性和关系。通过提取它们并将其组织成结构化格式，我们用视觉上下文取代了人工编写的参考文献，并帮助 VLM 更好地理解图像，从而提高评估性能。通过对多个数据集的元评估，我们验证了 VisCE$^2$ 在捕获字幕质量方面优于传统的预训练指标，并表现出与人类判断的卓越一致性。]]></description>
      <guid>https://arxiv.org/abs/2402.17969</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>一切都在一个图像中：大型多模态模型是图像内学习器</title>
      <link>https://arxiv.org/abs/2402.17971</link>
      <description><![CDATA[arXiv:2402.17971v1 公告类型：新
摘要：本文介绍了一种新的上下文学习（ICL）机制，称为图像内学习（I$^2$L），它将演示示例、视觉提示和指令结合到单个图像中，以增强 GPT-4V 的能力。与之前依赖将图像转换为文本或将视觉输入合并到语言模型中的方法不同，I$^2$L 将所有信息整合到一张图像中，并主要利用图像处理、理解和推理能力。这有几个优点：它避免了对复杂图像的不准确的文本描述，提供了定位演示示例的灵活性，减少了输入负担，并通过消除对多个图像和冗长文本的需要来避免超出输入限制。为了进一步结合不同 ICL 方法的优势，我们引入了一种自动策略，为给定任务中的数据示例选择适当的 ICL 方法。我们在 MathVista 和 Hallusionbench 上进行了实验，测试 I$^2$L 在复杂的多模态推理任务以及减轻语言幻觉和视错觉方面的有效性。此外，我们还探讨了图像分辨率、演示示例数量及其位置对 I$^2$L 有效性的影响。我们的代码可在 https://github.com/AGI-Edgerunners/IIL 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2402.17971</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>通过交换任务进行语义分割的弱监督协同训练</title>
      <link>https://arxiv.org/abs/2402.17891</link>
      <description><![CDATA[arXiv:2402.17891v1 公告类型：新
摘要：类激活图（CAM）通常用于弱监督语义分割（WSSS）来生成伪标签。由于类激活不完整或过度，现有研究通常诉诸离线 CAM 细化，引入额外阶段或提出离线模块。这可能会导致单阶段方法的优化困难并限制通用性。在本研究中，我们的目标是减少观察到的 CAM 不一致和错误，以减轻对细化流程的依赖。我们提出了一种包含引导 CAM 的端到端 WSSS 模型，其中我们的分割模型在训练的同时在线优化 CAM。我们的方法“与交换分配进行协同训练”(CoSA) 利用双流框架，其中一个子网络从另一个子网络生成的交换分配中学习。我们介绍三种技术：i）基于软困惑的正则化来惩罚不确定区域； ii) 动态修正置信度阈值的阈值搜索方法； iii) 对比分离以解决共存问题。 CoSA 展示了卓越的性能，在 VOC 和 COCO 验证数据集上分别实现了 76.2% 和 51.0% 的 mIoU，大幅超出了现有基线。值得注意的是，CoSA 是第一个优于所有现有多阶段方法（包括具有额外监督的方法）的单阶段方法。代码可在 \url{https://github.com/youshyee/CoSA} 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.17891</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>装箱绑定：T2I 扩散模型中的统一布局控制和属性绑定</title>
      <link>https://arxiv.org/abs/2402.17910</link>
      <description><![CDATA[arXiv:2402.17910v1 公告类型：新
摘要：虽然潜在扩散模型（LDM）擅长创建富有想象力的图像，但它们通常在语义保真度和对对象生成位置的空间控制方面缺乏精度。为了解决这些缺陷，我们引入了 Box-it-to-Bind-it (B2B) 模块——一种新颖的、免训练的方法，用于提高文本到图像 (T2I) 扩散模型的空间控制和语义准确性。 B2B 针对 T2I 中的三个关键挑战：灾难性忽视、属性绑定和布局指导。该过程包含两个主要步骤：i) 对象生成，调整潜在编码以保证对象生成并将其引导到指定的边界框中；ii) 属性绑定，保证生成的对象遵循提示中指定的属性。 B2B被设计为现有T2I模型的兼容即插即用模块，显着增强了解决关键挑战的模型性能。我们使用既定的 CompBench 和 TIFA 评分基准来评估我们的技术，与现有方法相比，表现出显着的性能改进。源代码将在 https://github.com/nextaistudio/BoxIt2BindIt 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2402.17910</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>REPrune：通过内核代表选择进行通道修剪</title>
      <link>https://arxiv.org/abs/2402.17862</link>
      <description><![CDATA[arXiv:2402.17862v1 公告类型：新
摘要：通道修剪被广泛接受以加速现代卷积神经网络（CNN）。由此产生的修剪模型受益于其在通用软件和硬件资源上的立即部署。然而，由于决定如何以及在何处向 CNN 引入稀疏性的不灵活性，其较大的剪枝粒度（特别是在卷积滤波器的单元上）通常会导致不期望的精度下降。在本文中，我们提出了 REPrune，这是一种模拟内核剪枝的新型通道剪枝技术，充分利用更精细但结构化的粒度。 REPrune 使用凝聚聚类来识别每个通道内的相似内核。然后，它选择最大化内核代表合并的过滤器，同时优化最大簇覆盖问题。通过与同时训练-剪枝范式集成，REPrune 在整个训练 CNN 中促进高效、渐进的剪枝，避免了传统的训练-剪枝-微调序列。实验结果表明，REPrune 在计算机视觉任务中比现有方法表现更好，有效地实现了加速比和性能保持之间的平衡。]]></description>
      <guid>https://arxiv.org/abs/2402.17862</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>具有自然语言语义的视觉转换器</title>
      <link>https://arxiv.org/abs/2402.17863</link>
      <description><![CDATA[arXiv:2402.17863v1 公告类型：新
摘要：与自然语言处理（NLP）中的对应物不同，视觉变换器（ViT）中的令牌或补丁缺乏必要的语义信息。通常，ViT 标记与缺乏特定语义上下文的矩形图像块相关联，这使得解释变得困难并且无法有效封装信息。我们引入了一种新颖的变压器模型，语义视觉变压器（sViT），它利用分割模型的最新进展来设计新颖的分词器策略。 sViT 有效地利用语义信息，创建让人想起卷积神经网络的归纳偏差，同时捕获作为 Transformer 特征的图像中的全局依赖性和上下文信息。通过使用真实数据集进行验证，sViT 展示了相对 ViT 的优越性，需要更少的训练数据，同时保持相似或优越的性能。此外，由于其尺度不变性语义特征，sViT 在分布外泛化和对自然分布变化的鲁棒性方面表现出显着的优越性。值得注意的是，语义标记的使用显着增强了模型的可解释性。最后，所提出的范例有助于在令牌（或段）级别引入新的强大的增强技术，从而增加训练数据的多样性和泛化能力。正如句子是由单词组成的一样，图像是由语义对象形成的。我们提出的方法利用了对象分割方面的最新进展，并朝着可解释和强大的视觉转换器迈出了重要而自然的一步。]]></description>
      <guid>https://arxiv.org/abs/2402.17863</guid>
      <pubDate>Thu, 29 Feb 2024 06:17:21 GMT</pubDate>
    </item>
    </channel>
</rss>