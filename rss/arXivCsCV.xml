<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 01 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>基于实时卷积神经网络的CubeSat星跟踪器星检测与质心方法</title>
      <link>https://arxiv.org/abs/2404.19108</link>
      <description><![CDATA[arXiv:2404.19108v1 公告类型：新
摘要：星跟踪器是用于绝对姿态确定的最精确的天体传感器之一。该设备检测捕获图像中的恒星，并以亚像素精度准确计算其在成像焦平面上的投影质心。传统的星体检测和质心算法通常依赖于星体像素检测的阈值调整和质心计算的像素亮度加权。然而，高传感器噪声和杂散光等挑战可能会影响算法性能。本文介绍了一种基于卷积神经网络 (CNN) 的恒星检测和质心方法，专门用于解决存在杂散光和其他伪影的情况下恒星跟踪器图像噪声带来的问题。 CNN 使用覆盖有真实传感器噪声和杂散光的模拟恒星图像进行训练，生成区分恒星像素与背景的二进制分割图和指示每个像素与最近恒星质心的接近程度的距离图。利用该距离信息和像素坐标将质心计算转换为一组可通过最小二乘法解决的三边测量问题。我们的方法采用高效的 UNet 变体作为底层 CNN 架构，并评估变体的性能。通过合成图像评估、硬件在环评估和夜空测试进行了全面的测试。测试一致表明，我们的方法在质心精度方面优于多种现有算法，并且对高传感器噪声和杂散光干扰表现出卓越的恢复能力。我们算法的另一个好处是它们可以在低功耗边缘人工智能处理器上实时执行。]]></description>
      <guid>https://arxiv.org/abs/2404.19108</guid>
      <pubDate>Wed, 01 May 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>EMOPortraits：情感增强的多模式一次性头部头像</title>
      <link>https://arxiv.org/abs/2404.19110</link>
      <description><![CDATA[arXiv:2404.19110v1 公告类型：新
摘要：由视觉信号动画的头部头像已经很受欢迎，特别是在驾驶员与动画角色不同的交叉驾驶合成中，这是一种具有挑战性但非常实用的方法。最近推出的 MegaPortraits 模型展示了该领域最先进的结果。我们对该模型进行了深入的检查和评估，特别关注其面部表情描述符的潜在空间，并揭示了其表达强烈面部运动的能力的一些局限性。为了解决这些限制，我们建议对训练流程和模型架构进行重大更改，以引入我们的 EMOPortraits 模型，其中我们：
  增强模型忠实支持强烈、不对称面部表情的能力，在情感转移任务中创造了新的最先进结果，在指标和质量上都超越了以前的方法。
  将语音驱动模式纳入我们的模型中，在音频驱动的面部动画中实现顶级性能，从而可以通过多种方式（包括视觉信号、音频或两者的混合）驱动源身份。
  我们提出了一种新颖的多视图视频数据集，具有广泛的强烈和不对称的面部表情，填补了现有数据集中缺乏此类数据的空白。]]></description>
      <guid>https://arxiv.org/abs/2404.19110</guid>
      <pubDate>Wed, 01 May 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>彻底改变交通标志识别：揭示视觉变压器的潜力</title>
      <link>https://arxiv.org/abs/2404.19066</link>
      <description><![CDATA[arXiv:2404.19066v1 公告类型：新
摘要：本研究介绍了一种利用深度学习技术的交通标志识别（TSR）创新方法，特别是视觉变压器。 TSR 在推进驾驶员辅助系统和自动驾驶汽车方面发挥着至关重要的作用。传统的 TSR 方法依赖于手动特征提取，已被证明是劳动密集型且成本高昂的。此外，基于形状和颜色的方法具有固有的局限性，包括对各种因素和光照条件变化的敏感性。本研究探索了 Vision Transformers 的三种变体（PVT、TNT、LNL）和六种卷积神经网络（AlexNet、ResNet、VGG16、MobileNet、EfficientNet、GoogleNet）作为基线模型。为了解决传统方法的缺点，提出了一种新型金字塔 EATFormer 主干，将进化算法 (EA) 与 Transformer 架构相结合。引入的基于 EA 的 Transformer 块通过其组件捕获多尺度、交互式和个体信息：前馈网络、全局和局部交互以及多尺度区域聚合模块。此外，还引入了调制变形 MSA 模块来动态模拟不规则位置。对 GTSRB 和 BelgianTS 数据集的实验评估证明了所提出的方法在提高预测速度和准确性方面的有效性。这项研究得出的结论是，Vision Transformers 在交通标志分类方面具有重大前景，并为 TSR 提供​​了新的算法框架。这些发现为开发精确可靠的 TSR 算法奠定了基础，使驾驶员辅助系统和自动驾驶汽车受益。]]></description>
      <guid>https://arxiv.org/abs/2404.19066</guid>
      <pubDate>Wed, 01 May 2024 06:19:46 GMT</pubDate>
    </item>
    <item>
      <title>GSTalker：通过可变形高斯泼溅生成实时音频驱动的说话人脸</title>
      <link>https://arxiv.org/abs/2404.19040</link>
      <description><![CDATA[arXiv:2404.19040v1 公告类型：新
摘要：我们提出了 GStalker，一种 3D 音频驱动的说话人脸生成模型，采用高斯 Splatting 技术，可用于快速训练（40 分钟）和实时渲染（125 FPS），并使用 3$\sim$5 分钟的视频作为训练材料，相比之下之前基于 NeRF 的 2D 和 3D 建模框架需要数小时的训练和数秒的每帧渲染。具体来说，GSTalker学习音频驱动的高斯变形场来平移和变换3D高斯以与音频信息同步，其中结合了基于多分辨率哈希网格的三平面和时间平滑模块来学习细粒度面部的精确变形细节。此外，还设计了姿势条件变形场来模拟稳定的躯干。为了有效优化条件高斯变形场，我们通过学习粗略静态高斯表示来初始化 3D 高斯。在带有音轨的特定人物视频中进行的大量实验验证了 GSTalker 可以通过快速训练和实时渲染速度生成高保真和音频唇形同步的结果。]]></description>
      <guid>https://arxiv.org/abs/2404.19040</guid>
      <pubDate>Wed, 01 May 2024 06:19:45 GMT</pubDate>
    </item>
    <item>
      <title>使用多光谱卫星图像通过类模糊度指数提高洪水淹没绘图的深度主动学习的可解释性</title>
      <link>https://arxiv.org/abs/2404.19043</link>
      <description><![CDATA[arXiv:2404.19043v1 公告类型：新
摘要：洪水淹没测绘是应对与全球变暖相关的日益增加的洪水风险的一项关键任务。近年来深度学习的重大进步引发了其广泛的应用，包括洪水淹没绘图。为了应对监督学习中耗时且费力的数据标记过程，深度主动学习策略是可行的方法之一。然而，对深度主动学习策略如何运作的可解释性的探索仍然有限，特别关注遥感领域的洪水淹没绘图。在这项研究中，我们介绍了一种用于洪水淹没绘图的可解释深度主动学习（IDAL-FIM）的新颖框架，特别是在多光谱卫星图像的类别模糊性方面。在实验中，我们使用Sen1Floods11数据集，并采用带有MC-dropout的U-Net。此外，我们还采用了五种采集函数，分别是随机、K-means、BALD、熵和边际采集函数。基于实验结果，我们证明了两个提出的类别模糊指数是通过与深度学习模型在图块级别的预测不确定性建立统计显着相关性来解释深度主动学习的有效变量。然后，我们通过可视化二维密度图来说明深度主动学习的行为，并提供有关深度主动学习在洪水淹没绘图中的操作的解释。]]></description>
      <guid>https://arxiv.org/abs/2404.19043</guid>
      <pubDate>Wed, 01 May 2024 06:19:45 GMT</pubDate>
    </item>
    <item>
      <title>文档分类的机器遗忘</title>
      <link>https://arxiv.org/abs/2404.19031</link>
      <description><![CDATA[arXiv:2404.19031v1 公告类型：新
摘要：文档理解模型最近通过利用广泛的用户文档集合表现出了卓越的性能。然而，由于文档通常包含大量个人数据，它们的使用可能会对用户隐私构成威胁，并削弱人类与人工智能服务之间的信任纽带。针对这些担忧，最近提出了提倡“被遗忘权”的立法，允许用户请求从计算机系统和神经网络模型中删除私人信息。一种被称为机器遗忘的新方法已经出现使人工智能模型忘记特定类别的数据 在我们的研究中，我们探索了文档分类问题的机器学习，据我们所知，这是对该领域的首次调查。具体来说，我们考虑了一个远程的现实场景。服务器拥有经过良好训练的模型，并且仅拥有一小部分训练数据。这项工作是为解决文档分析应用程序中的隐私问题而开发的机器遗忘方法迈出的开创性一步。代码可在 \url{https://github.com/leitro/MachineUnlearning-DocClassification} 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.19031</guid>
      <pubDate>Wed, 01 May 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>用于动画风格视频肖像的嵌入式表征学习网络</title>
      <link>https://arxiv.org/abs/2404.19038</link>
      <description><![CDATA[arXiv:2404.19038v1 公告类型：新
摘要：说话头像一代因其广泛的应用前景，特别是数字化身和3D动画设计，最近引起了广泛的关注。受这一实际​​需求的启发，一些作品探索了神经辐射场（NeRF）来合成说话的头像。然而，这些基于 NeRF 的方法面临两个挑战：（1）难以生成风格可控的头部说话。 (2) 渲染图像中颈部周围的位移伪影。为了克服这两个挑战，我们提出了一种具有两个学习阶段的新颖的生成范式 \textit{Embedded Representation Learning Network} (ERLNet)。首先，构建 \textit{ 音频驱动的 FLAME} (ADF) 模块来生成与内容音频和风格视频同步的面部表情和头部姿势序列。其次，给定 ADF 推导的序列，一种新颖的 \textit{双分支融合 NeRF} (DBF-NeRF) 探索这些内容以渲染最终图像。大量的实证研究表明，这两个阶段的协作有效地促进了我们的方法呈现比现有算法更真实的头部说话。]]></description>
      <guid>https://arxiv.org/abs/2404.19038</guid>
      <pubDate>Wed, 01 May 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>使用自注意力评分机制的多页文档视觉问答</title>
      <link>https://arxiv.org/abs/2404.19024</link>
      <description><![CDATA[arXiv:2404.19024v1 公告类型：新
摘要：文档是书面交流的二维载体，因此对文档的解释需要采用多模态方法，将文本和视觉信息有效结合起来。由于这种多模态特性，文档视觉问答 (Document VQA) 引起了文档理解和自然语言处理社区的极大兴趣。最先进的单页文档 VQA 方法表现出令人印象深刻的性能，但在多页场景中，这些方法却举步维艰。他们必须将所有页面连接成一个大页面进行处理，这需要大量的 GPU 资源，即使是评估也是如此。在这项工作中，我们提出了一种用于多页文档 VQA 任务的新方法和有效的训练策略。特别是，我们采用纯视觉文档表示，利用文档理解模型 Pix2Struct 中的编码器。我们的方法利用自注意力评分机制为每个文档页面生成相关性分数，从而实现相关页面的检索。通过这种调整，我们可以将单页文档 VQA 模型扩展到多页场景，而无需在评估期间限制页数，而且对 GPU 资源的需求极小。我们进行了大量的实验，证明它不仅可以在无需光学字符识别 (OCR) 的情况下实现最先进的性能，而且在扩展到近 800 页文档的场景中也能保持良好的性能，而 MP-DocVQA 数据集中最多只有 20 页。我们的代码可在 \url{https://github.com/leitro/SelfAttnScoring-MPDocVQA} 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.19024</guid>
      <pubDate>Wed, 01 May 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>MeGA：用于高保真渲染和头部编辑的混合网格高斯头部头像</title>
      <link>https://arxiv.org/abs/2404.19026</link>
      <description><![CDATA[arXiv:2404.19026v1 公告类型：新
摘要：从多视角视频创建高保真头部头像是许多 AR/VR 应用的核心问题。然而，现有的方法通常难以同时获得所有不同头部组件的高质量渲染，因为它们使用单​​一表示来对具有截然不同特征的组件（例如皮肤与头发）进行建模。在本文中，我们提出了一种混合网格高斯头部头像 (MeGA)，它使用更合适的表示来对不同的头部组件进行建模。具体来说，我们选择增强的 FLAME 网格作为我们的面部表示，并预测 UV 位移图以提供每个顶点的偏移，以改进个性化的几何细节。为了实现逼真的渲染，我们使用延迟神经渲染获得面部颜色并将神经纹理分解为三个有意义的部分。对于头发建模，我们首先使用 3D 高斯溅射构建静态规范头发。进一步应用刚性变换和基于 MLP 的变形场来处理复杂的动态表达。结合我们的遮挡感知混合，MeGA 可以为整个头部生成更高保真度的渲染，并自然支持更多下游任务。在 NeRSemble 数据集上进行的实验证明了我们设计的有效性，其性能优于之前最先进的方法，并支持各种编辑功能，包括发型更改和纹理编辑。]]></description>
      <guid>https://arxiv.org/abs/2404.19026</guid>
      <pubDate>Wed, 01 May 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>Simple-RF：使用更简单的解决方案对稀疏输入辐射场进行正则化</title>
      <link>https://arxiv.org/abs/2404.19015</link>
      <description><![CDATA[arXiv:2404.19015v1 公告类型：新
摘要：神经辐射场（NeRF）在逼真的自由视图场景渲染中表现出令人印象深刻的性能。与采用隐式表示的 NeRF 相比，TensoRF 和 ZipNeRF 等 NeRF 的最新改进采用显式模型来实现更快的优化和渲染。然而，隐式和显式辐射场都需要对给定场景中的图像进行密集采样。当只有一组稀疏的视图可用时，它们的性能会显着下降。研究人员发现，监督辐射场估计的深度有助于以更少的视图有效地训练它。深度监督是使用经典方法或在大型数据集上预训练的神经网络获得的。虽然前者可能只提供稀疏的监督，但后者可能会遇到泛化问题。与早期的方法相反，我们寻求通过设计增强模型并将其与主辐射场一起训练来学习深度监督。此外，我们的目标是设计一个可以跨不同隐式和显式辐射场工作的正则化框架。我们观察到这些辐射场模型的某些特征与稀疏输入场景中观察到的图像过度拟合。我们的主要发现是，降低辐射场在位置编码、分解张量分量的数量或哈希表大小方面的能力，会限制模型学习更简单的解决方案，从而在某些区域估计更好的深度。通过设计基于这种降低的能力的增强模型，我们获得了对主辐射场更好的深度监督。通过采用上述正则化，我们在包含前向和 360$^\circ$ 场景的流行数据集上通过稀疏输入视图实现了最先进的视图合成性能。]]></description>
      <guid>https://arxiv.org/abs/2404.19015</guid>
      <pubDate>Wed, 01 May 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>流程中有什么？利用时间运动线索进行无监督通用事件边界检测</title>
      <link>https://arxiv.org/abs/2404.18935</link>
      <description><![CDATA[arXiv:2404.18935v1 公告类型：新
摘要：通用事件边界检测（GEBD）任务旨在识别通用的、无分类的边界，将视频分割成有意义的事件。当前的方法通常涉及在大量数据上训练的神经模型，需要大量的计算能力和存储空间。我们探讨了与 GEBD 相关的两个关键问题：非参数算法能否优于无监督神经方法？仅运动信息就足以实现高性能吗？这项调查促使我们通过算法利用运动线索来识别视频中的通用事件边界。在这项工作中，我们提出了 FlowGEBD，一种非参数、无监督的 GEBD 技术。我们的方法需要两种利用光流的算法：（i）像素跟踪和（ii）流标准化。通过对具有挑战性的 Kinetics-GEBD 和 TAPOS 数据集进行彻底的实验，我们的结果将 FlowGEBD 确立为无监督方法中最先进的 (SOTA)。 FlowGEBD 超越了 Kinetics-GEBD 数据集上的神经模型，获得了 0.713 的 F1@0.05 分数，与无监督基线相比绝对增益为 31.7%，并且在 TAPOS 验证数据集上实现了 0.623 的平均 F1 分数。]]></description>
      <guid>https://arxiv.org/abs/2404.18935</guid>
      <pubDate>Wed, 01 May 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>CUE-Net：具有空间裁剪、增强型 UniformerV2 和改进的高效附加注意力的暴力检测视频分析</title>
      <link>https://arxiv.org/abs/2404.18952</link>
      <description><![CDATA[arXiv:2404.18952v1 公告类型：新
摘要：在本文中，我们介绍了 CUE-Net，这是一种专为视频监控中自动暴力检测而设计的新颖架构。随着技术进步和成本下降，监控系统变得越来越普遍，有效监控大量视频数据的挑战也随之加剧。 CUE-Net 通过将空间裁剪与 UniformerV2 架构的增强版本相结合，将卷积和自注意力机制与新颖的改进型高效附加注意力机制（可降低自注意力的二次时间复杂度）集成在一起，从而有效且高效地解决了这一挑战。识别暴力活动。这种方法旨在克服传统挑战，例如捕捉视频帧内远处或部分模糊的主体。通过关注局部和全局时空特征，CUE-Net 在 RWF-2000 和 RLVS 数据集上实现了最先进的性能，超越了现有方法。]]></description>
      <guid>https://arxiv.org/abs/2404.18952</guid>
      <pubDate>Wed, 01 May 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>用于解决数据异构性的无聚合联邦学习</title>
      <link>https://arxiv.org/abs/2404.18962</link>
      <description><![CDATA[arXiv:2404.18962v1 公告类型：新
摘要：联邦学习（FL）的性能取决于利用分布式数据集知识的有效性。传统的 FL 方法采用聚合然后适应的框架，其中客户端根据服务器在上一轮训练中聚合的全局模型来更新本地模型。此过程可能会导致客户端漂移，尤其是在跨客户端数据异质性显着的情况下，影响模型性能和 FL 算法的收敛性。为了应对这些挑战，我们引入了 FedAF，一种新颖的无聚合 FL 算法。在此框架中，客户端通过利用同行知识协作学习压缩数据，服务器随后使用从客户端接收到的压缩数据和软标签来训练全局模型。 FedAF本质上避免了客户端漂移问题，在数据异构性显着的情况下提高了压缩数据的质量，并提高了全局模型性能。对几个流行基准数据集的广泛数值研究表明，FedAF 在处理标签偏差和特征偏差数据异质性方面超越了各种最先进的 FL 算法，从而实现了卓越的全局模型精度和更快的收敛速度。]]></description>
      <guid>https://arxiv.org/abs/2404.18962</guid>
      <pubDate>Wed, 01 May 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>视觉体验数据集：超过 200 小时记录的集成眼动、里程计和自我中心视频</title>
      <link>https://arxiv.org/abs/2404.18934</link>
      <description><![CDATA[arXiv:2404.18934v1 公告类型：新
摘要：我们介绍了视觉体验数据集 (VEDB)，它是超过 240 小时的以自我为中心的视频的汇编，结合了注视和头部跟踪数据，提供了人类观察者所体验到的视觉世界的前所未有的视图。该数据集包含 717 个会话，由 58 名 6 至 49 岁的观察者记录。本文概述了为确保样本具有代表性而采取的数据收集、处理和标记协议，并讨论了数据集中潜在的错误或偏差来源。 VEDB 的潜在应用非常广泛，包括改进注视跟踪方法、评估时空图像统计数据以及完善用于场景和活动识别的深度神经网络。 VEDB 可通过已建立的开放科学平台访问，旨在成为一个包含扩展和社区贡献计划的动态数据集。它的发布强调道德考虑，例如参与者隐私和减少潜在偏见。通过提供基于现实世界经验的数据集，并附有广泛的元数据和支持代码，作者邀请研究界利用 VEDB 并为 VEDB 做出贡献，以促进对自然环境中的视觉感知和行为的更丰富的理解。]]></description>
      <guid>https://arxiv.org/abs/2404.18934</guid>
      <pubDate>Wed, 01 May 2024 06:19:40 GMT</pubDate>
    </item>
    <item>
      <title>学习胸部疾病分类的低阶特征</title>
      <link>https://arxiv.org/abs/2404.18933</link>
      <description><![CDATA[arXiv:2404.18933v1 公告类型：新
摘要：深度神经网络，包括卷积神经网络（CNN）和视觉变换器（ViT），在医学图像领域取得了惊人的成功。我们在本文中研究了胸部疾病的分类。有效提取疾病区域的特征对于放射图像的疾病分类至关重要。虽然各种神经架构和训练技术，例如具有对比/恢复学习的自监督学习，已被用于放射线图像的疾病分类，但没有原则性的方法可以有效减少噪声和背景或非噪声的不利影响。疾病领域，对放射线图像进行疾病分类。为了应对这一挑战，我们在本文中提出了一种新颖的低秩特征学习（LRFL）方法，该方法普遍适用于所有神经网络的训练。 LRFL 方法既在经验上受到本文所有医学数据集上观察到的低频特性的启发，又在理论上受到我们对具有低秩特征的神经网络的尖锐泛化界限的启发。在实证研究中，使用 ViT 或 CNN 等神经网络，通过 Masked Autoencoders (MAE) 对未标记的胸部 X 射线进行预训练，我们的新颖 LRFL 方法应用于预训练的神经网络，并展示了更好的分类结果就接收者操作曲线下的多类面积 (mAUC) 和分类准确性而言。]]></description>
      <guid>https://arxiv.org/abs/2404.18933</guid>
      <pubDate>Wed, 01 May 2024 06:19:39 GMT</pubDate>
    </item>
    </channel>
</rss>