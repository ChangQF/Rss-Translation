<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 11 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>CosmoCLIP：推广大型视觉语言模型用于天文成像</title>
      <link>https://arxiv.org/abs/2407.07315</link>
      <description><![CDATA[arXiv:2407.07315v1 公告类型：新
摘要：现有的视觉文本对比学习模型通过匹配成对的图像和标题嵌入同时将不相关的对分开来增强表示可转移性并支持零样本预测。然而，与互联网上可用的一般图像和标签数据集相比，天文图像标签数据集要小得多。我们引入了 CosmoCLIP，这是一个天文图像文本对比学习框架，使用 SpaceNet 和基于 BLIP 的字幕在预训练的 CLIP 模型上进行了精确微调。通过 FLARE 获得的 SpaceNet 包含约 13k 个最佳分布的图像，而 BLIP 充当丰富的知识提取器。从这个 SpaceNet 和 BLIP 描述中得出的丰富语义，在进行对比学习时，使 CosmoCLIP 能够在各种域内和域外任务中实现卓越的泛化。我们的结果表明，CosmoCLIP 是一个简单但功能强大的框架，在零样本分类和图像文本检索任务中表现明显优于 CLIP。]]></description>
      <guid>https://arxiv.org/abs/2407.07315</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>用于运动红外弱小目标检测的可变形特征对齐与细化</title>
      <link>https://arxiv.org/abs/2407.07289</link>
      <description><![CDATA[arXiv:2407.07289v1 公告类型：新
摘要：运动红外弱小目标的检测一直是一个具有挑战性和普遍性的研究课题。当前最先进的方法主要基于ConvLSTM来聚合来自相邻帧的信息以方便检测当前帧。然而，这些方法仅在训练阶段隐式地利用运动信息，而未能显式地探索运动补偿，导致在包含大运动的视频序列的情况下性能不佳。在本文中，我们提出了一种基于可变形卷积的可变形特征对齐和细化（DFAR）方法，以在训练和推理阶段显式地使用运动上下文。具体而言，基于设计的扩张卷积注意融合（DCAF）块开发了一个时间可变形对齐（TDA）模块，以在特征级别显式地将相邻帧与当前帧对齐。然后，特征细化模块自适应地融合对齐的特征，并通过所提出的注意力引导可变形融合 (AGDF) 块进一步聚合有用的时空信息。此外，为了改善相邻帧与当前帧的对齐，我们通过引入新的运动补偿损失来扩展传统的损失函数。大量实验结果表明，所提出的 DFAR 方法在 DAUB 和 IRDST 两个基准数据集上实现了最佳性能。]]></description>
      <guid>https://arxiv.org/abs/2407.07289</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>具有光谱超级标记的双阶段高光谱图像分类模型</title>
      <link>https://arxiv.org/abs/2407.07307</link>
      <description><![CDATA[arXiv:2407.07307v1 公告类型：新
摘要：高光谱图像分类是一项将预定义类别分配给遥感场景高光谱图像中每个像素的任务，由于忽略了光谱相似像素之间的相关性，它经常面临挑战。这种疏忽可能导致边缘定义不准确，并难以管理相邻区域中的微小光谱变化。为了解决这些问题，我们引入了受超像素概念启发的新型双级光谱超级标记分类器 (DSTC)。DSTC 采用基于光谱导数的像素聚类将具有相似光谱特征的像素分组为光谱超级标记。通过将这些标记的分类投影到图像空间上，我们获得了像素级结果，从而保持了区域分类一致性和精确边界。此外，认识到标记内的多样性，我们提出了一种基于类比例的软标签。该标签根据不同类别的普遍性自适应地为其分配权重，从而有效地管理数据分布不平衡并提高分类性能。在 WHU-OHS、IP、KSC 和 UP 数据集上进行的综合实验证实了 DSTC 的强大分类能力及其各个组件的有效性。代码将在 https://github.com/laprf/DSTC 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2407.07307</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>探索自动驾驶感知的摄像头编码器设计</title>
      <link>https://arxiv.org/abs/2407.07276</link>
      <description><![CDATA[arXiv:2407.07276v1 公告类型：新
摘要：自动驾驶汽车 (AV) 的基石是坚实的感知系统，其中摄像头编码器起着至关重要的作用。现有工作通常利用预先训练的卷积神经网络 (CNN) 或视觉变换器 (ViT)，用于一般视觉任务，例如图像分类、分割和 2D 检测。尽管这些众所周知的架构已经在 AV 相关任务（例如 3D 对象检测）中实现了最先进的精度，但由于工业级 AV 数据集的细微复杂性，网络设计仍有很大改进潜力。此外，现有的公共 AV 基准通常包含的数据不足，这可能导致对这些架构的评估不准确。为了揭示特定于 AV 的模型见解，我们从标准通用编码器 ConvNeXt 开始，并逐步改造设计。我们调整了不同的设计参数，包括模型的宽度和深度、阶段计算比率、注意机制和输入分辨率，并对每个修改进行了系统分析。这种定制产生了针对 AV 摄像头编码器优化的架构，与基线相比，mAP 提高了 8.79%。我们相信我们的努力可以成为 AV 图像编码器的甜蜜秘诀，并为下一级驱动系统铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2407.07276</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>MIGS：通过张量分解实现多身份高斯分布</title>
      <link>https://arxiv.org/abs/2407.07284</link>
      <description><![CDATA[arXiv:2407.07284v1 公告类型：新
摘要：我们引入了 MIGS（多身份高斯溅射），这是一种仅使用单目视频即可为多个身份学习单个神经表征的新方法。最近针对人类化身的 3D 高斯溅射 (3DGS) 方法需要对每个身份进行优化。然而，学习多身份表征在任意姿势下稳健地为人类制作动画方面具有优势。我们建议构建一个高阶张量，将所有可学习的 3DGS 参数组合到所有训练身份中。通过假设低秩结构并分解张量，我们在统一网络中对多个主体的复杂刚性和非刚性变形进行建模，从而显着减少参数总数。我们提出的方法利用来自所有训练身份的信息，在具有挑战性的看不见的姿势下实现稳健的动画，优于现有方法。我们还展示了如何将其扩展以学习看不见的身份。]]></description>
      <guid>https://arxiv.org/abs/2407.07284</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>通过条件松弛扩散反演生成少量样本图像</title>
      <link>https://arxiv.org/abs/2407.07249</link>
      <description><![CDATA[arXiv:2407.07249v1 公告类型：新 
摘要：在使用深度生成模型 (DGM) 的少样本图像生成 (FSIG) 领域，准确估计具有最少样本的目标域分布是一项重大挑战。这需要一种既能捕捉广泛多样性又能捕捉目标域分布真实特征的方法。我们提出了条件放松扩散反演 (CRDI)，这是一种创新的“免训练”方法，旨在增强合成图像生成中的分布多样性。与传统方法不同，CRDI 不依赖于仅基于少数样本的微调。相反，它专注于重建每个目标图像实例并通过少样本学习扩大多样性。该方法首先为扩散模型识别一个样本引导嵌入 (SGE)，其目的类似于某些生成对抗网络 (GAN) 模型中的显式潜在代码。随后，该方法涉及一个调度程序，该调度程序逐步向 SGE 引入扰动，从而增强多样性。综合实验表明，我们的方法超越了基于 GAN 的重建技术，并且在性能上与最先进的 (SOTA) FSIG 方法相当。此外，它有效地缓解了过度拟合和灾难性遗忘，这是微调方法的常见缺点。]]></description>
      <guid>https://arxiv.org/abs/2407.07249</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>基于主动学习的自适应采样数据集量化</title>
      <link>https://arxiv.org/abs/2407.07268</link>
      <description><![CDATA[arXiv:2407.07268v1 公告类型：新
摘要：深度学习最近取得了显著进展，这在很大程度上要归功于大量标记良好的数据集。然而，对此类数据集进行训练会增加成本和计算需求。为了解决这个问题，文献中探索了各种技术，如核心集选择、数据集提炼和数据集量化。与依赖于不同类别之间均匀样本分布的传统技术不同，我们的研究表明，即使分布不均匀，也能保持性能。我们发现，对于某些类别，样本数量的变化对性能的影响很小。受此观察的启发，一个直观的想法是减少稳定类的样本数量，增加敏感类的样本数量，以在相同采样率下实现更好的性能。那么问题就来了：我们如何自适应地从数据集中选择样本以实现最佳性能？在本文中，我们提出了一种基于主动学习的新型自适应采样策略，即基于主动学习的自适应采样数据集量化 (DQAS)，以优化样本选择。此外，我们引入了一种新颖的数据集量化流程，利用数据集量化最后阶段的特征空间来生成更精确的数据集箱。我们对多个数据集的全面评估表明，我们的方法优于最先进的数据集压缩方法。]]></description>
      <guid>https://arxiv.org/abs/2407.07268</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>基于参考的高斯溅射可控场景风格化</title>
      <link>https://arxiv.org/abs/2407.07220</link>
      <description><![CDATA[arXiv:2407.07220v1 公告类型：新
摘要：基于内容对齐的参考图像编辑外观的基于参考的场景风格化是一个新兴的研究领域。从预训练的神经辐射场 (NeRF) 开始，现有方法通常会学习与给定风格相匹配的新外观。尽管它们很有效，但它们本质上存在耗时的体积渲染问题，因此对于许多实时应用来说并不实用。在这项工作中，我们提出了 ReGS，它采用 3D 高斯溅射 (3DGS) 进行基于参考的风格化，以实现实时风格化视图合成。编辑预训练的 3DGS 的外观具有挑战性，因为它使用离散高斯作为 3D 表示，将外观与几何紧密结合。像以前的方法那样简单地优化外观通常不足以对给定的参考图像中的连续纹理进行建模。为了应对这一挑战，我们提出了一种新颖的纹理引导控制机制，该机制可以自适应地将局部响应高斯函数调整为新的几何排列，从而提供所需的纹理细节。所提出的流程由纹理线索引导，以实现有效的外观编辑，并由场景深度进行正则化，以保留原始几何结构。通过这些新颖的设计，我们展示了 ReG 可以产生尊重参考纹理的最先进的风格化结果，同时实现自由视图导航的实时渲染速度。]]></description>
      <guid>https://arxiv.org/abs/2407.07220</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:33 GMT</pubDate>
    </item>
    <item>
      <title>投毒攻击中的恶意客户端溯源至联邦学习</title>
      <link>https://arxiv.org/abs/2407.07221</link>
      <description><![CDATA[arXiv:2407.07221v1 公告类型：新
摘要：投毒攻击会损害联邦学习 (FL) 的训练阶段，使得学习到的全局模型对攻击者选择的输入（称为目标输入）进行错误分类。现有的防御措施主要侧重于保护 FL 的训练阶段，以使学习到的全局模型不受毒害。然而，当客户端的本地训练数据高度非独立同分布或恶意客户端数量很大时，这些防御措施的效果通常有限，这在我们的实验中得到了证实。在这项工作中，我们提出了 FLForensics，这是 FL 的第一个毒药取证方法。FLForensics 是对现有训练阶段防御措施的补充。特别是，当训练阶段防御措施失败并且部署了中毒的全局模型时，FLForensics 旨在在识别出错误分类的目标输入后追溯执行投毒攻击的恶意客户端。我们从理论上表明，在投毒攻击的正式定义下，FLForensics 可以准确区分良性和恶意客户端。此外，我们通过实证研究证明了 FLForensics 在五个基准数据集上追溯现有和自适应中毒攻击的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.07221</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:33 GMT</pubDate>
    </item>
    <item>
      <title>通过任务向量定制扩大个性化美学评估</title>
      <link>https://arxiv.org/abs/2407.07176</link>
      <description><![CDATA[arXiv:2407.07176v1 公告类型：新
摘要：个性化图像美学评估任务旨在通过用户提供的少量输入来定制美学分数预测模型，以匹配个人偏好。然而，当前方法的可扩展性和泛化能力受到对昂贵的精选数据库的依赖的极大限制。为了克服这一长期存在的可扩展性挑战，我们提出了一种独特的方法，利用现成的数据库进行一般图像美学评估和图像质量评估。具体来说，我们将每个数据库视为一个独特的图像分数回归任务，表现出不同程度的个性化潜力。通过确定已知代表每个数据库特定特征的任务向量的最佳组合，我们成功地为个人创建了个性化模型。这种集成多个模型的方法使我们能够利用大量数据。我们进行了广泛的实验，证明了我们的方法在推广到以前看不见的领域的有效性——这是以前的方法难以实现的挑战——使其高度适用于现实世界场景。我们的新方法为个性化美学评估提供了可扩展的解决方案，并为未来的研究建立了高标准，显著推动了该领域的发展。https://yeolj00.github.io/personal-projects/personalized-aesthetics/]]></description>
      <guid>https://arxiv.org/abs/2407.07176</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:32 GMT</pubDate>
    </item>
    <item>
      <title>检测几乎不可见的表面裂纹，确保风力涡轮机的可持续性</title>
      <link>https://arxiv.org/abs/2407.07186</link>
      <description><![CDATA[arXiv:2407.07186v1 公告类型：新
摘要：风能的生产是可持续发展和减少对化石燃料依赖的关键部分。保持风力涡轮机的完整性以生产这种能源是一项昂贵且耗时的任务，需要反复检查和维护。虽然自主无人机已被证明可以使这个过程更有效率，但由于一些危险的缺陷（例如几乎看不见的细微裂纹），用于检测异常以防止涡轮叶片发生灾难性损坏的算法已经落后。现有的数据集和文献缺乏，并且倾向于检测明显和可见的缺陷，而且地理分布不广泛。在本文中，我们介绍了一个新颖而多样化的数据集，该数据集是从大量风力涡轮机检查中收集的几乎看不见的细微裂纹。为了证明我们数据集的有效性，我们详细介绍了从图像采集阶段到使用预测提供自动维护建议以延长风力涡轮机的寿命和效率的端到端部署的涡轮机裂纹检测流程。]]></description>
      <guid>https://arxiv.org/abs/2407.07186</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:32 GMT</pubDate>
    </item>
    <item>
      <title>ColorPeel：通过颜色和形状解缠，利用扩散模型进行颜色提示学习</title>
      <link>https://arxiv.org/abs/2407.07197</link>
      <description><![CDATA[arXiv:2407.07197v1 公告类型：新
摘要：随着扩散模型的出现，文本到图像 (T2I) 生成取得了重大进展。这些模型表现出基于文本提示生成图像的卓越能力。当前的 T2I 模型允许用户使用语言颜色名称指定对象颜色。然而，这些标签涵盖了广泛的颜色范围，很难实现精确的颜色匹配。为了解决这个具有挑战性的任务，即颜色提示学习，我们建议学习针对用户选择的颜色量身定制的特定颜色提示。现有的 T2I 个性化方法往往会导致颜色形状纠缠。为了克服这个问题，我们生成了几个目标颜色的基本几何对象，允许在颜色提示学习过程中解开颜色和形状的纠缠。我们的方法称为 ColorPeel，成功地帮助 T2I 模型从这些彩色形状中剥离出新的颜色提示。在实验中，我们证明了 ColorPeel 在使用 T2I 模型实现精确颜色生成的有效性。此外，我们推广了 ColorPeel，以有效地学习抽象属性概念，包括纹理、材料等。我们的研究结果代表着朝着提高 T2I 模型的精度和多功能性迈出了重要一步，为创意应用和设计任务提供了新的机会。我们的项目可在 https://moatifbutt.github.io/colorpeel/ 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.07197</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:32 GMT</pubDate>
    </item>
    <item>
      <title>ItTakesTwo：利用对等表示进行半监督 LiDAR 语义分割</title>
      <link>https://arxiv.org/abs/2407.07171</link>
      <description><![CDATA[arXiv:2407.07171v1 公告类型：新
摘要：为语义 LiDAR 分割方法建模而生成大型训练集的注释过程成本高昂且耗时，这推动了半监督学习 (SSL) 方法的发展。然而，这种 SSL 方法通常专注于仅对单个 LiDAR 表示采用一致性学习。这种狭窄的关注点导致有限的扰动，通常无法实现有效的一致性学习。此外，这些 SSL 方法采用基于从有限的正负嵌入样本集中采样的对比学习。本文介绍了一种名为 ItTakesTwo (IT2) 的新型半监督 LiDAR 语义分割框架。IT2 旨在确保来自对等 LiDAR 表示的一致预测，从而提高一致性学习中的扰动有效性。此外，我们的对比学习使用从整个训练集中学习到的正负嵌入分布中提取的信息样本。公开基准测试的结果表明，我们的方法比该领域之前最先进的 (SOTA) 方法取得了显著的改进。代码可在以下网址获取：https://github.com/yyliu01/IT2。]]></description>
      <guid>https://arxiv.org/abs/2407.07171</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:31 GMT</pubDate>
    </item>
    <item>
      <title>CamFreeDiff：使用扩散模型实现无摄像头图像到全景图的生成</title>
      <link>https://arxiv.org/abs/2407.07174</link>
      <description><![CDATA[arXiv:2407.07174v1 公告类型：新
摘要：本文介绍了一种无相机扩散 (CamFreeDiff) 模型，用于从单个无相机图像和文本描述中进行 360 度图像修复。该方法与现有策略（如 MVDiffusion）的区别在于，它消除了对预定义相机姿势的要求。相反，我们的模型结合了一种在多视图扩散框架内直接预测单应性的机制。我们方法的核心是通过预测从输入视图到预定义规范视图的单应性变换来制定相机估计。单应性在输入图像和目标全景图像之间提供点级对应关系，允许以完全可区分的方式通过对应感知注意力来强化连接。定性和定量实验结果证明了我们的模型在无相机输入的挑战性环境中对 360 度图像修复的强大鲁棒性和泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2407.07174</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:31 GMT</pubDate>
    </item>
    <item>
      <title>基于扩散模型的视频编辑：综述</title>
      <link>https://arxiv.org/abs/2407.07111</link>
      <description><![CDATA[arXiv:2407.07111v1 公告类型：新
摘要：扩散模型 (DM) 的快速发展极大地推动了图像和视频应用的发展，使“所见即所得”成为现实。其中，视频编辑引起了广泛关注，研究活动迅速增加，因此有必要对现有文献进行全面而系统的回顾。本文回顾了基于扩散模型的视频编辑技术，包括理论基础和实际应用。我们首先概述数学公式和图像领域的关键方法。随后，我们根据核心技术的内在联系对视频编辑方法进行分类，描绘出进化轨迹。本文还深入探讨了新颖的应用，包括基于点的编辑和姿势引导的人体视频编辑。此外，我们使用我们新推出的 V2VBench 进行了全面的比较。基于迄今为止取得的进展，本文最后总结了持续的挑战和未来研究的潜在方向。]]></description>
      <guid>https://arxiv.org/abs/2407.07111</guid>
      <pubDate>Thu, 11 Jul 2024 06:20:30 GMT</pubDate>
    </item>
    </channel>
</rss>