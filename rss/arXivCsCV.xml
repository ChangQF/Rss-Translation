<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Fri, 12 Jan 2024 03:15:11 GMT</lastBuildDate>
    <item>
      <title>空间相关传感器很重要：文本语义辅助的 3D 人体运动重建。 （arXiv：2401.05412v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05412</link>
      <description><![CDATA[利用可穿戴设备进行运动重建已成为一种
经济可行的技术。某些方法采用稀疏惯性
人体测量单元 (IMU) 并利用数据驱动策略
来模拟人体姿势。然而，仅基于运动的重建
稀疏的 IMU 数据本质上充满了模糊性，这是由于
对应于不同姿势的许多相同的 IMU 读数。在这个
论文中，我们探讨了多个传感器的空间重要性，并由
描述特定操作的文本。具体来说，引入了不确定性
导出每个 IMU 的加权特征。我们还设计了一个层次结构
时间变换器（HTT）并应用对比学习来实现精确
传感器数据的时间和特征与文本语义的对齐。
实验结果表明我们提出的方法取得了显着的成果
与现有方法相比，多个指标都有改进。值得注意的是，与
文本监督，我们的方法不仅区分歧义
坐、站等动作也变得更加精确和自然
运动。
]]></description>
      <guid>http://arxiv.org/abs/2401.05412</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>D3GU：通过增强域对齐实现多目标主动域适应。 （arXiv：2401.05465v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05465</link>
      <description><![CDATA[用于图像分类的无监督域适应（UDA）已经取得了进展
在从标记中转移分类知识方面取得了显着进展
得益于有效域，源域到未标记的目标域
对齐技术。最近，为了进一步提高性能
目标域，许多单目标主动域适应（ST-ADA）方法
已提议识别和注释显着目标和范例目标
样品。然而，它需要为每个模型训练和部署一个模型。
目标域和与每个测试样本关联的域标签。这
很大程度上限制了其在多种场景下的应用
目标域。因此，我们提出了多目标主动域适应
(MT-ADA) 图像分类框架，名为 D3GU，同时
对齐不同的领域并主动从中选择样本进行注释。
据我们所知，这是该领域的首次研究工作。 D3GU
在训练期间应用分解域辨别 (D3) 以实现这两个目标
源-目标和目标-目标域对齐。然后在主动采样期间，
梯度效用 (GU) 分数旨在对每个未标记的目标进行加权
图像对分类和域对齐任务的贡献，
并进一步与KMeans聚类结合形成GU-KMeans，用于各种
图像采样。在三个基准数据集 Office31、
OfficeHome 和 DomainNet 已进行一致验证
D3GU 对 MT-ADA 的卓越性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.05465</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>用于域广义水下物体检测的域相似性感知标签分配。 （arXiv：2401.05401v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05401</link>
      <description><![CDATA[水体的固有特性和光的波动产生
水下不同层次和区域之间的巨大差异
环境。当测试集是在不同的海洋区域收集时
训练集出现域转移问题，严重影响
模型的泛化能力。领域对抗性学习（DAL）
以前曾利用培训策略来应对此类挑战。
然而，DAL 严重依赖于手动的单热域标签，这意味着
同一域内的样本之间没有差异。这样的假设结果
DAL 的不稳定。本文介绍了Domain的概念
相似性感知标签分配（DSP）。每个图像的域标签是
视为其与指定域的相似性。通过特定领域
数据增强技术，我们在
水下跨域目标检测基准S-UODAC2020。此外，我们
在 Cityscapes 数据集中验证了我们的方法的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2401.05401</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>使用多传感器数据进行碰撞跌倒检测事件的机器学习和特征排名。 （arXiv：2401.05407v1 [eess.SP]）</title>
      <link>http://arxiv.org/abs/2401.05407</link>
      <description><![CDATA[个人尤其是老年人的跌倒可能会导致
严重伤害和并发症。检测跌倒时的冲击时刻
事件对于提供及时援助和最大程度地减少负面影响至关重要
结果。在这项工作中，我们的目标是通过应用来应对这一挑战
对多传感器数据集进行彻底的预处理技术，目标是
消除噪音并提高数据质量。此外，我们采用了一个功能
选择过程来识别从中衍生出的最相关的特征
多传感器 UP-FALL 数据集，这反过来又会提高性能和
机器学习模型的效率。然后我们评估效率
各种机器学习模型使用
来自多个传感器的结果数据信息。通过广泛
实验中，我们使用各种方法评估我们方法的准确性
评估指标。我们的结果在影响方面实现了高准确率
检测，展示了利用多传感器数据进行跌倒的力量
检测任务。这凸显了我们增强秋季效果的方法的潜力
检测系统并提高个人的整体安全和福祉
有跌倒的危险。
]]></description>
      <guid>http://arxiv.org/abs/2401.05407</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>AT-2FF：自适应 2 类模糊滤波器，用于对椒盐损坏的图像进行去噪。 （arXiv：2401.05392v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05392</link>
      <description><![CDATA[数字图像中不可避免地存在噪声，导致视觉图像
恶化。因此，需要采用合适的过滤方法来减少
噪声，同时保留图像特征（边缘、角点等）。这张纸
提出了具有自适应能力的高效 2 类模糊加权均值滤波器
消除 SAP 噪声的阈值。本过滤器有两个主要步骤：
第一阶段将图像分为轻度、中度和重度损坏
基于自适应阈值，通过将已处理像素的 M-ALD 与
2 类模糊标识符的上、下 MF。第二阶段
通过使用 GMF 计算适当的权重来消除损坏的像素
过滤器窗口中未损坏像素的均值和方差。
仿真结果生动地表明，所获得的去噪图像保留了
图像特征，即边缘、角点和其他尖锐结构，与
不同的过滤方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.05392</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:08 GMT</pubDate>
    </item>
    <item>
      <title>失去它的权利：基于学习的视觉里程计中的欧几里得和黎曼度量。 （arXiv：2401.05396v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05396</link>
      <description><![CDATA[本文概述了不同的姿态表示和度量函数
视觉里程计（VO）网络。 VO网络的性能很大程度上依赖于
他们的架构如何编码信息。姿势的选择
表示和损失函数显着影响网络收敛和
概括。我们通过以下方式研究 VO 网络 DeepVO 中的这些因素
实现基于欧拉、四元数和弦距离的损失函数
并分析它们对性能的影响。这项研究的结果提供了
深入了解损失函数如何影响高效准确的设计
用于相机运动估计的 VO 网络。实验表明，一个
符合度量数学要求的距离，例如
弦距离，提供更好的泛化和更快的收敛。
实验代码可以在以下位置找到
https://github.com/remaro-network/Loss_VO_right
]]></description>
      <guid>http://arxiv.org/abs/2401.05396</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:08 GMT</pubDate>
    </item>
    <item>
      <title>AutoVisual Fusion Suite：HuggingFace 平台上图像分割和语音转换工具的综合评估。 （arXiv：2401.05379v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05379</link>
      <description><![CDATA[这项研究对现有工具进行了全面评估
HuggingFace 平台适用于人工智能领域的两个关键应用：
图像分割和语音转换。主要目标是确定
每个类别中的前三个工具，然后安装和配置
Linux 系统上的这些工具。我们利用了预训练的力量
分割模型，例如具有 ResNet-50 主干的 SAM 和 DETR 模型
图像分割，以及用于语音转换的 so-vits-svc-fork 模型。这
论文深入探讨了研究过程中遇到的方法和挑战
实施过程，并展示了视频的成功结合
分段和语音转换位于名为 AutoVisual Fusion 的统一项目中
套房。
]]></description>
      <guid>http://arxiv.org/abs/2401.05379</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:07 GMT</pubDate>
    </item>
    <item>
      <title>基于建筑物中灯具的自动检测、识别和定位生成BIM数据。 （arXiv：2401.05390v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05390</link>
      <description><![CDATA[在本文中，我们介绍了一种支持检测的方法，
建筑物中灯具的识别和定位，主要目标是
通过建筑信息自动输入其能源模型
建模 (BIM) 方法。因此，所提出的方法提供了有用的信息
应用节能策略以减少建筑物的能源消耗
通过对照明基础设施的正确管理。基于
灯的独特几何形状和亮度以及仅使用灰度
图像，我们的方法能够获得准确的结果，尽管其低
计算需求，从而实现近实时处理。主要新颖性
是候选搜索的焦点不是整个图像，而是
而只是在总结特定特征的有限区域上
灯的。从我们的方法中获得的信息被用于绿色
构建 XML 架构以说明从 BIM 数据自动生成
算法的结果。
]]></description>
      <guid>http://arxiv.org/abs/2401.05390</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:07 GMT</pubDate>
    </item>
    <item>
      <title>ImbaGCD：不平衡的广义类别发现。 （arXiv：2401.05353v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05353</link>
      <description><![CDATA[广义类发现（GCD）旨在推断已知和未知类别
在未标记的数据集中利用标记集的先验知识，包括
已知的类。现有的研究隐含/明确地假设
每个类别的出现频率，无论已知还是未知，都是
与未标记数据大致相同。然而，在大自然中，我们更多的是
据报道，与未知/不常见的类别相比，更有可能遇到已知/常见的类别
视觉类的长尾属性。因此，我们提出一个
具有挑战性的实际问题，不平衡的广义类别发现
（ImbaGCD），其中未标记数据的分布是不平衡的，已知
课程比未知课程更频繁。为了解决这些问题，我们
提出 ImbaGCD，一种新颖的基于最优传输的期望最大化
通过调整类别来实现广义类别发现的框架
边缘类先验分布。 ImbaGCD 还集成了系统化的
GCD下估计不平衡类先验分布的机制
设置。我们的综合实验表明 ImbaGCD 超越了之前的
最先进的 GCD 方法，实现了大约 2 的改进 -
CIFAR-100 上为 4%，ImageNet-100 上为 15 - 19%，表明其优越性
解决不平衡 GCD 问题的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2401.05353</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:06 GMT</pubDate>
    </item>
    <item>
      <title>开发用于表面缺陷检测的资源约束 EdgeAI 模型。 （arXiv：2401.05355v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05355</link>
      <description><![CDATA[资源限制限制了多个 EdgeAI 应用程序的机器学习能力
学习推理方法，其中模型在云上进行训练
部署到边缘设备。这带来了诸如带宽、延迟、
以及与模型构建的异地存储数据相关的隐私。训练
边缘设备可以通过消除需要来克服这些挑战
将数据传输到另一台设备进行存储和模型开发。设备上
由于模型可以重新训练，训练还可以提供对数据变化的鲁棒性
基于新获取的数据以提高性能。因此，我们建议
从 Xception 修改而来的轻量级 EdgeAI 架构，用于设备上训练
在资源受限的边缘环境中。我们在 PCB 上评估我们的模型
缺陷检测任务并将其性能与现有轻量级进行比较
模型 - MobileNetV2、EfficientNetV2B0 和 MobileViT-XXS。我们的结果
实验表明我们的模型具有显着的性能
无需预训练，准确率达到 73.45%。这与测试相当
非预训练的 MobileViT-XXS 的准确率 (75.40%) 比其他的要好得多
非预训练模型（MobileNetV2 - 50.05%，EfficientNetV2B0 - 54.30%）。这
未经预训练的模型的测试准确性与预训练的相当
MobileNetV2 模型 - 75.45%，优于预训练的 EfficientNetV2B0 模型 -
58.10%。在内存效率方面，我们的模型表现优于
EfficientNetV2B0 和 MobileViT-XXS。我们发现资源效率
机器学习模型不仅仅取决于参数的数量，还取决于
还取决于架构方面的考虑。我们的方法可以应用于
其他资源受限的应用程序，同时保持显着
表现。
]]></description>
      <guid>http://arxiv.org/abs/2401.05355</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:06 GMT</pubDate>
    </item>
    <item>
      <title>DualTeacher：桥接未标记类的共存，用于半监督增量对象检测。 （arXiv：2401.05362v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05362</link>
      <description><![CDATA[在现实世界的应用中，对象检测器经常遇到对象
来自新类的实例并需要有效地容纳它们。以前的
工作将这个关键问题表述为增量对象检测（IOD），
它假设新类的对象实例被完全注释
增量数据。然而，由于监管信号通常很少见并且
成本高昂，受监督的 IOD 可能不切实际实施。在这个
工作中，我们考虑一个更现实的设置，称为半监督 IOD (SSIOD)，
对象检测器需要从一些类别中逐步学习新类别
标记数据和大量未标记数据，不会发生灾难性遗忘
老班。监督 IOD 的一个常用策略是鼓励
当前模型（作为学生）模仿旧模型（作为学生）的行为
老师），但它在 SSIOD 中通常会失败，因为对象数量占主导地位
新旧类的实例共存且未标记，
老师只认识其中的一小部分。观察到只学习
感兴趣的类别往往会排除其他类别的检测，我们建议
通过构建两个教师模型来弥合未标记类的共存
分别针对旧类和新类，并使用它们的串联
预测来指导学生。这种方法被称为
DualTeacher，可以作为有限的 SSIOD 的强大基线
资源开销并且没有额外的超参数。我们建立各种基准
SSIOD 并进行大量实验来证明我们的优越性
方法（例如，MS-COCO 上的性能领先高达 18.28 AP）。我们的代码是
可以在\url{https://github.com/chuxiuhong/DualTeacher}获取。
]]></description>
      <guid>http://arxiv.org/abs/2401.05362</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:06 GMT</pubDate>
    </item>
    <item>
      <title>DISTWAR：基于光栅的渲染管道上的快速可微分渲染。 （arXiv：2401.05345v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05345</link>
      <description><![CDATA[可微分渲染是一种重要的新兴类别中使用的技术
涉及将 3D 场景表示为
使用梯度下降从 2D 图像训练的模型。最近的作品（例如
3D Gaussian Splatting）使用光栅化管道来实现高渲染
从这些学习的 3D 模型中高速生成逼真的高质量图像。
这些方法已被证明非常有前途，提供
为许多重要任务提供最先进的质量。然而，训练一个模型
即使使用强大的 GPU，表示场景仍然是一项耗时的任务。
在这项工作中，我们观察到训练期间的梯度计算阶段是
由于大量的原子操作，GPU 上的一个重大瓶颈
需要处理的。这些原子操作压倒了原子单元
L2 分区导致停顿。为了应对这一挑战，我们利用
在梯度计算过程中观察到：（1）对于大多数扭曲，所有
线程自动更新相同的内存位置； (2) 产生扭曲
不同数量的原子流量（因为某些线程可能处于非活动状态）。我们
提出 DISTWAR，一种加速原子操作的软件方法
两个关键想法：首先，我们在 SM 上实现线程的扭曲级别减少
使用寄存器来利用 intra-warp 原子中的局部性的子核
更新。其次，我们在扭曲级别之间分配原子计算
SM 和 L2 原子单元的减少以提高吞吐量
原子计算。许多线程执行原子更新的扭曲
相同的内存位置被调度在 SM 上，其余的使用 L2 原子
单位。我们使用现有的扭曲级基元实现 DISTWAR。我们评估
DISTWAR 适用于广泛使用的基于光栅的可微分渲染工作负载。我们
表现出平均 2.44 倍的显着加速（高达 5.7 倍）。
]]></description>
      <guid>http://arxiv.org/abs/2401.05345</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:05 GMT</pubDate>
    </item>
    <item>
      <title>长尾识别的广义类别发现。 （arXiv：2401.05352v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05352</link>
      <description><![CDATA[广义类发现（GCD）在辨别两者方面发挥着关键作用
通过利用洞察力从未标记的数据集中识别已知和未知类别
派生自包含已识别类别的标记集。一个重要的
流行的 GCD 方法的局限性在于它们假设公平
未标记数据中的分布式类别出现。与这个假设相反，
自然环境中的视觉类别通常表现出长尾特征
分布，已知或流行的类别出现的频率比
他们的稀有同行。我们的研究致力于通过以下方式弥合这种脱节：
专注于长尾广义类别发现（Long-tailed GCD）
范式，这呼应了现实世界未标记数据集的固有不平衡。
为了应对长尾 GCD 带来的独特挑战，我们提出了
以两项战略调整为基础的稳健方法：(i) 重新调整权重
增强代表性较少的尾端的突出地位的机制
类别，以及（ii）与预期一致的类先验约束
班级分布。综合实验表明我们提出的方法
通过改进，超越了以前最先进的 GCD 方法
在 ImageNet100 上约为 6 - 9%，在 CIFAR100 上具有竞争性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.05352</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:05 GMT</pubDate>
    </item>
    <item>
      <title>STR-Cert：深度学习管道和视觉转换器上的深度文本识别的鲁棒性认证。 （arXiv：2401.05338v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05338</link>
      <description><![CDATA[鲁棒性认证，旨在正式证明预测
针对对抗性输入的神经网络已成为
安全关键型应用的重要工具。尽管取得了相当大的进步，
现有的认证方法仅限于基本架构，例如
卷积网络、循环网络和最近的变形金刚，
基准数据集，例如 MNIST。在本文中，我们重点关注鲁棒性
场景文本识别（STR）认证，这是一个复杂且复杂的过程
广泛部署的基于图像的序列预测问题。我们解决三个
STR 模型架构的类型，包括标准 STR 管道和
视觉变压器。我们提出STR-Cert，这是STR的第一个认证方法
模型，通过显着扩展 DeepPoly 多面体验证
通过导出关键 STR 模型的新颖多面体边界和算法来构建框架
成分。最后，我们在六个数据集上验证并比较 STR 模型，
展示稳健性认证的效率和可扩展性，
特别是对于视觉转换器。
]]></description>
      <guid>http://arxiv.org/abs/2401.05338</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:04 GMT</pubDate>
    </item>
    <item>
      <title>MicroGlam：带有化妆品的显微皮肤图像数据集。 （arXiv：2401.05339v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.05339</link>
      <description><![CDATA[在本文中，我们提出了一个特定于化妆品的皮肤图像数据集。它包含
来自 $45$ 补丁的皮肤图像（每个 $5$ 皮肤补丁来自 $9$ 参与者）
三种化妆品（即粉底、腮红、
和荧光笔）。我们设计了一种新颖的捕捉设备，灵感来自 Light Stage。
使用该设备，我们拍摄了超过 600 美元的每个皮肤贴片的图像
30 美元秒内实现多种照明条件。我们重复了这个过程
三种化妆品下的同一个皮肤贴片。最后，我们展示了
具有基于图像到图像转换的管道的数据集的可行性
化妆品渲染并将我们的数据驱动方法与现有的方法进行比较
化妆品渲染方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.05339</guid>
      <pubDate>Fri, 12 Jan 2024 03:15:04 GMT</pubDate>
    </item>
    </channel>
</rss>