<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Thu, 14 Dec 2023 03:14:41 GMT</lastBuildDate>
    <item>
      <title>超越端到端培训：通过上下文供应促进贪婪本地学习。 （arXiv：2312.07636v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07636</link>
      <description><![CDATA[深度网络的传统端到端（E2E）训练需要存储
反向传播的中间激活，产生大量内存
GPU 占用空间和有限的模型并行化。作为备选，
贪婪的局部学习将网络划分为梯度隔离的模块，
根据当地初步损失进行监督训练，从而提供
大幅减少内存的异步和并行训练方法
成本。然而，实证实验表明，随着分割的数量
梯度隔离模块的增加，局部的性能
学习方案大幅退化，严重限制了其可扩展性。到
为了避免这个问题，我们从理论上分析了贪婪局部学习
从信息论的角度提出了ContSup方案，
合并隔离模块之间的上下文供应以补偿
信息丢失。在基准数据集（即CIFAR、SVHN、STL-10）上进行实验
达到 SOTA 结果并表明我们提出的方法可以显着
用最少的内存提高贪婪局部学习的性能
计算开销，允许增加隔离的数量
模块。我们的代码可在 https://github.com/Tab-ct/ContSup 获取。
]]></description>
      <guid>http://arxiv.org/abs/2312.07636</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>通过在人机交互中利用人类凝视和增强现实来教学未知物体。 （arXiv：2312.07638v1 [cs.HC]）</title>
      <link>http://arxiv.org/abs/2312.07638</link>
      <description><![CDATA[由于以下原因，机器人在各种环境中变得越来越受欢迎
其卓越的工作能力、精度、效率和可扩展性。
人工智能的进步进一步鼓励了这一发展
智能，特别是机器学习。通过采用复杂的神经网络
网络中，机器人具有检测物体并与物体交互的能力
他们的附近。然而，一个显着的缺点来自于底层
依赖于广泛的数据集和大量的可用性
这些对象检测模型的训练数据。这个问题就变成了
当机器人的具体部署位置和
周围的环境，事先并不知晓。庞大且不断扩大的阵列
的对象使得几乎不可能全面覆盖整个
仅使用预先存在的数据集来分析现有对象的范围。此举的目标
论文的目的是在人机对话的背景下教机器人未知的物体
交互（HRI），将其从数据依赖中解放出来，释放
它来自预定义的场景。在此背景下，结合眼动追踪
和增强现实创造了强大的协同作用，增强了人类的能力
老师可以与机器人交流并轻松地指出物体
人类凝视的手段。这种整体方法导致了
多模式 HRI 系统使机器人能够识别和视觉分割
3D 空间中的感兴趣对象。通过提供的类信息
与人类相比，机器人能够学习这些物体并在稍后重新检测它们
阶段。由于从基于 HRI 的教学中获得的知识，机器人的
物体检测能力表现出与
最先进的物体检测器在广泛的数据集上进行了训练，而无需
仅限于预定义的类，展示其多功能性和适应性。
]]></description>
      <guid>http://arxiv.org/abs/2312.07638</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>CLIP 作为 RNN：无需训练即可分割无数视觉概念。 （arXiv：2312.07661v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07661</link>
      <description><![CDATA[现有的开放词汇图像分割方法需要微调
踩在掩模注释和/或图像文本数据集上。面膜标签有
劳动密集型，限制了细分中的类别数量
数据集。因此，预训练 VLM 的开放词汇能力为
微调后大幅降低。然而，在没有微调的情况下，VLM 训练
在弱图像文本监督下往往会做出次优的掩模预测
有文本查询引用图像中不存在的概念。到
为了缓解这些问题，我们引入了一种新颖的循环框架
逐步过滤掉不相关的文本并增强蒙版质量，而无需
培训工作。循环单元是一个基于 VLM 的两级分段器
与冷冻重量。因此，我们的模型保留了 VLM 广泛的词汇空间
并强化细分能力。实验结果表明我们的
该方法不仅优于未经训练的方法，而且优于那些
使用数百万个额外数据样本进行微调，并设置新的
零样本语义和参考图像的最先进记录
分割任务。具体来说，我们将当前记录提高了 28.8、16.0、
Pascal VOC、COCO 对象和 Pascal 上下文的 6.9 MIoU。
]]></description>
      <guid>http://arxiv.org/abs/2312.07661</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>多模态情感分析：感知情感与诱发情感。 （arXiv：2312.07627v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07627</link>
      <description><![CDATA[社交媒体创建了一个全球网络，人们可以轻松访问和
交换大量信息。这些信息引发了各种
意见，反映积极和消极的观点。 GIF 脱颖而出
多媒体格式为用户提供一种视觉上引人入胜的交流方式。在
在这项研究中，我们提出了一个集成视觉和
文本特征来预测 GIF 情绪。它还包含属性
包括面部情绪检测和 OCR 生成的字幕来捕捉
GIF 的语义方面。所开发的分类器的准确度为
Twitter GIF 上的准确率为 82.7%，这比最先进的模型有所改进。
此外，我们的研究基于 ReactionGIF 数据集，分析了
作者感知到的情绪与作者所引发的情绪之间存在差异
读者
]]></description>
      <guid>http://arxiv.org/abs/2312.07627</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>预先训练的通用医学图像转换器。 （arXiv：2312.07630v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07630</link>
      <description><![CDATA[自我监督学习已成为利用
大量未标记的医学成像数据，解决了
医学图像分析中标记数据的稀缺性。特别是，蒙版图像
具有视觉标记重建的建模（MIM）在以下方面显示了有希望的结果：
通用计算机视觉（CV）领域，并作为医学的候选者
图像分析。然而，异构 2D 和 3D 医学图像的存在
通常限制了可以有效地训练数据的数量和多样性
用于单个模型结构。在这项工作中，我们提出了一种空间
自适应卷积（SAC）模块，自适应调整卷积
基于输入图像的体素间距的参数。使用此 SAC
模块，我们构建了一个通用视觉分词器和一个通用视觉
Transformer (ViT) 能够有效处理各种医疗
具有各种成像模式和空间特性的图像。此外，在
为了增强视觉分词器重建的鲁棒性
对于 MIM 的目标，我们建议概括 的离散令牌输出
视觉标记器到概率软标记。我们证明了广义的
软令牌表示可以与先前的有效集成
通过建设性解释进行分布正则化。因此，
我们预先训练一个通用视觉分词器，然后通过以下方式预训练通用 ViT
对 55 个公共医学图像数据集进行视觉标记重建，包括
超过 900 万个 2D 切片（包括超过 48,000 个 3D 图像）。这代表了
最大、最全面、最多样化的 3D 医学预训练数据集
我们所知的图像模型。下游医学图像实验结果
分类和分割任务展示了其优越的性能
我们的模型并提高了标签效率。
]]></description>
      <guid>http://arxiv.org/abs/2312.07630</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>特征指导：大指导尺度下 DDPM 的非线性校正。 （arXiv：2312.07586v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07586</link>
      <description><![CDATA[线性去噪扩散概率模型 (DDPM) 的流行指南
将不同的条件模型组合在一起以提供增强的控制
样品。然而，这种方法忽略了非线性效应，这些效应成为
当指导尺度较大时显着。为了解决这个问题，我们建议
特征引导，一种提供非线性校正的新方法
无分类器引导的 DDPM。这种修正迫使引导 DDPM
尊重其基础扩散过程的福克-普朗克方程，
第一性原理、免训练、免导数、兼容的方式
与现有的采样方法。实验表明，特征引导
适合各种应用，增强对样品的控制
生成，即使对于潜在空间也能抑制颜色和曝光问题
采样，并且可以处理诸如相变之类的物理问题。
]]></description>
      <guid>http://arxiv.org/abs/2312.07586</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>用于动态场景理解的时空事件图。 （arXiv：2312.07621v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07621</link>
      <description><![CDATA[动态场景理解是计算机系统解释的能力
并理解现实世界视频中呈现的视觉信息
场景。在本文中，我们提出了一系列动态场景框架
从自动驾驶的道路事件检测开始理解
透视复杂的视频活动检测，然后持续学习
模型的终身学习方法。首先，我们介绍一下
据我们所知，用于自动驾驶的 ROad 事件意识数据集 (ROAD)
同类中的第一个。由于缺乏正式配备的数据集
指定了逻辑需求，我们还引入了ROad事件感知
具有逻辑要求的数据集（ROAD-R），第一个公开可用的
自动驾驶数据集，其要求表达为逻辑
约束，作为推动该领域神经符号研究的工具。接下来，我们
通过提出两个复杂的事件检测将事件检测扩展到整体场景理解
活动检测方法。在第一种方法中，我们提出了一个可变形的，
时空场景图方法，由三个主要构建块组成：
动作管检测，专为学习而设计的 3D 可变形 RoI 池化层
组成动作管的灵活、可变形的几何形状以及场景
将所有部分视为节点并基于连接它们而构建的图
在不同的语义上。在从第一种方法演变而来的第二种方法中，我们
提出一种混合图神经网络，该网络结合了应用于
使用时间图对局部（短期）动态场景进行图编码
对整体长期活动进行建模。最后，最后一部分
论文是关于提出一种新的持续半监督学习（CSSL）
范例。
]]></description>
      <guid>http://arxiv.org/abs/2312.07621</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>用于细粒度染色体识别的监督对比学习。 （arXiv：2312.07623v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07623</link>
      <description><![CDATA[染色体识别是核型分析中的一项重要任务，它在核型分析中发挥着重要作用
在出生缺陷诊断和生物医学研究中发挥着重要作用。然而，现有的
由于类间的差异，分类方法面临着重大挑战
染色体的相似性和类内变异。为了解决这个问题，我们
提出一种专门用于训练的监督对比学习策略
与模型无关的深度网络，用于可靠的染色体分类。这
该方法能够提取潜在空间中的细粒度染色体嵌入。
这些嵌入有效地扩展了类间边界并减少了
类内差异，增强其预测的独特性
染色体类型。在两个大规模染色体数据集之上，我们
全面验证我们的对比学习策略的力量
促进 Transformers 和 ResNets 等尖端深度网络的发展。广泛的
结果表明它可以显着提高模型的泛化能力
性能，准确度提升高达 +4.5%。代码和预训练
模型将在接受这项工作后发布。
]]></description>
      <guid>http://arxiv.org/abs/2312.07623</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>研究针对视障人士的室外障碍物检测的 YOLO 模型。 （arXiv：2312.07571v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07571</link>
      <description><![CDATA[利用基于深度学习的目标检测是一种有效的方法
帮助视障人士避开障碍物的方法。在这个
论文中，我们实现了七种不同的 YOLO 目标检测模型
\textit{viz}.、YOLO-NAS（小型、中型、大型）、YOLOv8、YOLOv7、YOLOv6 和
YOLOv5并经过精心调整进行了全面评估
超参数，分析这些模型如何在包含以下内容的图像上执行
道路和人行道上常见的日常生活物品。经过系统的
调查发现YOLOv8是最好的模型，达到了精度
$80\%$ 以及在著名的障碍物数据集上召回 $68.2\%$
包括来自 VOC 数据集、COCO 数据集和 TT100K 数据集的图像以及
该领域的研究人员收集的图像。尽管是最新的
模型并在许多其他应用程序中展示更好的性能，YOLO-NAS
被发现对于障碍物检测任务来说不是最佳的。
]]></description>
      <guid>http://arxiv.org/abs/2312.07571</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>使用切片处理技术和改进的 Xception 分类器从计算机断层扫描图像中检测 COVID-19。 (arXiv:2312.07580v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.07580</link>
      <description><![CDATA[本文扩展了我们之前的 COVID-19 诊断方法，提出了一种
用于从计算机断层扫描 (CT) 图像中检测 COVID-19 的增强型解决方案。
为了减少模型错误分类，图像处理的两个关键步骤是
受雇。首先，去掉最上面和最下面的切片，保留
每位患者切片的百分之六十。其次，所有切片均经过手工
裁剪以强调肺部区域。随后，调整了 CT 扫描的大小（224
224）被输入到 Xception 迁移学习模型中。利用 Xception
架构和预训练权重，修改后的模型实现了二进制
分类。 COV19-CT 数据库上的有希望的结果显示出更高的
切片和患者级别的验证准确性和宏观 F1 评分
与我们之前的解决方案和相同数据集上的替代方案进行比较。
]]></description>
      <guid>http://arxiv.org/abs/2312.07580</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>DFGET：用于腺体实例分割的位移场辅助图形能量发射器。 （arXiv：2312.07584v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07584</link>
      <description><![CDATA[腺体实例分割是一项重要但具有挑战性的任务
腺癌的诊断和治疗。现有模型通常可以达到
通过多任务学习和边界损失进行腺体实例分割
约束。然而，如何处理腺体粘连的问题呢？
分割复杂样本时边界不准确仍然是一个挑战。在
这项工作，我们提出了一种位移场辅助图能量发射器
（DFGET）框架来解决这些问题。具体来说，一条新颖的消息
开发了基于各向异性扩散的传递方式来更新节点
特征，可以区分同构图并提高
复杂样本的图节点的表达能力。使用这样的图框架，
腺体语义分割图和位移场（DF）
图节点是用两个图网络分支来估计的。随着约束
针对DF的问题，提出了一种基于扩散理论的图聚类模块来改进
类内特征一致性和类间特征差异，如
以及将粘附腺体与语义分割图分开。
GlaS 数据集上的广泛比较和消融实验表明
DFGET 的优越性和所提出的各向异性消息的有效性
传递方式和聚类方法。与最佳对比模型相比，
DFGET 将 object-Dice 和 object-F1 得分分别提高了 2.5% 和 3.4%
分别降低了物体HD 32.4%，实现
最先进的性能。
]]></description>
      <guid>http://arxiv.org/abs/2312.07584</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>Make-A-Storyboard：具有分离和合并控制的故事板通用框架。 （arXiv：2312.07549v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07549</link>
      <description><![CDATA[故事可视化旨在生成与故事提示相符的图像，
通过视觉一致性反映故事书的连贯性
人物和场景。而目前的方法只集中于
字符并忽略上下文相关的视觉一致性
场景，产生独立的角色图像，没有图像间
连贯性。为了解决这个问题，我们为 Story 提出了一种新的呈现形式
称为故事板的可视化，受到电影制作的启发，如图所示
图 1. 具体来说，故事板将故事展开为视觉表示
一个场景接一个场景。在故事板的每个场景中，角色都参与其中
在同一地点进行活动，需要两个视觉上一致的场景
对于故事板，我们设计了一个通用框架，如下所示
制作故事板，对内容的一致性进行分离控制
上下文相关的人物和场景，然后将它们合并形成
协调图像。大量实验证明 1) 有效性。
该方法在故事对齐、人物一致性和
场景关联； 2）概括。我们的方法可以无缝集成
融入主流图像定制方法，赋予他们
故事可视化能力。
]]></description>
      <guid>http://arxiv.org/abs/2312.07549</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:36 GMT</pubDate>
    </item>
    <item>
      <title>理解文本到图像生成模型中的（非）预期记忆。 （arXiv：2312.07550v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07550</link>
      <description><![CDATA[多模态机器学习，尤其是像 Stable 这样的文本到图像模型
Diffusion 和 DALL-E 3，对于将文本转换为文本具有重要意义
详细的图像。

尽管它们的使用不断增长且生成能力显着，但仍有
迫切需要对这些模型的行为进行详细检查，
特别是在记忆方面。从历史上看，记忆
机器学习一直依赖于上下文，并且出现了不同的定义
从分类任务到大型语言模型 (LLM) 等复杂模型
和扩散模型。然而，一个明确的记忆概念与
文本到图像合成的复杂性仍然难以捉摸。这
理解至关重要，因为记忆会带来隐私风险，但对于
满足用户期望，特别是在生成表示时
代表性不足的实体。在本文中，我们引入一个专门的定义
针对文本到图像模型定制的记忆，将其分为三类
根据用户期望的不同类型。我们仔细审视微妙之处
有意记忆和无意记忆之间的区别，强调
平衡用户隐私与模型生成质量的重要性
输出。使用稳定扩散模型，我们提供示例来验证我们的
记忆定义并阐明其应用。
]]></description>
      <guid>http://arxiv.org/abs/2312.07550</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:36 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的结构检测和从历史地籍地图（施泰尔马克省 19 世纪初的方济会地籍）和当前高分辨率卫星和遥感航空图像中提取信息。 （arXiv：2312.07560v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07560</link>
      <description><![CDATA[19 世纪的地籍是复杂而丰富的资源
历史学家和考古学家，其使用给他们带来了巨大的挑战。
针对考古和历史遥感，我们训练了几位Deep
学习模型、CNN 以及 Vision Transformer，以提取大规模数据
来自该知识表示的数据。我们提出的主要结果
我们的工作在这里，我们展示了我们基于浏览器的工具的演示器
使研究人员和公共利益相关者能够快速识别
19 世纪方济会地籍中的特色建筑。该工具不仅
支持学者和研究人员更好地理解
施蒂利亚州地区的定居历史，它也有助于公众
政府和同胞们迅速确定问题严重的领域
对该地区文化遗产的敏感性。
]]></description>
      <guid>http://arxiv.org/abs/2312.07560</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:36 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习根据腕戴式加速度计数据注释儿童的睡眠状态。 （arXiv：2312.07561v1 [eess.SP]）</title>
      <link>http://arxiv.org/abs/2312.07561</link>
      <description><![CDATA[睡眠检测和注释对于研究人员理解至关重要
睡眠模式，尤其是儿童。配备现代腕戴式手表
包括内置加速度计，可以收集睡眠日志。但是，那
将这些日志注释为不同的睡眠事件：开始和唤醒，证明
具有挑战性。这些注释必须是自动化的、精确的和可扩展的。
我们建议使用不同的机器学习对加速度计数据进行建模
(ML) 技术，例如支持向量、Boosting、集成方法等
涉及 LSTM 和基于区域的 CNN 的复杂方法。以后我们的目标是
使用事件检测平均精度 (EDAP) 评估这些方法
得分（类似于 IOU 指标）以最终比较预测能力
和模型性能。
]]></description>
      <guid>http://arxiv.org/abs/2312.07561</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:36 GMT</pubDate>
    </item>
    </channel>
</rss>