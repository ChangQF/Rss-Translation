<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 04 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用积分梯度的对抗性 3D 虚拟补丁</title>
      <link>https://arxiv.org/abs/2406.00282</link>
      <description><![CDATA[arXiv:2406.00282v1 公告类型：新
摘要：LiDAR 传感器广泛用于自动驾驶汽车，以更好地感知环境。然而，先前的研究表明，LiDAR 信号可以被欺骗，以向 3D 物体探测器隐藏真实物体。本研究探讨了通过基于虚拟补丁 (VP) 的新型物体隐藏策略减少所需欺骗区域的可行性。我们首先手动设计 VP (MVP)，并表明以 VP 为重点的攻击可以实现与先前工作相似的成功率，但所需欺骗面积仅为其一小部分。然后，我们设计了一个框架 Saliency-LiDAR (SALL)，它可以使用积分梯度识别 LiDAR 物体的关键区域。与我们的基线相比，在关键区域 (CVP) 上制作的 VP 将物体检测召回率降低了至少 15%，对于平均尺寸的车辆，欺骗面积减少了约 50%。]]></description>
      <guid>https://arxiv.org/abs/2406.00282</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:05 GMT</pubDate>
    </item>
    <item>
      <title>使用图卷积网络进行直立调整</title>
      <link>https://arxiv.org/abs/2406.00263</link>
      <description><![CDATA[arXiv:2406.00263v1 公告类型：新
摘要：我们提出了一种用于 360 图像直立调整的新方法。我们的网络由两个模块组成，即卷积神经网络 (CNN) 和图卷积网络 (GCN)。使用 CNN 处理输入的 360 图像以进行视觉特征提取，并将提取的特征图转换为找到输入的球面表示的图。我们还引入了一种新颖的损失函数来解决在球体表面上定义的离散概率分布问题。实验结果表明，我们的方法优于基于全连接的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.00263</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>使用扩展注意力机制实现视频中时间一致的对象编辑</title>
      <link>https://arxiv.org/abs/2406.00272</link>
      <description><![CDATA[arXiv:2406.00272v1 公告类型：新 
摘要：随着大规模扩散模型的兴起，图像生成和编辑取得了长足的进步，这些模型允许用户控制文本、蒙版、深度图等不同模态。然而，视频的受控编辑仍然落后。该领域的先前工作集中于使用 2D 扩散模型全局改变现有视频的风格。另一方面，在许多实际应用中，编辑视频的局部部分至关重要。在这项工作中，我们提出了一种使用预先训练的修复图像扩散模型编辑视频的方法。我们系统地重新设计了模型的前向路径，用创建帧级依赖关系的扩展版本的注意模块替换自注意模块。通过这种方式，我们确保无论蒙版区域的形状和位置如何，编辑的信息在所有视频帧中都是一致的。我们在多个视频编辑任务（如对象重定向、对象替换和对象移除任务）的准确性方面定性地比较了我们的结果与最新成果。模拟证明了所提策略的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2406.00272</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>StyDeSty：单域泛化的最小最大风格化和去风格化</title>
      <link>https://arxiv.org/abs/2406.00275</link>
      <description><![CDATA[arXiv:2406.00275v1 公告类型：新
摘要：单域泛化（单 DG）旨在从一个训练域中学习一个可推广到未知域的稳健模型，这是一项雄心勃勃且具有挑战性的任务。最先进的方法主要依赖于数据增强，例如对抗性扰动和风格增强，以合成新数据并提高鲁棒性。然而，他们在很大程度上忽视了增强域之间的潜在一致性，这反过来又导致在现实世界中的结果较差。在本文中，我们提出了一个简单而有效的方案，称为 \emph{StyDeSty}，以明确考虑数据增强过程中源域和伪域的对齐，使它们能够以自洽的方式相互作用，并进一步产生具有强大泛化能力的潜在域。 StyDeSty 的核心在于 \emph{风格化} 模块与 \emph{去风格化} 模块之间的相互作用，前者使用源域生成新的风格化样本，后者将风格化样本和源样本传输到潜在域以学习内容不变特征。风格化和去风格化模块相互对抗，相互加强。在推理过程中，去风格化模块将输入样本以任意风格转换转换到潜在域，在该域​​中执行下游任务。具体而言，去风格化层在主干网络中的位置由专用的神经架构搜索 (NAS) 策略确定。我们在多个基准上对 StyDeSty 进行了评估，并证明它取得了令人鼓舞的结果，在分类准确率上比现有技术高出多达 {13.44%}。代码可在此处获取：https://github.com/Huage001/StyDeSty。]]></description>
      <guid>https://arxiv.org/abs/2406.00275</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>Artemis：面向复杂视频的指称理解</title>
      <link>https://arxiv.org/abs/2406.00258</link>
      <description><![CDATA[arXiv:2406.00258v1 公告类型：新
摘要：视频携带丰富的视觉信息，包括对象描述、动作、交互等，但现有的多模态大型语言模型 (MLLM) 在基于视频的引用等指称理解场景中存在不足。在本文中，我们提出了 Artemis，这是一种将基于视频的指称理解推向更精细水平的 MLLM。给定一个视频，Artemis 会在任何视频帧中接收一个带有边界框的自然语言问题，并描述整个视频中引用的目标。实现这一目标的关键在于提取紧凑的、特定于目标的视频特征，我们通过跟踪和选择视频中的时空特征来设置坚实的基线。我们在新建立的 VideoRef45K 数据集上使用 45K 视频问答对训练 Artemis，并设计一个计算效率高的三阶段训练程序。结果在数量和质量上都很有希望。此外，我们表明 \model 可以与视频基础和文本摘要工具集成，以理解更复杂的场景。代码和数据可在https://github.com/qiujihao19/Artemis获取。]]></description>
      <guid>https://arxiv.org/abs/2406.00258</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>PuzzleFusion++：通过去噪和验证实现自动凝聚 3D 断裂组装</title>
      <link>https://arxiv.org/abs/2406.00259</link>
      <description><![CDATA[arXiv:2406.00259v1 公告类型：新
摘要：本文提出了一种新颖的“自动聚集”3D 断裂组装方法 PuzzleFusion++，类似于人类解决具有挑战性的空间难题的方式。从单个碎片开始，该方法 1) 将碎片对齐并合并成更大的组，类似于聚集聚类；2) 迭代重复该过程以完成类似于自回归方法的组装。具体来说，扩散模型同时对碎片的 6-DoF 对齐参数进行去噪，而变换器模型验证并将成对对齐合并为更大的对齐，该过程迭代重复。在 Breaking Bad 数据集上进行的大量实验表明，PuzzleFusion++ 在所有指标上都以显着优势胜过所有其他最先进的技术，特别是在部分准确度方面超过 10%，在倒角距离方面超过 50%。代码将在我们的项目页面上提供：https://puzzlefusion-plusplus.github.io。]]></description>
      <guid>https://arxiv.org/abs/2406.00259</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶的公平性：了解恶劣天气下物体检测的混杂因素</title>
      <link>https://arxiv.org/abs/2406.00219</link>
      <description><![CDATA[arXiv:2406.00219v1 公告类型：新
摘要：自动驾驶汽车 (AV) 的部署正在迅速扩展到众多城市。在 AV 的核心中，物体检测模块起着至关重要的作用，它通过考虑附近行人、车辆等的存在直接影响所有下游决策任务。尽管在保留的数据集上检测到行人的准确度很高，但此类物体检测器中是否存在算法偏差，尤其是在恶劣的天气条件下，仍不清楚。这项研究对基于 Transformer 的最先进的物体检测器在检测行人方面的公平性进行了全面的实证分析。除了经典指标外，我们还引入了新颖的基于概率的指标来衡量物体检测的各种复杂属性。利用最先进的 FACET 数据集和 Carla 高保真车辆模拟器，我们的分析探讨了受保护属性（例如性别、肤色和体型）对不同环境条件下（例如环境黑暗和雾）物体检测性能的影响。我们的定量分析揭示了之前被忽视但直观的因素（例如场景中的人口群体分布、天气严重程度、行人与 AV 的距离等）如何影响物体检测性能。我们的代码可在 https://github.com/bimsarapathiraja/fair-AV 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.00219</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>ImplicitTerrain：用于地形数据分析的连续表面模型</title>
      <link>https://arxiv.org/abs/2406.00227</link>
      <description><![CDATA[arXiv:2406.00227v1 公告类型：新
摘要：数字地形模型 (DTM) 在遥感、制图和景观管理中至关重要，需要准确的表面表示和拓扑信息恢复。虽然拓扑分析传统上依赖于平滑流形，但由于缺乏易于使用的大型地形连续表面模型，因此人们更喜欢离散网格。基于拓扑的结构表示提供了简洁的表面描述，为许多地形分析应用奠定了基础。然而，在离散网格上，数值问题出现了，并且设计了复杂的算法来处理它们。本文将地形数据分析的背景带回到连续世界，并介绍了 ImplicitTerrain（项目主页可在 https://fengyee.github.io/implicit-terrain/ 上找到），这是一种用于连续和可区分地建模高分辨率地形的隐式神经表示 (INR) 方法。我们的综合实验证明了卓越的表面拟合精度、有效的拓扑特征检索以及在这种紧凑表示上并行实现的各种地形特征提取。据我们所知，ImplicitTerrain 开创了一种可行的连续地形表面建模管道，为我们的社区提供了一条新的研究途径。]]></description>
      <guid>https://arxiv.org/abs/2406.00227</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>脉冲耦合神经网络在计算机视觉和图像处理中的应用综述</title>
      <link>https://arxiv.org/abs/2406.00239</link>
      <description><![CDATA[arXiv:2406.00239v1 公告类型：新
摘要：受哺乳动物视觉皮层启发的神经模型研究已导致许多脉冲神经网络，例如脉冲耦合神经网络 (PCNN)。这些模型是振荡的时空模型，受图像刺激以产生多个基于时间的响应。本文回顾了 PCNN 的最新进展，涵盖了其数学公式、变体和文献中发现的其他简化。我们介绍了几种应用，其中 PCNN 架构已成功解决一些基本的图像处理和计算机视觉挑战，包括图像分割、边缘检测、医学成像、图像融合、图像压缩、对象识别和遥感。这些应用中取得的结果表明，PCNN 架构可生成与各种计算机视觉任务相关的有用感知信息。]]></description>
      <guid>https://arxiv.org/abs/2406.00239</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>SNED：高效视频传播模型的叠加网络架构搜索</title>
      <link>https://arxiv.org/abs/2406.00195</link>
      <description><![CDATA[arXiv:2406.00195v1 公告类型：新
摘要：虽然人工智能生成的内容引起了广泛关注，但实现照片般逼真的视频合成仍然是一项艰巨的挑战。尽管传播模型在视频生成质量方面取得了令人鼓舞的进展，但复杂的模型架构以及训练和推理的大量计算需求导致这些模型与实际应用之间存在巨大差距。本文提出了一种用于高效视频传播模型的叠加网络架构搜索方法 SNED。我们的方法采用超网训练范式，使用权重共享方法针对各种模型成本和分辨率选项。此外，我们提出了超网训练采样预热，以实现快速训练优化。为了展示我们方法的灵活性，我们进行了涉及像素空间和潜在空间视频传播模型的实验。结果表明，我们的框架能够以高效率在不同的模型选项中一致地产生可比的结果。根据像素空间视频扩散模型的实验，我们可以同时在 64 x 64 到 256 x 256 分辨率下实现一致的视频生成结果，模型大小范围从 640M 到像素空间视频扩散模型的 1.6B 数量。]]></description>
      <guid>https://arxiv.org/abs/2406.00195</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>A-SDM：通过模型组装和特征继承策略加速稳定扩散</title>
      <link>https://arxiv.org/abs/2406.00210</link>
      <description><![CDATA[arXiv:2406.00210v1 公告类型：新
摘要：稳定扩散模型（SDM）是一种流行且有效的文本到图像（T2I）和图像到图像（I2I）生成的模型。尽管在采样器优化、模型蒸馏和网络量化方面进行了各种尝试，但这些方法通常保持原始网络架构。广泛的参数规模和大量的计算需求限制了对调整模型架构的研究。1）对于调整方法，我们设计了一种模型组装策略来重建轻量级模型，同时通过蒸馏保持性能。其次，为了减轻由于修剪造成的性能损失，我们将多专家条件卷积（ME-CondConv）合并到压缩的 UNet 中，以通过增加容量而不牺牲速度来增强网络性能。第三，我们验证了多 UNet 切换方法对提高网络速度的有效性。2）对于免调整方法，我们提出了一种特征继承策略，通过跳过网络结构中块、层或单元级别的局部计算来加速推理。我们还研究了时间步长级特征继承的多种采样模式。实验表明，所提出的调整方法和免调整方法都可以提高 SDM 的速度和性能。通过模型组装策略重建的轻量级模型将生成速度提高了 $22.4\%$，而特征继承策略将 SDM 生成速度提高了 $40.0\%$。]]></description>
      <guid>https://arxiv.org/abs/2406.00210</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>增强视觉创造力：图像编辑建议的视觉语言助手</title>
      <link>https://arxiv.org/abs/2406.00121</link>
      <description><![CDATA[arXiv:2406.00121v1 公告类型：新
摘要：基于文本的图像生成和编辑的进步彻底改变了内容创作，使用户能够从富有想象力的文本提示中创建令人印象深刻的内容。然而，现有的方法并不能很好地处理在典型场景中经常遇到的过于简单的提示，当用户开始编辑时，脑海中只有模糊或抽象的目的。这些场景需要用户进行精心构思，以弥合这些模糊的起点与描述预期结果所需的详细创意之间的差距。在本文中，我们介绍了图像编辑推荐 (IER) 的任务。此任务旨在从输入图像和代表用户未指定编辑目的的简单提示中自动生成各种创意编辑指令。为此，我们引入了 Creativity-Vision 语言助手~(Creativity-VLA)，这是一个专为编辑指令生成而设计的多模式框架。我们在专门为 IER 策划的编辑指令数据集上训练 Creativity-VLA。我们通过一种新颖的“标记本地化”机制进一步增强了我们的模型，使其能够支持全局和本地编辑操作。我们的实验结果证明了 \ours{} 在建议指令方面的有效性，这些指令不仅包含引人入胜的创意元素，而且还与输入图像和用户的初始提示保持高度相关性。]]></description>
      <guid>https://arxiv.org/abs/2406.00121</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>推进耳朵生物识别技术：通过深度学习提高准确性和稳健性</title>
      <link>https://arxiv.org/abs/2406.00135</link>
      <description><![CDATA[arXiv:2406.00135v1 公告类型：新
摘要：生物特征识别是一种基于个人独特的身体或行为特征验证个人身份的可靠方法，为密码或 PIN 等传统方法提供了一种安全的替代方案。本研究重点关注耳朵生物特征识别，利用其独特的功能提高准确性、可靠性和可用性。虽然过去的研究通常研究面部识别和指纹分析，但我们的研究证明了耳朵生物特征识别在克服面部表情和光照条件变化等限制方面的有效性。我们使用了两个数据集：AMI（来自 100 个人的 700 张图像）和 EarNV1.0（来自 164 个人的 28,412 张图像）。为了提高耳朵生物特征识别系统的准确性和稳健性，我们应用了各种技术，包括数据预处理和增强。我们的模型在 AMI 数据集上的测试准确率为 99.35%，在 EarNV1.0 数据集上的测试准确率为 98.1%，展示了我们的方法在基于耳朵生物特征精确识别个人方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.00135</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>多样化查询：区域引导变压器用于时间句子基础</title>
      <link>https://arxiv.org/abs/2406.00143</link>
      <description><![CDATA[arXiv:2406.00143v1 公告类型：新
摘要：时间句子定位是一项具有挑战性的任务，旨在定位与语言描述相关的时刻跨度。尽管最近基于 DETR 的模型通过利用多个可学习的时刻查询取得了显着的进展，但它们存在重叠和冗余的提议，导致预测不准确。我们将这种限制归因于缺乏与任务相关的指导，以使可学习查询服务于特定模式。此外，变量和开放词汇语言描述生成的复杂解决方案空间加剧了优化难度，使可学习查询更难自适应地相互区分。为了解决这个限制，我们提出了一个用于时间句子定位的区域引导转换器 (RGTR)，它使时刻查询多样化以消除重叠和冗余的预测。RGTR 不使用可学习的查询，而是采用一组锚点对作为时刻查询来引入显式的区域指导。每个锚点对负责特定时间区域的矩预测，从而降低优化难度并确保最终预测的多样性。此外，我们设计了一个 IoU 感知评分头来提高提案质量。大量实验证明了 RGTR 的有效性，在 QVHighlights、Charades-STA 和 TACoS 数据集上的表现优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.00143</guid>
      <pubDate>Tue, 04 Jun 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>Bootstrap3D：利用合成数据改进 3D 内容创作</title>
      <link>https://arxiv.org/abs/2406.00093</link>
      <description><![CDATA[arXiv:2406.00093v1 公告类型：新
摘要：近年来，用于 3D 内容创作的多视图扩散模型取得了显著进展。然而，与 2D 扩散模型相比，图像质量和提示跟随能力仍然存在很大差距。一个关键的瓶颈是缺乏具有详细字幕的高质量 3D 资产。为了应对这一挑战，我们提出了 Bootstrap3D，这是一个新颖的框架，可自动生成任意数量的多视图图像以协助训练多视图扩散模型。具体而言，我们引入了一个数据生成管道，该管道采用 (1) 2D 和视频扩散模型根据构造的文本提示生成多视图图像，以及 (2) 我们经过微调的 3D 感知 MV-LLaVA 来过滤高质量数据并重写不准确的字幕。利用这个管道，我们生成了 100 万张带有密集描述性字幕的高质量合成多视图图像，以解决高质量 3D 数据的短缺问题。此外，我们提出了一种训练时间步长重新安排 (TTR) 策略，该策略利用去噪过程来学习多视图一致性，同时保持原始的 2D 扩散先验。大量实验表明，Bootstrap3D 可以生成具有卓越美学质量、图像文本对齐和保持视图一致性的高质量多视图图像。]]></description>
      <guid>https://arxiv.org/abs/2406.00093</guid>
      <pubDate>Tue, 04 Jun 2024 06:19:59 GMT</pubDate>
    </item>
    </channel>
</rss>