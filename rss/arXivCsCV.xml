<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 05 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>使用学习表示对自动驾驶系统中的 2D 对象检测进行运行时自省</title>
      <link>https://arxiv.org/abs/2403.01172</link>
      <description><![CDATA[arXiv:2403.01172v1 公告类型：新
摘要：对周围环境中各种物体和道路使用者的可靠检测对于自动驾驶系统（ADS）的安全运行至关重要。尽管最近在开发基于深度神经网络 (DNN) 的高精度物体检测器方面取得了进展，但它们仍然容易出现检测错误，这可能会在 ADS 等安全关键应用中导致致命后果。解决这个问题的有效方法是为系统配备运行时监控，在自治系统的背景下称为内省。受此启发，我们引入了一种新颖的自省解决方案，该解决方案在帧级别运行，用于基于 DNN 的 2D 对象检测，并利用神经网络激活模式。所提出的方法使用几种不同的模式预处理对象检测器主干的神经激活模式。为了提供广泛的比较分析和公平比较，我们还使用在 KITTI 和 BDD 上评估的一级和两级目标检测器，采用并实现了几种最先进的 (SOTA) 内省机制，用于 2D 目标检测中的错误检测数据集。我们比较了所提出的解决方案在错误检测、数据集转换的适应性以及计算和内存资源要求方面的性能。我们的性能评估表明，所提出的内省解决方案优于 SOTA 方法，在 BDD 数据集中实现了 9% 至 17% 的遗漏错误率的绝对减少。]]></description>
      <guid>https://arxiv.org/abs/2403.01172</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:30 GMT</pubDate>
    </item>
    <item>
      <title>辅助任务增强弱监督语义分割的双亲和力学习</title>
      <link>https://arxiv.org/abs/2403.01156</link>
      <description><![CDATA[arXiv:2403.01156v1 公告类型：新
摘要：大多数现有的弱监督语义分割（WSSS）方法依赖类激活映射（CAM）使用图像级标签提取粗略的类特定定位图。先前的工作通常使用离线启发式阈值处理，将 CAM 地图与由通用预训练显着性模型生成的现成显着性地图相结合，以产生更准确的伪分割标签。我们提出了 AuxSegNet+，一个弱监督的辅助学习框架，用于探索这些显着性图中的丰富信息以及显着性检测和语义分割之间的显着任务间相关性。在提出的 AuxSegNet+ 中，显着性检测和多标签图像分类被用作辅助任务，以改进仅使用图像级真实标签的语义分割的主要任务。我们还提出了一种跨任务亲和力学习机制，用于从显着性和分割特征图中学习像素级亲和力。特别是，我们提出了一个跨任务双亲和力学习模块来学习成对和一元亲和力，它们用于通过聚合查询相关和查询无关的全局上下文来增强特定于任务的特征和预测以进行显着性检测和语义分割。学习到的跨任务成对亲和力还可以用于细化和传播 CAM 映射，以为这两个任务提供更好的伪标签。通过跨任务亲和性学习和伪标签更新来实现分割性能的迭代改进。大量实验证明了所提出的方法的有效性，并在具有挑战性的 PASCAL VOC 和 MS COCO 基准上获得了最先进的 WSSS 结果。]]></description>
      <guid>https://arxiv.org/abs/2403.01156</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:29 GMT</pubDate>
    </item>
    <item>
      <title>从事件提示中了解可疑异常情况以进行视频异常检测</title>
      <link>https://arxiv.org/abs/2403.01169</link>
      <description><![CDATA[arXiv:2403.01169v1 公告类型：新
摘要：大多数弱监督视频异常检测（WS-VAD）模型依赖于多实例学习，旨在区分正常和异常片段，而不指定异常类型。跨上下文的异常定义的模糊性会在检测异常包内的异常和正常片段时引入偏差。第一步向模型展示其异常的原因，提出了一个新颖的框架来指导从事件提示中学习可疑异常。给定潜在异常事件的文本提示字典和异常视频生成的字幕，可以计算它们之间的语义异常相似度，以识别每个视频片段的可疑异常事件。它支持新的多提示学习过程来约束所有视频的视觉语义特征，并提供一种标记伪异常以进行自我训练的新方法。为了证明有效性，在 XD-Violence、UCF-Crime、TAD 和 ShanghaiTech 四个数据集上进行了全面的实验和详细的消融研究。我们提出的模型在 AP 或 AUC 方面优于大多数最先进的方法（82.6%、87.7%、93.1% 和 97.4%）。此外，它在开放集和跨数据集情况下表现出了良好的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.01169</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:29 GMT</pubDate>
    </item>
    <item>
      <title>作为 2D 视频的动态 3D 点云序列</title>
      <link>https://arxiv.org/abs/2403.01129</link>
      <description><![CDATA[arXiv:2403.01129v1 公告类型：新
摘要：动态 3D 点云序列是动态现实环境最常见、最实用的表示方式之一。然而，它们在空间和时间域上的非结构化性质对有效和高效的处理提出了重大挑战。现有的深度点云序列建模方法通过开发复杂的时空点邻居分组和特征聚合方案来模仿成熟的2D视频学习机制，通常导致方法缺乏有效性、效率和表达能力。在本文中，我们提出了一种新颖的通用表示形式，称为 \textit{结构化点云视频} (SPCV)。直观地说，利用 3D 几何形状本质上是 2D 流形这一事实，SPCV 将点云序列重新组织为具有空间平滑性和时间一致性的 2D 视频，其中像素值对应于点的 3D 坐标。我们的 SPCV 表示的结构化性质允许无缝适应成熟的 2D 图像/视频技术，从而实现 3D 点云序列的高效且有效的处理和分析。为了实现这种重组，我们设计了一个自我监督的学习管道，该管道是几何正则化的，并由自我重建和变形场学习目标驱动。此外，我们还为低级和高级 3D 点云序列处理和分析任务构建了基于 SPCV 的框架，包括动作识别、时间插值和压缩。大量的实验证明了所提出的 SPCV 的多功能性和优越性，它有可能为非结构化 3D 点云序列的深度学习提供新的可能性。代码将在 https://github.com/ZENGYIMING-EAMON/SPCV 发布。]]></description>
      <guid>https://arxiv.org/abs/2403.01129</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:28 GMT</pubDate>
    </item>
    <item>
      <title>基于神经辐射场的全息术 [邀请]</title>
      <link>https://arxiv.org/abs/2403.01137</link>
      <description><![CDATA[arXiv:2403.01137v1 公告类型：新
摘要：本研究提出了一种基于神经辐射场（NeRF）技术生成全息图的新方法。在全息图计算中生成三维（3D）数据很困难。 NeRF 是一种基于体渲染从 2D 图像重建 3D 光场的最先进技术。 NeRF 可以快速预测不包含训练数据集的新视图图像。在这项研究中，我们直接根据 NeRF 从 2D 图像生成的 3D 光场构建了渲染管道，以便在合理的时间内使用深度神经网络生成全息图。该管道包含三个主要组件：NeRF、深度预测器和全息图生成器，所有组件均使用深度神经网络构建。该管道不包括任何物理计算。使用所提出的管道计算从任何方向观看的 3D 场景的预测全息图。给出了模拟和实验结果。]]></description>
      <guid>https://arxiv.org/abs/2403.01137</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:28 GMT</pubDate>
    </item>
    <item>
      <title>采用惯性 Bregman 交替线性化最小化的边缘引导低光图像增强</title>
      <link>https://arxiv.org/abs/2403.01142</link>
      <description><![CDATA[arXiv:2403.01142v1 公告类型：新
摘要：基于先验的低光图像增强方法通常面临着从暗淡图像中提取可用先验信息的挑战。为了克服这个限制，我们引入了一个简单而有效的 Retinex 模型，该模型具有所提出的边缘提取先验。更具体地说，我们设计了一个边缘提取网络来直接从低光图像中捕获精细边缘特征。基于 Retinex 理论，我们将低光图像分解为其照明和反射分量，并引入边缘引导的 Retinex 模型来增强低光图像。为了求解所提出的模型，我们提出了一种新颖的惯性 Bregman 交替线性化最小化算法。该算法解决了与边缘引导 Retinex 模型相关的优化问题，能够有效增强低光图像。通过严格的理论分析，我们建立了算法的收敛特性。此外，我们通过非凸优化理论证明了所提出的算法收敛到问题的驻点。此外，在多个真实世界的低光图像数据集上进行了广泛的实验，以证明该方案的效率和优越性。]]></description>
      <guid>https://arxiv.org/abs/2403.01142</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:28 GMT</pubDate>
    </item>
    <item>
      <title>通过图像感知属性减少进行视觉接地的对抗性测试</title>
      <link>https://arxiv.org/abs/2403.01118</link>
      <description><![CDATA[arXiv:2403.01118v1 公告类型：新
摘要：由于融合多种模态信息的优势，多模态学习越来越受到关注。视觉接地（VG）是多模态学习的一项基本任务，旨在通过自然语言表达来定位图像中的对象。由于任务的复杂性，确保 VG 模型的质量面临着巨大的挑战。在黑盒场景中，现有的对抗性测试技术通常无法充分利用这两种信息模式的潜力。他们通常仅根据图像或文本信息应用扰动，而忽略两种模式之间的关键相关性，这将导致测试预言失败或无法有效挑战 VG 模型。为此，我们提出了 PEELING，一种通过图像感知属性减少来进行 VG 模型的对抗性测试的文本扰动方法。其核心思想是减少原始表达式中与属性相关的信息，同时保证减少后的表达式仍然能够唯一地描述图像中的原始对象。为了实现这一点，PEELING 首先进行对象和属性提取和重组，以生成候选属性约简表达式。然后，通过使用视觉理解技术查询图像，选择准确描述原始对象的满意表达式，同时确保图像中没有其他对象满足该表达式。我们在最先进的 VG 模型（即 OFA-VG）上评估 PEELING，涉及三个常用数据集。结果显示，PEELING 生成的对抗性测试在多模式影响评分 (MMI) 中达到了 21.4%，比图像和文本的最新基线高出 8.2%--15.1%。]]></description>
      <guid>https://arxiv.org/abs/2403.01118</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:27 GMT</pubDate>
    </item>
    <item>
      <title>ELA：深度卷积神经网络的高效局部注意力</title>
      <link>https://arxiv.org/abs/2403.01123</link>
      <description><![CDATA[arXiv:2403.01123v1 公告类型：新
摘要：注意力机制由于能够有效增强深度神经网络的性能而在计算机视觉领域获得了显着的认可。然而，现有的方法通常难以有效地利用空间信息，或者即使有效利用空间信息，也会以减少通道维度或增加神经网络的复杂性为代价。为了解决这些限制，本文引入了一种高效局部注意力（ELA）方法，该方法以简单的结构实现了显着的性能提升。通过分析坐标注意力方法的局限性，我们发现了Batch Normalization缺乏泛化能力、降维对通道注意力的不利影响以及注意力生成过程的复杂性。为了克服这些挑战，我们建议结合一维卷积和组归一化特征增强技术。该方法通过有效地编码两个一维位置特征图来实现感兴趣区域的精确定位，无需降维，同时允许轻量级实现。我们在ELA中精心设计了三个超参数，产生了四个不同的版本：ELA-T、ELA-B、ELA-S和ELA-L，以满足图像分类、目标检测和语义等不同视觉任务的具体要求。分割。 ELA可以无缝集成到ResNet、MobileNet和DeepLab等深度CNN网络中。对 ImageNet、MSCOCO 和 Pascal VOC 数据集的广泛评估证明了所提出的 ELA 模块在上述所有三个视觉任务中相对于当前最先进的方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2403.01123</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:27 GMT</pubDate>
    </item>
    <item>
      <title>文本引导的可探索图像超分辨率</title>
      <link>https://arxiv.org/abs/2403.01124</link>
      <description><![CDATA[arXiv:2403.01124v1 公告类型：新
摘要：在本文中，我们介绍了开放域图像超分辨率解决方案的零样本文本引导探索问题。我们的目标是允许用户探索多样化的、语义上准确的重建，从而保持数据与不同大下采样因子的低分辨率输入的一致性，而无需针对这些特定的退化进行明确的训练。我们提出了两种零样本文本引导超分辨率方法 - i）修改文本到图像 \textit{T2I} 扩散模型的生成过程以促进与低分辨率输入的一致性，以及 ii）将语言指导纳入基于零样本扩散的恢复方法。我们表明，所提出的方法产生了多种解决方案，这些解决方案与文本提示提供的语义相匹配，同时保持数据与降级输入的一致性。我们评估了极端超分辨率任务所提出的基线，并展示了在恢复质量、多样性和解决方案的可探索性方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2403.01124</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:27 GMT</pubDate>
    </item>
    <item>
      <title>深度信息辅助单幅图像去雾协作互促网络</title>
      <link>https://arxiv.org/abs/2403.01105</link>
      <description><![CDATA[arXiv:2403.01105v1 公告类型：新
摘要：从单个模糊图像中恢复清晰图像是一个开放的逆问题。尽管已经取得了重大的研究进展，但大多数现有方法忽略了下游任务在促进上游去雾中所发挥的作用。从雾霾生成机制的角度来看，场景的深度信息与雾霾图像之间存在潜在的关系。基于此，我们提出了一种双任务协同相互促进框架来实现单个图像的去雾。该框架通过双任务交互机制将深度估计和去雾集成在一起，实现了两者性能的相互增强。为了实现这两个任务的联合优化，开发了一种具有差异感知的替代执行机制。一方面，提出了去雾结果的深度图与理想图像之间的差异感知，以促进去雾网络关注去雾的非理想区域。另一方面，通过提高有雾图像难以恢复区域的深度估计性能，去雾网络可以明确地使用有雾图像的深度信息来辅助清晰图像恢复。为了促进深度估计，我们建议利用去雾图像与地面实况之间的差异来引导深度估计网络聚焦于去雾的不理想区域。它允许去雾和深度估计以相辅相成的方式发挥各自的优势。实验结果表明，所提出的方法可以实现比最先进的方法更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.01105</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:26 GMT</pubDate>
    </item>
    <item>
      <title>通过扩散模型进行面部交换</title>
      <link>https://arxiv.org/abs/2403.01108</link>
      <description><![CDATA[arXiv:2403.01108v1 公告类型：新
摘要：本技术报告提出了一种基于扩散模型的框架，用于两个肖像图像之间的面部交换。基本框架由三个组件组成，即IP-Adapter、ControlNet和Stable Diffusion的修复管道，分别用于人脸特征编码、多条件生成和人脸修复。此外，我还引入了面部引导优化和基于 CodeFormer 的混合，以进一步提高生成质量。
  具体来说，我们采用了最近的轻量级定制方法（即DreamBooth-LoRA），通过以下方式保证身份一致性：1）使用罕见的标识符“sks”来表示源身份，2）注入源肖像的图像特征像文本特征一样进入每个交叉注意力层。然后借助Stable Diffusion强大的修复能力，以目标人像的canny图像和人脸检测标注为条件，指导ContorlNet的生成，并将源人像与目标人像对齐。为了进一步纠正面部对齐，我们添加面部引导损失以优化样本生成过程中的文本嵌入。]]></description>
      <guid>https://arxiv.org/abs/2403.01108</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:26 GMT</pubDate>
    </item>
    <item>
      <title>通过目标编码和分类损失的神经场分类器</title>
      <link>https://arxiv.org/abs/2403.01058</link>
      <description><![CDATA[arXiv:2403.01058v1 公告类型：新
摘要：神经场方法在计算机视觉和计算机图形学的各种长期任务中取得了巨大进展，包括新颖的视图合成和几何重建。由于现有的神经场方法试图预测一些基于坐标的连续目标值，例如神经辐射场（NeRF）的RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。然而，回归模型真的比神经场方法的分类模型更好吗？在这项工作中，我们尝试从机器学习的角度探讨神经领域这个非常基本但被忽视的问题。我们成功提出了一种新颖的神经场分类器（NFC）框架，它将现有的神经场方法制定为分类任务而不是回归任务。通过采用新颖的目标编码模块并优化分类损失，所提出的 NFC 可以轻松地将任意神经场回归器 (NFR) 转换为其分类变体。通过将连续回归目标编码为高维离散编码，我们自然地制定了多标签分类任务。大量的实验证明了 NFC 的有效性令人印象深刻，并且几乎不需要额外的计算成本。此外，NFC 还表现出对稀疏输入、损坏图像和动态场景的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2403.01058</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:25 GMT</pubDate>
    </item>
    <item>
      <title>通过不确定性量化从量化网络中提取可用预测以进行 OOD 检测</title>
      <link>https://arxiv.org/abs/2403.01076</link>
      <description><![CDATA[arXiv:2403.01076v1 公告类型：新
摘要：随着网络设计的进步和​​任务复杂性的增加，OOD 检测变得更加相关。识别给定网络的数据的哪些部分被错误分类已经变得与网络的整体性能一样有价值。我们可以通过量化来压缩模型，但它的性能损失很小。性能损失进一步需要导出网络预测的置信度估计。根据这种想法，我们引入了不确定性量化（UQ）技术来量化预训练视觉模型的预测的不确定性。随后，我们利用这些信息来提取有价值的预测，同时忽略不可信的预测。我们观察到，我们的技术可以避免高达 80% 的被忽略样本被错误分类。此处提供了相同的代码。]]></description>
      <guid>https://arxiv.org/abs/2403.01076</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:25 GMT</pubDate>
    </item>
    <item>
      <title>超越夜间能见度：红外和可见光图像的自适应多尺度融合</title>
      <link>https://arxiv.org/abs/2403.01083</link>
      <description><![CDATA[arXiv:2403.01083v1 公告类型：新
摘要：除了弱光之外，夜间图像还会因光效应（例如眩光、泛光灯等）而导致图像质量下降。然而，现有的夜间能见度增强方法普遍关注弱光区域，忽略甚至放大了光效应。为了解决这个问题，我们提出了一种具有红外和可见光图像的自适应多尺度融合网络（AMFusion），它根据不同的照明区域设计融合规则。首先，我们分别融合红外和可见光图像的空间特征和语义特征，前者用于调整光分布，后者用于提高检测精度。由此，我们获得了没有弱光和光效应的图像，从而提高了夜间物体检测的性能。其次，我们利用预先训练的主干提取的检测特征来指导语义特征的融合。因此，我们设计了一个检测引导的语义融合模块（DSFM）来弥合检测和语义特征之间的领域差距。第三，我们提出了一种新的照明损失来约束具有正常光强度的融合图像。实验结果证明了 AMFusion 的优越性，具有更好的视觉质量和检测精度。源代码将在同行评审过程后发布。]]></description>
      <guid>https://arxiv.org/abs/2403.01083</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:25 GMT</pubDate>
    </item>
    <item>
      <title>G3DR：ImageNet 中的生成 3D 重建</title>
      <link>https://arxiv.org/abs/2403.00939</link>
      <description><![CDATA[arXiv:2403.00939v1 公告类型：新
摘要：我们在 ImageNet 中引入了一种新颖的 3D 生成方法，即生成 3D 重建（G3DR），能够从单个图像生成多样化且高质量的 3D 对象，解决了现有方法的局限性。我们框架的核心是一种新颖的深度正则化技术，可以生成具有高几何保真度的场景。 G3DR 还利用预训练的语言视觉模型（例如 CLIP）来实现新视图中的重建并提高几代人的视觉真实感。此外，G3DR 设计了一个简单但有效的采样程序，以进一步提高世代质量。 G3DR 基于类或文本条件提供多样化且高效的 3D 资产生成。尽管很简单，但 G3DR 能够击败最先进的方法，在感知指标上比它们提高了 22%，在几何分数上比它们提高了 90%，而只需要一半的训练时间。代码可在 https://github.com/preddy5/G3DR 获取]]></description>
      <guid>https://arxiv.org/abs/2403.00939</guid>
      <pubDate>Tue, 05 Mar 2024 15:13:24 GMT</pubDate>
    </item>
    </channel>
</rss>