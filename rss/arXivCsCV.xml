<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 26 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>InterFusion：文本驱动的 3D 人机交互生成</title>
      <link>https://arxiv.org/abs/2403.15612</link>
      <description><![CDATA[arXiv:2403.15612v1 公告类型：新
摘要：在这项研究中，我们解决了以零镜头文本到 3D 方式从文本描述生成 3D 人机交互 (HOI) 的复杂任务。我们确定并解决了两个关键挑战：HOI 中直接文本到 3D 方法的结果不令人满意，这主要是由于缺乏配对的文本交互数据，以及同时生成具有复杂空间关系的多个概念的固有困难。为了有效解决这些问题，我们推出了 InterFusion，这是一个专为 HOI 生成而设计的两阶段框架。 InterFusion 涉及从文本导出的人体姿势估计作为几何先验，这简化了文本到 3D 的转换过程，并为准确的对象生成引入了额外的约束。在第一阶段，InterFusion 从描述各种交互的合成图像数据集中提取 3D 人体姿势，随后将这些姿势映射到交互描述。 InterFusion 的第二阶段利用了文本转 3D 生成的最新发展，能够制作逼真且高质量的 3D HOI 场景。这是通过局部全局优化过程实现的，其中人体和物体的生成分别进行优化，并与整个场景的全局优化共同细化，确保无缝且上下文连贯的集成。我们的实验结果证实，InterFusion 在 3D HOI 生成方面显着优于现有的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2403.15612</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>语义高斯：使用 3D 高斯泼溅进行开放词汇场景理解</title>
      <link>https://arxiv.org/abs/2403.15624</link>
      <description><![CDATA[arXiv:2403.15624v1 公告类型：新
摘要：开放词汇 3D 场景理解对计算机视觉提出了重大挑战，在实体代理和增强现实系统中具有广泛的应用。以前的方法采用神经辐射场 (NeRF) 来分析 3D 场景。在本文中，我们介绍了 SemanticGaussians，一种基于 3D 高斯分布的新型开放词汇场景理解方法。我们的关键想法是将预先训练的 2D 语义提炼为 3D 高斯。我们设计了一种多功能投影方法，将预训练图像编码器中的各种 2D 语义特征映射到 3D 高斯的新颖语义组件，而无需 NeRF 所需的额外训练。我们进一步构建了一个 3D 语义网络，可以直接从原始 3D 高斯函数中预测语义成分，以进行快速推理。我们探索了语义高斯的几种应用：ScanNet-20 上的语义分割，与之前的开放词汇场景理解同行相比，我们的方法获得了 4.2% mIoU 和 4.0%macc 的改进；对象部分分割、场景编辑和时空分割，在 2D 和 3D 基线上具有更好的定性结果，突出了其支持各种下游任务的多功能性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.15624</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>FairerCLIP：使用 RKHS 中的函数消除 CLIP 的零样本预测偏差</title>
      <link>https://arxiv.org/abs/2403.15593</link>
      <description><![CDATA[arXiv:2403.15593v1 公告类型：新
摘要：大型预训练视觉语言模型（例如 CLIP）提供了文本和图像的紧凑且通用的表示，这些表示在多个下游零样本预测任务中表现出明显的有效性。然而，由于其训练过程的性​​质，这些模型有可能 1) 传播或放大训练数据中的社会偏见，2) 学会依赖虚假特征。本文提出了 FairerCLIP，这是一种使 CLIP 的零样本预测对虚假相关性更加公平和鲁棒的通用方法。我们提出了在再现内核希尔伯特空间（RKHS）中联合消除 CLIP 图像和文本表示偏差的问题，这具有多种好处：1）灵活性：与专门用于在有或没有真实标签的情况下学习的现有方法不同，FairerCLIP 是适应两种情况下的学习。 2) 易于优化：FairerCLIP 适合于涉及封闭式求解器的迭代优化，这使得训练速度比现有方法快 4 倍 - 10 倍。 3) 样本效率：在样本有限的条件下，FairerCLIP 在完全失败时显着优于基线。并且，4）性能：根据经验，FairerCLIP 在基准公平性和虚假相关数据集上相对于各自的基线实现了可观的准确性提升。]]></description>
      <guid>https://arxiv.org/abs/2403.15593</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>基于梯度的黑盒显着图生成的前向学习</title>
      <link>https://arxiv.org/abs/2403.15603</link>
      <description><![CDATA[arXiv:2403.15603v1 公告类型：新
摘要：基于梯度的显着性图被广泛用于解释深度神经网络决策。然而，随着模型变得更深、更黑盒，例如在 ChatGPT 等闭源 API 中，计算梯度变得具有挑战性，阻碍了传统的解释方法。在这项工作中，我们引入了一种新颖的统一框架，用于估计黑盒设置中的梯度并生成显着图来解释模型决策。我们采用似然比方法来估计输出到输入的梯度，并将其用于显着图生成。此外，我们提出了分块计算技术来提高估计精度。黑盒设置中的大量实验验证了我们方法的有效性，证明了生成的显着性图的准确梯度估计和可解释性。此外，我们通过应用它来解释 GPT-Vision 来展示我们的方法的可扩展性，揭示基于梯度的解释方法在大型、闭源和黑盒模型时代的持续相关性。]]></description>
      <guid>https://arxiv.org/abs/2403.15603</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>高效组装标准化层和正则化以实现联合域泛化</title>
      <link>https://arxiv.org/abs/2403.15605</link>
      <description><![CDATA[arXiv:2403.15605v1 公告类型：新
摘要：域转移是机器学习中的一个棘手问题，它会导致模型在看不见的域上进行测试时出现性能下降。联合域泛化（FedDG）尝试以保护隐私的方式使用协作客户端来训练全局模型，该模型可以很好地泛化到可能存在域转移的未见过的客户端。然而，大多数现有的 FedDG 方法要么会导致数据泄露的额外隐私风险，要么会导致客户端通信和计算的巨大成本，这是联邦学习范式中的主要问题。为了规避这些挑战，我们在这里引入了一种新颖的 FedDG 架构方法，即 gPerXAN，它依赖于与引导正则化器配合使用的归一化方案。特别是，我们精心设计了个性化显式组装标准化，以强制客户端模型有选择地过滤偏向本地数据的特定于域的特征，同时保留对这些特征的区分。然后，我们结合一个简单而有效的正则化器来指导这些模型直接捕获全局模型的分类器可以利用的域不变表示。对两个基准数据集（即 PACS 和 Office-Home）以及真实世界医学数据集 Camelyon17 的广泛实验结果表明，我们提出的方法在解决这一特定问题方面优于其他现有方法。]]></description>
      <guid>https://arxiv.org/abs/2403.15605</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>使用预先训练的文本到图像模型增强 3D 网格纹理多视图一致性的优化框架</title>
      <link>https://arxiv.org/abs/2403.15559</link>
      <description><![CDATA[arXiv:2403.15559v1 公告类型：新
摘要：使用预训练的文本到图像模型进行 3D 网格纹理化的一个基本问题是确保多视图一致性。最先进的方法通常使用扩散模型来聚合多视图输入，其中常见问题是聚合步骤中的平均操作引起的模糊或局部特征的不一致。本文介绍了一个优化框架，该框架分四个阶段进行以实现多视图一致性。具体来说，第一阶段使用 MV 一致扩散过程从预定义的一组视点生成一组超完备的 2D 纹理。第二阶段选择在覆盖底层 3D 模型的同时相互一致的视图子集。我们展示了如何通过求解半定规划来实现这一目标。第三阶段执行非刚性对齐以跨重叠区域对齐所选视图。第四阶段解决 MRF 问题，将每个网格面与选定的视图相关联。特别是，迭代第三和第四阶段，第四阶段中获得的切割鼓励第三阶段中的非刚性对齐以集中于靠近切割的区域。实验结果表明，我们的方法在定性和定量上都显着优于基线方法。]]></description>
      <guid>https://arxiv.org/abs/2403.15559</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>U-ARE-ME：曼哈顿环境中的不确定性旋转估计</title>
      <link>https://arxiv.org/abs/2403.15583</link>
      <description><![CDATA[arXiv:2403.15583v1 公告类型：新
摘要：从单个图像估计相机旋转是一项具有挑战性的任务，通常需要深度数据和/或相机内在参数，而这些数据通常不适用于野外视频。尽管惯性测量单元 (IMU) 等外部传感器可以提供帮助，但它们经常会出现漂移，并且不适用于非惯性参考系。我们提出了 U-ARE-ME，一种估计相机旋转以及未校准 RGB 图像的不确定性的算法。使用曼哈顿世界假设，我们的方法利用单图像表面法线预测中编码的每像素几何先验，并对 SO(3) 流形执行优化。给定图像序列，我们可以使用每帧旋转估计及其不确定性来执行多帧优化，从而实现鲁棒性和时间一致性。我们的实验表明，U-ARE-ME 的性能与 RGB-D 方法相当，并且比基于稀疏特征的 SLAM 方法更稳健。我们鼓励读者观看 https://callum-rhodes.github.io/U-ARE-ME 上的随附视频，以直观地了解我们的方法。]]></description>
      <guid>https://arxiv.org/abs/2403.15583</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>MedPromptX：用于胸部 X 射线诊断的接地多模式提示</title>
      <link>https://arxiv.org/abs/2403.15585</link>
      <description><![CDATA[arXiv:2403.15585v1 公告类型：新
摘要：胸部 X 射线图像通常用于预测急性和慢性心肺疾病，但由于电子健康记录 (EHR) 不完整，将其与结构化临床数据整合的努力面临挑战。本文介绍了 \textbf{MedPromptX}，这是第一个集成多模态大语言模型 (MLLM)、少镜头提示 (FP) 和视觉基础 (VG) 的模型，将图像与 EHR 数据相结合进行胸部 X 射线诊断。利用预先训练的 MLLM 来补充缺失的 EHR 信息，从而全面了解患者的病史。此外，FP 减少了对 MLLM 进行大量培训的必要性，同时有效解决了幻觉问题。然而，确定小样本样本的最佳数量和选择高质量候选样本的过程可能很繁重，但却深刻地影响了模型的性能。因此，我们提出了一种新技术，可以动态地细化少量数据，以便实时调整新的患者场景。此外，VG 有助于将模型的注意力集中在 X 射线图像中的相关感兴趣区域，从而增强异常的识别。我们发布了 MedPromptX-VQA，这是一个新的上下文视觉问答数据集，包含源自 MIMIC-IV 和 MIMIC-CXR 数据库的交错图像和 EHR 数据。结果证明了 MedPromptX 的 SOTA 性能，与基线相比，F1 分数提高了 11%。代码和数据可在 \url{https://github.com/BioMedIA-MBZUAI/MedPromptX} 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.15585</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>Pixel-GS：使用像素感知梯度进行 3D 高斯泼溅的密度控制</title>
      <link>https://arxiv.org/abs/2403.15530</link>
      <description><![CDATA[arXiv:2403.15530v1 公告类型：新
摘要：3D 高斯泼溅 (3DGS) 展示了令人印象深刻的新颖视图合成结果，同时提高了实时渲染性能。然而，它严重依赖于初始点云的质量，导致初始化点不足的区域出现模糊和针状伪影。这主要归因于3DGS中的点云生长条件仅考虑可观察视点的点的平均梯度大小，因此无法生长对于许多视点可观察的大高斯，而其中许多仅被边界覆盖。为此，我们提出了一种名为 Pixel-GS 的新方法，在计算生长条件期间考虑每个视图中高斯覆盖的像素数量。我们将覆盖的像素数作为权重，对不同视图的梯度进行动态平均，从而可以促进大高斯的增长。结果，可以更有效地生长初始化点不足的区域内的点，从而实现更准确和详细的重建。此外，我们提出了一种简单而有效的策略，根据到相机的距离缩放梯度场，以抑制相机附近飞蚊的生长。大量的定性和定量实验表明，我们的方法在具有挑战性的 Mip-NeRF 360 和 Tanks &amp; Temples 数据集上实现了最先进的渲染质量，同时保持实时渲染速度。]]></description>
      <guid>https://arxiv.org/abs/2403.15530</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>用于单眼深度估计的基于语言的深度提示</title>
      <link>https://arxiv.org/abs/2403.15551</link>
      <description><![CDATA[arXiv:2403.15551v1 公告类型：新
摘要：单目深度估计 (MDE) 本质上是不明确的，因为给定的图像可能来自许多不同的 3D 场景，反之亦然。为了解决这种模糊性，MDE 系统必须对给定输入最可能的 3D 场景做出假设。这些假设可以是明确的，也可以是隐含的。在这项工作中，我们演示了如何使用自然语言作为关于世界结构的明确先验的来源。假设人类语言对各种对象在深度空间中的可能分布进行编码。我们首先证明语言模型在训练期间对这种隐式偏差进行编码，并且可以使用非常简单的学习方法来提取它。然后，我们证明，可以使用现成的实例分割模型来提供该预测作为 MDE 系统的明确假设源，该模型提供用作语言模型输入的标签。我们在 NYUD2 数据集上展示了我们的方法的性能，与基线和随机对照相比显示出改进。]]></description>
      <guid>https://arxiv.org/abs/2403.15551</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>EC-IoU：通过以自我为中心的 Intersection-over-Union 为物体探测器提供安全导向</title>
      <link>https://arxiv.org/abs/2403.15474</link>
      <description><![CDATA[arXiv:2403.15474v1 公告类型：新
摘要：本文通过一种新颖的以自我为中心的交集（EC-IoU）测量提出了面向安全的物体检测，解决了在安全关键领域应用最先进的基于学习的感知模型时的实际问题比如自动驾驶。具体来说，我们提出了一种加权机制来改进广泛使用的 IoU 度量，使其能够为从自我代理的角度覆盖地面真实对象的较近点的预测分配更高的分数。所提出的 EC-IoU 测量可用于典型的评估过程，为下游任务选择具有更高安全相关性能的目标检测器。它还可以集成到常见的损失函数中以进行模型微调。虽然着眼于安全性，但我们对 KITTI 数据集的实验表明，在 EC-IoU 上训练的模型的性能在平均精度方面也优于在 IoU 上训练的变体。]]></description>
      <guid>https://arxiv.org/abs/2403.15474</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>学习推断视觉概念的生成模板程序</title>
      <link>https://arxiv.org/abs/2403.15476</link>
      <description><![CDATA[arXiv:2403.15476v1 公告类型：新
摘要：人们从几个例子中掌握灵活的视觉概念。我们探索了一个神经符号系统，该系统学习如何推断以领域通用方式捕获视觉概念的程序。我们引入模板程序：来自特定领域语言的编程表达式，指定输入概念常见的结构和参数模式。我们的框架支持多个与概念相关的任务，包括少数镜头生成和通过解析进行联合分割。我们开发了一种学习范例，使我们能够训练直接从包含概念分组的视觉数据集推断模板程序的网络。我们在多个视觉领域进行实验：2D 布局、Omniglot 字符和 3D 形状。我们发现我们的方法优于特定于任务的替代方法，并且在其存在的有限领域中与特定于领域的方法竞争。]]></description>
      <guid>https://arxiv.org/abs/2403.15476</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>关于使用统计技术检测视觉模型中的异常或不均匀数据</title>
      <link>https://arxiv.org/abs/2403.15497</link>
      <description><![CDATA[arXiv:2403.15497v1 公告类型：新
摘要：分布外数据和异常输入是当今机器学习系统的漏洞，通常导致系统做出错误的预测。这些模型所使用的数据范围广泛，使得检测非典型输入成为一项困难而重要的任务。我们评估了一种工具，即本福德定律，作为一种用于量化真实输入和损坏输入之间差异的方法。我们相信，在许多设置中，它可以充当异常数据点和分布外数据信号的过滤器。我们希望就这些应用以及该技术尚未充分探索的其他领域展开讨论。]]></description>
      <guid>https://arxiv.org/abs/2403.15497</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>揭示不断变化的世界中的异常：持续学习中像素级异常检测的基准</title>
      <link>https://arxiv.org/abs/2403.15463</link>
      <description><![CDATA[arXiv:2403.15463v1 公告类型：新
摘要：异常检测是许多现实应用中的一个相关问题，特别是在处理图像时。然而，很少有人关注输入数据分布随时间变化的问题，这可能会导致性能显着下降。在这项研究中，我们研究了持续学习设置中的像素级异常检测问题，其中新数据随着时间的推移到达，目标是在新数据和旧数据上表现良好。我们实施了几种最先进的技术来解决经典设置中的异常检测问题，并使它们适应持续学习设置。为了验证这些方法，我们使用具有基于像素的异常的真实图像数据集来提供可靠的基准，并作为该领域进一步发展的基础。我们提供了全面的分析，讨论了哪些异常检测方法以及哪些方法系列似乎更适合持续学习设置。]]></description>
      <guid>https://arxiv.org/abs/2403.15463</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:42 GMT</pubDate>
    </item>
    <item>
      <title>利用超分辨率成像识别低分辨率模糊车牌：Real-ESRGAN、A-ESRGAN 和 StarSRGAN 的比较研究</title>
      <link>https://arxiv.org/abs/2403.15466</link>
      <description><![CDATA[arXiv:2403.15466v1 公告类型：新
摘要：随着技术的蓬勃发展，车牌识别技术现在可以很好地应用于各种场景，例如道路监控、被盗车辆跟踪、停车场出入口检测等。然而，这些应用程序正常运行的前提是车牌必须足够“清晰”，以便系统能够识别出正确的车牌号。如果车牌由于一些外界因素而变得模糊，那么识别的准确性就会大大降低。虽然台湾有很多道路监控摄像头，但大多数摄像头的质量都不好，经常导致由于照片分辨率低而无法识别车牌号。因此，本研究重点研究利用超分辨率技术来处理模糊车牌。本研究将主要对Real-ESRGAN、A-ESRGAN和StarSRGAN这三种超分辨率模型进行微调，并比较它们在增强车牌照片分辨率和实现准确车牌识别方面的效果。通过比较不同的超分辨率模型，希望找到最适合该任务的模型，为未来的研究人员提供有价值的参考。]]></description>
      <guid>https://arxiv.org/abs/2403.15466</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:42 GMT</pubDate>
    </item>
    </channel>
</rss>