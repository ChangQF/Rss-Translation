<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Wed, 13 Dec 2023 15:14:23 GMT</lastBuildDate>
    <item>
      <title>SkyScenes：用于航空场景理解的综合数据集。 （arXiv：2312.06719v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06719</link>
      <description><![CDATA[现实世界的航空场景理解因缺乏数据集而受到限制
包含在不同条件下整理的密集注释图像。到期的
在受控现实世界中获取此类图像的固有挑战
设置中，我们提出了 SkyScenes，这是一个密集注释的航空数据集
从无人机 (UAV) 角度捕获的图像。我们小心翼翼地
整理来自 CARLA 的 SkyScenes 图像，以全面捕捉跨领域的多样性
布局（城市和乡村地图）、天气状况、一天中的时间、俯仰角
以及具有相应语义、实例和深度注释的高度。
通过使用 SkyScenes 的实验，我们表明 (1) 模型训练
SkyScenes 可以很好地推广到不同的现实世界场景，(2)
使用 SkyScenes 数据对真实图像进行训练可以提高现实世界的性能，
(3) SkyScenes 中的受控变化可以提供有关模型如何运作的见解
响应视点条件的变化，以及（4）纳入额外的
传感器模式（深度）可以提高对空中场景的理解。
]]></description>
      <guid>http://arxiv.org/abs/2312.06719</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:23 GMT</pubDate>
    </item>
    <item>
      <title>分离和增强：Text2Image 扩散模型的组合微调。 （arXiv：2312.06712v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06712</link>
      <description><![CDATA[尽管基于扩散的文本到图像最近取得了重大进展
(T2I) 模型，当前系统仍然无法确保良好的性能
与文本提示对齐的构图生成，特别是对于
多对象生成。这项工作阐明了造成这种情况的根本原因
错位，查明与低注意力激活分数相关的问题以及
掩模重叠。虽然之前的研究工作已经单独解决了这些问题
问题上，我们认为整体方法至关重要。因此，我们提出两个
新颖的目标，单独损失和增强损失，减少对象
分别掩盖重叠和最大化注意力分数。我们的方法有所不同
摆脱传统的测试时间适应技术，专注于微调
关键参数，增强可扩展性和通用性。
综合评估表明我们的模型在以下方面具有优越的性能
图像真实感、文本图像对齐和适应性方面，特别是
超越突出的基线。最终，这项研究为
T2I 扩散模型具有增强的成分能力和更广泛的
适用性。该项目网页位于
https://zpbao.github.io/projects/SepEn/。
]]></description>
      <guid>http://arxiv.org/abs/2312.06712</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:22 GMT</pubDate>
    </item>
    <item>
      <title>TeTriRF：用于高效自由视点视频的时间三平面辐射场。 （arXiv：2312.06713v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06713</link>
      <description><![CDATA[神经辐射场 (NeRF) 通过以下方式彻底改变了视觉媒体领域
提供逼真的自由视点视频 (FVV) 体验，
观众无与伦比的沉浸感和互动性。然而，该技术的
显着的存储需求和计算复杂性
生成和渲染目前限制了其更广泛的应用。要关闭此
差距，本文提出了时间三平面辐射场（TeTriRF），一种新颖的
显着减少自由视点视频存储空间的技术
（FVV），同时保持低成本的生成和渲染。 TeTriRF 推出了
具有三平面和体素网格的混合表示，以支持扩展到
具有复杂运动或快速变化的长时间序列和场景。我们
提出量身定制的团体培训方案，以实现高培训效率
并产生时间一致的、低熵的场景表示。
利用表示的这些属性，我们引入了压缩
具有现成视频编解码器的管道，实现了更少的数量级
与最先进的技术相比，存储大小。我们的实验表明
TeTriRF 可以通过更高的压缩率实现具有竞争力的质量。
]]></description>
      <guid>http://arxiv.org/abs/2312.06713</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:22 GMT</pubDate>
    </item>
    <item>
      <title>从层分布式神经表示的谱聚类中破译“什么”和“哪里”的视觉路径。 （arXiv：2312.06716v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06716</link>
      <description><![CDATA[我们提出了一种分析包含在
神经网络的激活，允许提取空间布局和
根据大型预训练视觉模型的行为进行语义分割。
与之前的工作不同，我们的方法对网络的整体分析
激活状态，利用所有层的功能并消除需要
猜测模型的哪一部分包含相关信息。动机是
经典的谱聚类，我们根据
优化目标涉及一组亲和矩阵，每个矩阵由
比较不同层内的特征。解决这个优化问题
使用梯度下降允许我们的技术从单个图像扩展到
数据集级分析，包括后者的图像内和图像间分析
关系。分析预先训练的生成变压器提供了见解
进入此类模型学习的计算策略。将亲和力等同于
跨注意力层的关键查询相似性产生编码场景的特征向量
空间布局，而通过值向量相似度定义亲和力产生
编码对象身份的特征向量。这个结果表明 key 和 query
向量根据空间接近度协调注意力信息流
（“where”路径），而值向量则细化语义类别
表示（“什么”途径）。
]]></description>
      <guid>http://arxiv.org/abs/2312.06716</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:22 GMT</pubDate>
    </item>
    <item>
      <title>SIFU：用于现实世界可用衣服人体重建的侧视条件隐函数。 （arXiv：2312.06704v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06704</link>
      <description><![CDATA[从单个图像创建高质量的着装人体 3D 模型
现实世界的应用至关重要。尽管最近取得了进展，但准确地说
在野外重建复杂姿势或穿着宽松衣服的人类
图像以及预测未见过区域的纹理仍然是一个重要的
挑战。先前方法的一个关键限制是它们的先验不足
从 2D 过渡到 3D 以及纹理预测的指导。作为回应，
我们介绍 SIFU（真实世界的侧视条件隐式函数）
可用衣服人体重建），一种结合侧视图的新颖方法
使用 3D 一致纹理细化管道解耦 Transformer.SIFU
使用 SMPL-X 在变压器内采用交叉注意力机制
法线作为查询，以在过程中有效地解耦侧视图特征
将 2D 特征映射到 3D。该方法不仅提高了检测精度
3D 模型以及它们的鲁棒性，特别是当 SMPL-X 估计不正确时
完美的。我们的纹理细化过程利用基于文本到图像的扩散
在为不可见视图生成逼真且一致的纹理之前。
通过大量的实验，SIFU 在几何和空间方面都超越了 SOTA 方法。
纹理重建，展示复杂场景下增强的鲁棒性
实现前所未有的倒角和 P2S 测量。我们的方法延伸到
3D打印和场景构建等实际应用，展示
它在现实场景中的广泛实用性。项目页面
https://river-zhang.github.io/SIFU-projectpage/ 。
]]></description>
      <guid>http://arxiv.org/abs/2312.06704</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:21 GMT</pubDate>
    </item>
    <item>
      <title>UNeR3D：在无监督重建中从 2D 图像生成多功能且可扩展的 3D RGB 点云。 （arXiv：2312.06706v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06706</link>
      <description><![CDATA[在 2D 图像的 3D 重建领域，一个持续存在的挑战是
在没有 3D Ground Truth 数据的情况下实现高精度重建
依赖。我们提出了 UNeR3D，这是一种开创性的无监督方法，它设定了
仅从 2D 视图生成详细 3D 重建的新标准。
我们的模型显着降低了与监督相关的培训成本
方法并将 RGB 着色引入 3D 点云，丰富了
视觉体验。采用颜色反距离加权技术
渲染时，UNeR3D 确保无缝色彩过渡，增强视觉效果
保真度。我们模型的灵活架构支持任意数量的训练
视图的数量，并且独特的是，它不受所使用的视图数量的限制
在训练期间执行重建时。它可以推断出任意
推理过程中的视图计数，提供无与伦比的多功能性。
此外，该模型的连续空间输入域允许生成
任何所需分辨率的点云，从而能够创建
高分辨率 3D RGB 点云。我们巩固重建过程
具有新颖的多视图几何损失和颜色损失，证明我们的
该模型在单视图输入及其他方面表现出色，从而重塑了
3D 视觉中的无监督学习。我们的贡献标志着实质性的飞跃
3D 视觉向前发展，为跨领域的内容创作提供新视野
应用程序。代码可在 https://github.com/HongbinLin3589/UNeR3D 获取。
]]></description>
      <guid>http://arxiv.org/abs/2312.06706</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:21 GMT</pubDate>
    </item>
    <item>
      <title>用于基于扩散的视频编辑的中性编辑框架。 （arXiv：2312.06708v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06708</link>
      <description><![CDATA[文本条件图像编辑已成功应用于各种类型的编辑
基于扩散框架。不幸的是，这一成功并没有延续下去
到视频，这仍然具有挑战性。现有的视频编辑系统
仍然局限于风格转换和对象等僵化类型的编辑
覆盖。为此，本文提出中性编辑（NeuEdit）框架
通过改变人/物体的运动来实现复杂的非刚性编辑
在视频中，这是以前从未尝试过的。 NeuEdit引入一个概念
“中和”增强了基于扩散的调谐编辑过程
通过利用输入视频和文本以与模型无关的方式编辑系统
无需任何其他辅助工具（例如视觉蒙版、视频字幕）。
对大量视频进行的广泛实验证明了适应性和
NeuEdit 框架的有效性。我们的工作网站已上线
这里：https://neuedit.github.io
]]></description>
      <guid>http://arxiv.org/abs/2312.06708</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:21 GMT</pubDate>
    </item>
    <item>
      <title>AM-RADIO：凝聚模型——将所有领域缩减为一个。 （arXiv：2312.06709v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06709</link>
      <description><![CDATA[最近出现了一些视觉基础模型（VFM）
许多下游任务的骨干。像 CLIP、DINOv2、SAM 这样的 VFM
训练有明确的目标，表现出不同的独特特征
下游任务。我们发现，尽管概念上存在差异，但这些
通过多老师可以有效地将模型合并为统一模型
蒸馏。我们将这种方法命名为 AM-RADIO（聚合模型——减少所有
域合而为一）。这种集成方法不仅超越了性能
个体教师模型的同时也融合了他们的独特特征，
例如零样本视觉语言理解、详细的像素级
理解和开放词汇分割功能。为了追求
最硬件效率最高的骨干网，我们评估了我们的众多架构
使用相同的训练配方的多教师蒸馏管道。这导致
开发超越性能的新颖架构（E-RADIO）
其前身的速度至少比教师模型快 7 倍。我们的
全面的基准测试流程涵盖下游任务，包括 ImageNet
分类、ADE20k 语义分割、COCO 对象检测和
LLaVa-1.5 框架。

代码：https://github.com/NVlabs/RADIO
]]></description>
      <guid>http://arxiv.org/abs/2312.06709</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:21 GMT</pubDate>
    </item>
    <item>
      <title>利用生成语言模型进行视频语言联合学习中的弱监督句子成分分析。 （arXiv：2312.06699v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06699</link>
      <description><![CDATA[对文本数据的透彻理解是学习的基本要素
多模态视频分析任务。然而，最近的研究表明，
当前模型无法实现对文本数据的全面理解
在目标下游任务的培训期间。与前一个正交
为了解决这个限制，我们假设理解其重要性
根据目标任务调整句子成分可以潜在地增强
模型的性能。因此，我们利用预训练的知识
大语言模型（LLM）从原始文本生成文本样本，
针对特定的句子成分。我们提出弱监督
重要性估计模块，用于计算的相对重要性
组件并利用它们来改进不同的视频语言任务。通过
严格的定量分析，我们提出的方法表现出显着的
多个视频语言任务的改进。特别是，我们的方法
显着增强了视频文本检索，相对提高了 8.3%
视频到文本的检索和文本到视频检索的检索比基线提高了 1.4\%
R@1。此外，在视频时刻检索中，平均 mAP 显示出相对
不同基线的改进范围为 2.0% 到 13.7%。
]]></description>
      <guid>http://arxiv.org/abs/2312.06699</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:20 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶系统的动态对抗攻击。 （arXiv：2312.06701v1 [cs.RO]）</title>
      <link>http://arxiv.org/abs/2312.06701</link>
      <description><![CDATA[本文介绍了一种攻击机制来挑战
自动驾驶系统。具体来说，我们操纵决策
通过动态显示对抗性来了解自动驾驶车辆的流程
安装在另一辆移动车辆上的屏幕上的补丁。这些补丁是
优化以欺骗目标检测模型来错误分类目标
物体，例如交通标志。这种操纵具有重大影响
用于关键的多车辆交互，例如十字路口和车道
变化，这对于安全高效的自动驾驶系统至关重要。
特别是，我们做出了四项主要贡献。首先给大家介绍一部小说
对抗性攻击方法，其中补丁与其目标不在同一位置，
实现更通用和隐秘的攻击。此外，我们的方法利用
屏幕上显示动态补丁，允许自适应更改和
运动，增强攻击的灵活性和性能。为此，我们
设计一个屏幕图像转换网络（SIT-Net），它模拟
环境对显示图像的影响，缩小了之间的差距
模拟和真实场景。此外，我们整合位置损失
将术语纳入对抗性训练过程，以提高对抗性训练的成功率
动态攻击。最后，我们将焦点从仅仅攻击感性
影响自动驾驶系统决策算法的系统。
我们的实验证明了这种动态的首次成功实施
现实世界自动驾驶场景中的对抗性攻击，铺平了道路
表彰稳健、安全的自动驾驶领域的进步。
]]></description>
      <guid>http://arxiv.org/abs/2312.06701</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:20 GMT</pubDate>
    </item>
    <item>
      <title>OpenSD：统一开放词汇分割和检测。 （arXiv：2312.06703v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06703</link>
      <description><![CDATA[最近，一些开放词汇方法被提出，采用
统一的架构来处理通用的分割和检测任务。
然而，由于以下原因，它们的性能仍然落后于特定任务模型：
不同任务之间的冲突及其开放词汇能力是
由于 CLIP 的使用不充分而受到限制。为了应对这些挑战，我们
提出了一个基于 Transformer 的通用框架，缩写为 OpenSD，它
利用相同的架构和网络参数来处理开放词汇表
分割和检测任务。首先我们介绍一个解耦的解码器
缓解事物与人员之间语义冲突的学习策略
类别，以便可以更有效地学习每个单独的任务
相同的框架。二、更好地利用CLIP进行端到端分段
和检测，我们提出双分类器来处理词汇域
和词汇外域。文本编码器被进一步训练
通过解耦提示对事物和物品类别进行区域感知
学习，使他们能够过滤掉重复和低质量的预测，
这对于端到端分割和检测很重要。广泛的
在不同情况下对多个数据集进行实验。这
结果表明 OpenSD 优于最先进的开放词汇表
封闭和开放词汇中的分割和检测方法
设置。代码可在 https://github.com/strongwolf/OpenSD 获取
]]></description>
      <guid>http://arxiv.org/abs/2312.06703</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:20 GMT</pubDate>
    </item>
    <item>
      <title>使用引导扩散模型编辑真实图像的感知相似性指导和文本指导优化。 （arXiv：2312.06680v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06680</link>
      <description><![CDATA[使用扩散模型进行图像编辑时，有时会出现
修改后的图像可能与源图像有很大不同。为了解决这个问题，我们应用了
双引导方法，以在以下领域保持对原始内容的高度保真度：
没有被改变。首先，我们采用文本引导优化，使用文本
直接潜在空间和无分类器指导的嵌入。其次，我们使用
感知相似性指导，利用后验优化潜在向量
在逆过程中通过Tweedie公式进行采样。该方法确保
编辑元素的真实渲染和保存
原始图像中未经编辑的部分。
]]></description>
      <guid>http://arxiv.org/abs/2312.06680</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:19 GMT</pubDate>
    </item>
    <item>
      <title>Robo360：3D 全方位多材料机器人操作数据集。 （arXiv：2312.06686v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06686</link>
      <description><![CDATA[长期以来，制造能够自动执行劳动密集型任务的机器人一直是人们的追求。
计算机视觉和机器人技术进步背后的核心动机
社区。最近对利用 3D 算法（尤其是神经算法）的兴趣
领域，导致了机器人感知和物理理解的进步
在操纵场景中。然而，现实世界的复杂性
重大挑战。为了应对这些挑战，我们推出了 Robo360，
具有密集视图覆盖的机器人操作的数据集，
实现高质量的 3D 神经表示学习，以及多种
具有各种物理和光学特性的物体并促进研究
在各种对象操作和物理世界建模任务中。我们确认
使用现有动态 NeRF 的数据集的有效性并评估其
学习多视角政策的潜力。希望Robo360能够开辟新的道路
在理解的交叉点上尚待探索的研究方向
3D 物理世界和机器人控制。
]]></description>
      <guid>http://arxiv.org/abs/2312.06686</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:19 GMT</pubDate>
    </item>
    <item>
      <title>基于组织病理学图像的外部验证机器学习模型用于女性乳腺癌诊断、分类、预后或治疗结果预测的性能：系统评价。 （arXiv：2312.06697v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.06697</link>
      <description><![CDATA[针对乳腺癌开发了许多机器学习 (ML) 模型
使用各种类型的数据。机器学习模型的成功外部验证 (EV)
是其普遍性的重要证据。本系统的目的
审查的目的是评估基于外部验证的 ML 模型的性能
用于诊断、分类、预后或治疗的组织病理学图像
女性乳腺癌的结果预测。 MEDLINE 的系统检索，
EMBASE、CINAHL、IEEE、MICCAI、SPIE会议进行研究
2010 年 1 月至 2022 年 2 月期间发布。预测模型风险
使用偏差评估工具（PROBAST），结果是叙述性的
描述。 2011 年非重复引用中，8 篇期刊文章和 2 篇
会议记录符合纳入标准。三项外部研究
已验证的用于诊断的 ML 模型、4 个用于分类的模型、2 个用于预后的模型、1 个用于预测的模型
用于分类和预后。大多数研究使用卷积神经网络
网络和一种使用逻辑回归算法。为了
诊断/分类模型，报告的最常见的性能指标
EV中的准确率和曲线下面积均大于87%
和 90% 分别使用病理学家的注释作为基本事实。这
预测 ML 模型的 EV 的风险比在 1.7 之间（95% CI，
1.2-2.6) 和 1.8 (95% CI, 1.3-2.7) 预测远处无病生存；
复发率为 1.91 (95% CI, 1.11-3.29)，复发率为 0.09 (95% CI, 0.01-0.70)
总生存期为 0.65（95% CI，0.43-0.98），使用临床数据作为
基本事实。尽管 EV 是临床前的重要一步
ML 模型的应用，尚未常规执行。大的
训练/验证数据集、方法、性能指标的可变性，
和报告的信息限制了模型的比较和分析
他们的结果 (...)
]]></description>
      <guid>http://arxiv.org/abs/2312.06697</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:19 GMT</pubDate>
    </item>
    <item>
      <title>对抗端到端自动驾驶中速度和延迟的影响。 （arXiv：2312.06670v1 [cs.RO]）</title>
      <link>http://arxiv.org/abs/2312.06670</link>
      <description><![CDATA[在端到端驾驶的行为克隆方法中，专家数据集
收集驾驶信息，模型学会猜测专家会做什么
不同的情况。观察和输出中总结了情况
是低级或中级命令（例如制动、油门和转向；或
轨迹）。模型学习将时间 T 的观察结果与行动相匹配
在 T 或尽可能同时记录。然而，在部署时
模型到现实世界（或异步模拟），动作
基于时间 T 的观察进行预测，并在 T + $\Delta$ T 处应用。
在各种情况下，$\Delta$ T 可能会产生相当大且显着的影响
表现。

我们首先证明以两种不同的速度行驶实际上是两种
不同的任务。延迟部分导致了这种差异并线性放大
它。即使没有计算延迟，执行器延迟和滑动也会由于
惯性导致需要在快速行驶时先发制人地采取行动。
与以下相比，将观察结果映射到命令的函数变得不同
缓慢驾驶。我们的实验表明，经过训练可以快速驾驶的模型不能
执行看似更容易的慢速驾驶任务，反之亦然。驾驶良好
由于在“安全低速”下进行测试，模型可能会被判定为较差，
他们无法执行的任务。

其次，我们展示如何抵消端到端延迟的影响
通过改变目标标签来构建网络。这与方法相反
试图最大限度地减少延迟，即原因，而不是结果。举例说明
针对现实世界中的问题和解决方案，我们使用 1:10 比例的微型车
计算能力有限，使用行为克隆进行端到端驱动。一些
这里讨论的想法可以转移到更广泛的背景
自动驾驶，具有更多计算能力和端到端或模块化的车辆
接近。
]]></description>
      <guid>http://arxiv.org/abs/2312.06670</guid>
      <pubDate>Wed, 13 Dec 2023 15:14:18 GMT</pubDate>
    </item>
    </channel>
</rss>