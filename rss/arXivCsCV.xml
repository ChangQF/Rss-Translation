<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>BEVSpread：基于视觉的路边 3D 物体检测中用于鸟瞰视图表示的扩展体素池</title>
      <link>https://arxiv.org/abs/2406.08785</link>
      <description><![CDATA[arXiv:2406.08785v1 公告类型：新
摘要：基于视觉的路边 3D 物体检测在自动驾驶领域引起了越来越多的关注，因为它具有减少盲点和扩大感知范围的固有优势。而以前的工作主要集中于准确估计 2D 到 3D 映射的深度或高度，忽略了体素池化过程中的位置近似误差。受此启发，我们提出了一种新颖的体素池化策略来减少此类误差，称为 BEVSpread。具体而言，BEVSpread 不是将包含在视锥点中的图像特征带到单个 BEV 网格，而是将每个视锥点视为源，并使用自适应权重将图像特征传播到周围的 BEV 网格。为了实现卓越的传播性能，设计了一个特定的权重函数来根据距离和深度动态控制权重的衰减速度。在定制的 CUDA 并行加速的帮助下，BEVSpread 实现了与原始体素池化相当的推理时间。在两个大规模路边基准上进行的大量实验表明，作为插件，BEVSpread 可以显著提高现有基于视锥体的 BEV 方法在车辆、行人和自行车上的性能，幅度高达 (1.12, 5.26, 3.01) AP。]]></description>
      <guid>https://arxiv.org/abs/2406.08785</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:03 GMT</pubDate>
    </item>
    <item>
      <title>DenoiseReID：用于行人重新识别表征学习的去噪模型</title>
      <link>https://arxiv.org/abs/2406.08773</link>
      <description><![CDATA[arXiv:2406.08773v1 公告类型：新
摘要：本文提出了一种用于表示学习的新型去噪模型，并将行人重新识别（ReID）作为基准任务，称为 DenoiseReID，通过联合特征提取和去噪来提高特征判别性。在深度学习时代，由级联嵌入层（例如卷积或变压器）组成的主干网络变得流行，以逐步提取有用的特征。我们首先将主干中的每个嵌入层视为去噪层，处理级联嵌入层就像我们逐步递归地对特征进行去噪一样。这统一了特征提取和特征去噪的框架，前者逐步从低级到高级嵌入特征，后者逐步递归地对特征进行去噪。然后，我们设计了一种新颖的特征提取和特征去噪融合算法（FEFDFA），并\textit{从理论上证明}其融合前后的等价性。FEFDFA 将去噪层的参数合并到现有的嵌入层中，从而使特征去噪无需计算。这是一种无标签算法，可以逐步改进特征，并且如果可用的话还可以补充标签。此外，它还有两个优点：1）它是一个无计算和无标签的插件，可以逐步改进 ReID 特征。2）如果有标签，它可以补充标签。在各种任务（大规模图像分类、细粒度图像分类、图像检索）和主干（变压器和卷积）上的实验结果表明了我们方法的可扩展性和稳定性。在 4 个 ReID 数据集和各种主干上的实验结果显示了稳定性和显著的改进。我们还将提出的方法扩展到大规模（ImageNet）和细粒度（例如CUB200）分类任务，并证明了类似的改进。]]></description>
      <guid>https://arxiv.org/abs/2406.08773</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:02 GMT</pubDate>
    </item>
    <item>
      <title>ALINA：高级线条识别和标记算法</title>
      <link>https://arxiv.org/abs/2406.08775</link>
      <description><![CDATA[arXiv:2406.08775v1 公告类型：新
摘要：标签是监督机器学习算法的基石。大多数视觉识别方法都是完全监督的，使用边界框或逐像素分割进行对象定位。传统的标记方法（例如众包）由于成本、数据隐私、时间量以及大型数据集上的潜在错误而令人望而却步。为了解决这些问题，我们提出了一种新颖的注释框架，即高级线识别和标记算法 (ALINA)，该算法可用于标记由不同摄像机视角和可变天气属性（晴天和多云）组成的滑行道数据集。此外，还提出了圆形阈值像素发现和遍历 (CIRCLEDAT) 算法，这是确定与滑行道线标记相对应的像素不可或缺的步骤。一旦识别出像素，ALINA 就会在帧上生成相应的像素坐标注释。使用这种方法，已经标记了滑行道数据集 AssistTaxi 中的 60,249 个帧。为了评估性能，我们根据边缘特征和连通性手动生成了基于上下文的边缘图 (CBEM) 集。使用 CBEM 集测试带注释的标签后，检测率为 98.45%，证明了其可靠性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.08775</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:02 GMT</pubDate>
    </item>
    <item>
      <title>深度卷积神经网络在检测医学图像伪造品方面的比较分析</title>
      <link>https://arxiv.org/abs/2406.08758</link>
      <description><![CDATA[arXiv:2406.08758v1 公告类型：新
摘要：生成对抗网络 (GAN) 在包括医学成像在内的各种应用中都表现出了显著的进步。虽然许多最先进的深度卷积神经网络 (DCNN) 架构以其熟练的特征提取而闻名，但本文研究了它们在医学图像深度伪造检测中的功效。主要目标是通过对 13 个最先进的 DCNN 进行全面评估，有效区分真实与篡改或操纵的医学图像。性能评估基于各种评估指标，包括时间效率和计算资源需求的考虑。我们的研究结果表明，ResNet50V2 在精确度和特异性方面表现出色，而 DenseNet169 则以其准确度、召回率和 F1 分数而著称。我们研究了一种模型比另一种模型更受欢迎的特定场景。此外，MobileNetV3Large 的性能也极具竞争力，在保持相对较少的参数数量的情况下，成为所考虑的 DCNN 模型中速度最快的模型。我们还评估了所研究的 DCNN 中的潜在空间可分离性质量，结果显示 DenseNet 和 EfficientNet 模型系列都具有优越性，并且有助于更深入地了解医学图像深度伪造。本研究中的实验分析为医学成像领域的深度伪造图像检测领域提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2406.08758</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>高斯森林：用于压缩场景建模的分层混合 3D 高斯分层</title>
      <link>https://arxiv.org/abs/2406.08759</link>
      <description><![CDATA[arXiv:2406.08759v1 公告类型：新
摘要：新视图合成领域最近出现了 3D 高斯 Splatting，它以基于点的方式表示场景并通过光栅化进行渲染。与依赖光线追踪的辐射场相比，这种方法表现出卓越的渲染质量和速度。然而，3D 高斯的显式和非结构化性质带来了巨大的存储挑战，阻碍了其更广泛的应用。为了应对这一挑战，我们引入了高斯森林建模框架，该框架将场景分层表示为混合 3D 高斯森林。每个混合高斯都保留其独特的显式属性，同时与其兄弟高斯共享隐式属性，从而以更少的变量优化参数化。此外，设计了自适应增长和修剪策略，确保在复杂区域中进行详细表示，并显着减少所需的高斯数量。大量实验表明，Gaussian-Forest 不仅保持了相当的速度和质量，而且压缩率超过 10 倍，在场景建模效率方面取得了显著的进步。代码可从 https://github.com/Xian-Bei/GaussianForest 获取。]]></description>
      <guid>https://arxiv.org/abs/2406.08759</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>MMFakeBench：LVLM 的混合源多模态错误信息检测基准</title>
      <link>https://arxiv.org/abs/2406.08772</link>
      <description><![CDATA[arXiv:2406.08772v1 公告类型：新 
摘要：当前的多模态错误信息检测 (MMD) 方法通常假设每个样本只有一个来源和一种伪造类型，这对于多个伪造源共存的现实场景来说是不够的。混合源错误信息的基准缺乏阻碍了该领域的进展。为了解决这个问题，我们推出了 MMFakeBench，这是第一个全面的混合源 MMD 基准。MMFakeBench 包括 3 个关键来源：文本真实性扭曲、视觉真实性扭曲和跨模态一致性扭曲，以及 12 个错误信息伪造类型的子类别。我们进一步在零样本设置下对 MMFakeBench 上的 6 种流行检测方法和 15 种大型视觉语言模型 (LVLM) 进行了广泛的评估。结果表明，当前的方法在这种具有挑战性和现实性的混合源 MMD 设置下很难发挥作用。此外，我们提出了一个创新的统一框架，该框架整合了 LVLM 代理的原理、操作和工具使用能力，大大提高了准确性和泛化能力。我们相信这项研究将促进未来对更现实的混合源多模态错误信息的研究，并为错误信息检测方法提供公平的评估。]]></description>
      <guid>https://arxiv.org/abs/2406.08772</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>Vivid-ZOO：基于扩散模型的多视角视频生成</title>
      <link>https://arxiv.org/abs/2406.08659</link>
      <description><![CDATA[arXiv:2406.08659v1 公告类型：新
摘要：虽然扩散模型在 2D 图像/视频生成中表现出色，但基于扩散的文本到多视图视频 (T2MVid) 生成仍未得到充分探索。T2MVid 生成带来的新挑战在于缺乏大量带字幕的多视图视频以及对这种多维分布进行建模的复杂性。为此，我们提出了一种基于扩散的新型管道，可从文本生成以动态 3D 对象为中心的高质量多视图视频。具体而言，我们将 T2MVid 问题分解为视点空间和时间分量。这种分解使我们能够组合和重用高级预训练多视图图像和 2D 视频扩散模型层，以确保生成的多视图视频的多视图一致性和时间连贯性，从而大大降低训练成本。我们进一步引入了对齐模块来对齐预训练多视图和二维视频扩散模型中各层的潜在空间，解决了由于二维和多视图数据之间的域差距而导致的重用层不兼容问题。为了支持这项研究和未来的研究，我们进一步贡献了一个带字幕的多视图视频数据集。实验结果表明，在给定各种文本提示的情况下，我们的方法可以生成高质量的多视图视频，展现生动的运动、时间连贯性和多视图一致性。]]></description>
      <guid>https://arxiv.org/abs/2406.08659</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>UnO：用于感知和预测的无监督占用场</title>
      <link>https://arxiv.org/abs/2406.08691</link>
      <description><![CDATA[arXiv:2406.08691v1 公告类型：新
摘要：感知世界并预测其未来状态是自动驾驶的关键任务。监督方法利用带注释的对象标签来学习世界模型——传统上是通过对象检测和轨迹预测，或时间鸟瞰 (BEV) 占用场。然而，这些注释很昂贵，并且通常仅限于一组预定义的类别，这些类别并不涵盖我们在路上可能遇到的所有内容。相反，我们学习使用来自 LiDAR 数据的自我监督来感知和预测连续的 4D（时空）占用场。这种无监督的世界模型可以轻松有效地转移到下游任务。我们通过添加轻量级学习渲染器来解决点云预测问题，并在 Argoverse 2、nuScenes 和 KITTI 中实现最先进的性能。为了进一步展示其可迁移性，我们对 BEV 语义占用预测模型进行了微调，并表明其表现优于完全监督的最先进的模型，尤其是在标记数据稀缺的情况下。最后，与之前最先进的时空几何占用预测模型相比，我们的 4D 世界模型对与自动驾驶相关的类别中的物体的召回率更高。]]></description>
      <guid>https://arxiv.org/abs/2406.08691</guid>
      <pubDate>Fri, 14 Jun 2024 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>FSBI：利用频率增强自混合图像进行 Deepfakes 检测</title>
      <link>https://arxiv.org/abs/2406.08625</link>
      <description><![CDATA[arXiv:2406.08625v1 公告类型：新
摘要：深度伪造研究的进步导致了人眼无法察觉的近乎完美的操纵和一些深度伪造检测工具的诞生。最近，已经提出了几种将深度伪造与真实图像和视频区分开来的技术。本文介绍了一种用于深度伪造检测的频率增强自混合图像 (FSBI) 方法。该提出的方法利用离散小波变换 (DWT) 从自混合图像 (SBI) 中提取判别特征，以用于训练卷积网络架构模型。SBI 通过在混合图像之前在图像副本中引入几个伪造伪像将图像与自身混合。这可以通过学习更通用的表示来防止分类器过度拟合特定伪像。然后将这些混合图像输入到频率特征提取器中，以检测在时间域中不易检测到的伪像。所提出的方法已经在 FF++ 和 Celeb-DF 数据集上进行了评估，并且获得的结果优于跨数据集评估协议的最先进的技术。]]></description>
      <guid>https://arxiv.org/abs/2406.08625</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>TC-Bench：对文本到视频和图像到视频生成中的时间组合性进行基准测试</title>
      <link>https://arxiv.org/abs/2406.08656</link>
      <description><![CDATA[arXiv:2406.08656v1 公告类型：新
摘要：视频生成除了图像生成之外，还面临许多独特的挑战。时间维度引入了跨帧的广泛可能变化，这些变化可能会破坏一致性和连续性。在这项研究中，我们超越了对简单动作的评估，并认为生成的视频应该随着时间的推移融入新概念的出现及其关系转换，就像现实世界的视频一样。为了评估视频生成模型的时间组合性，我们提出了 TC-Bench，这是一个精心制作的文本提示、相应的地面真实视频和稳健评估指标的基准。提示阐明了场景的初始状态和最终状态，有效地减少了帧开发的歧义并简化了转换完成的评估。此外，通过收集与提示相对应的对齐的真实世界视频，我们将 TC-Bench 的适用性从文本条件模型扩展到可以执行生成帧插值的图像条件模型。我们还开发了新的指标来衡量生成视频中组件转换的完整性，这些指标与人类判断的相关性明显高于现有指标。我们的综合实验结果表明，大多数视频生成器实现的构图变化不到 20%，这凸显了未来巨大的改进空间。我们的分析表明，当前的视频生成模型很难解释构图变化的描述，也很难在不同的时间步骤中合成各种组件。]]></description>
      <guid>https://arxiv.org/abs/2406.08656</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>FakeInversion：通过反转稳定扩散学习从未见过的文本到图像模型中检测图像</title>
      <link>https://arxiv.org/abs/2406.08603</link>
      <description><![CDATA[arXiv:2406.08603v1 公告类型：新
摘要：由于 GenAI 系统极有可能被滥用，检测合成图像的任务最近引起了研究界的极大兴趣。不幸的是，随着新的高保真文本到图像模型以惊人的速度开发，现有的图像空间检测器很快就过时了。在这项工作中，我们提出了一种新的合成图像检测器，它使用通过反转开源预训练的稳定扩散模型获得的特征。我们表明，即使检测器仅在通过稳定扩散生成的低保真度假图像上进行训练，这些反转特征也能使我们的检测器很好地推广到高视觉保真度的未见生成器（例如 DALL-E 3）。该检测器在多个训练和评估设置中实现了新的最先进水平。此外，我们引入了一种新的具有挑战性的评估协议，该协议使用反向图像搜索来减轻检测器评估中的风格和主题偏差。我们表明，最终的评估分数与探测器的野外性能非常吻合，并将这些数据集发布为未来研究的公共基准。]]></description>
      <guid>https://arxiv.org/abs/2406.08603</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>LayeredDoc：采用层分离方法的领域自适应文档恢复</title>
      <link>https://arxiv.org/abs/2406.08610</link>
      <description><![CDATA[arXiv:2406.08610v1 公告类型：新
摘要：智能文档处理系统的快速发展需要能够适应不同领域的强大解决方案，而无需进行大量再培训。传统方法通常会因文档类型变化而失效，从而导致性能不佳。为了克服这些限制，本文介绍了一种文本图形层分离方法，可增强文档图像恢复 (DIR) 系统中的域适应性。我们提出了 LayeredDoc，它利用了两层信息：第一层针对粗粒度图形组件，而第二层则细化机器打印的文本内容。这个分层 DIR 框架会动态调整输入文档的特征，从而促进有效的域适应。我们使用为本研究开发的新真实世界数据集 LayeredDocDB 对我们的方法进行了定性和定量评估。我们的模型最初是在合成生成的数据集上进行训练的，它展示了 DIR 任务的强大泛化能力，为处理真实世界数据的变化提供了一种有希望的解决方案。我们的代码可以在 GitHub 上访问。]]></description>
      <guid>https://arxiv.org/abs/2406.08610</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>DiTFastAttn：扩散变压器模型的注意力压缩</title>
      <link>https://arxiv.org/abs/2406.08552</link>
      <description><![CDATA[arXiv:2406.08552v1 公告类型：新 
摘要：扩散变换器 (DiT) 在图像和视频生成方面表现出色，但由于自注意力的二次复杂度而面临计算挑战。我们提出了 DiTFastAttn，一种新颖的训练后压缩方法，以缓解 DiT 的计算瓶颈。我们在 DiT 推理过程中的注意力计算中确定了三个关键冗余：1. 空间冗余，其中许多注意力头关注局部信息；2. 时间冗余，相邻步骤的注意力输出之间具有高度相似性；3. 条件冗余，其中条件和无条件推理表现出显着的相似性。为了解决这些冗余，我们提出了三种技术：1. 带有残差缓存的窗口注意力以减少空间冗余；2. 时间相似性减少以利用步骤之间的相似性；3. 条件冗余消除以跳过条件生成期间的冗余计算。为了证明 DiTFastAttn 的有效性，我们将其应用于 DiT、用于图像生成任务的 PixArt-Sigma 和用于视频生成任务的 OpenSora。评估结果表明，对于图像生成，我们的方法最多可减少 88% 的 FLOP，并在高分辨率生成中实现高达 1.6 倍的加速。]]></description>
      <guid>https://arxiv.org/abs/2406.08552</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士辅助概念发现：自动识别和解释神经元功能</title>
      <link>https://arxiv.org/abs/2406.08572</link>
      <description><![CDATA[arXiv:2406.08572v1 公告类型：新
摘要：为深度神经网络 (DNN) 中的神经元提供基于文本概念的解释对于理解 DNN 模型的工作原理非常重要。先前的研究已经根据概念示例或预定义的概念集将概念与神经元相关联，从而将可能的解释限制在用户期望的范围内，尤其是在发现新概念时。此外，定义概念集需要用户手动操作，要么直接指定它们，要么收集示例。为了克服这些问题，我们建议利用多模态大型语言模型进行自动和开放式概念发现。我们表明，在没有一组受限制的预定义概念的情况下，我们的方法会产生更忠实于模型行为的新型可解释概念。为了量化这一点，我们通过生成示例和反例并评估神经元对这组新图像的响应来验证每个概念。总的来说，我们的方法可以发现概念并同时验证它们，从而为解释深度神经网络提供了一种可靠的自动化工具。]]></description>
      <guid>https://arxiv.org/abs/2406.08572</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>利用共享分类器进行自适应教学以实现知识提炼</title>
      <link>https://arxiv.org/abs/2406.08528</link>
      <description><![CDATA[arXiv:2406.08528v1 公告类型：新
摘要：知识蒸馏（KD）是一种将知识从过度参数化的教师网络转移到参数化程度较低的学生网络的技术，从而最大限度地减少由此造成的性能损失。KD 方法可以分为离线和在线方法。离线 KD 利用强大的预训练教师网络，而在线 KD 允许动态调整教师网络以提高学生网络的学习效率。最近发现，共享教师网络的分类器可以显著提高学生网络的性能，而网络参数的数量只会增加很少。基于这些见解，我们提出了使用共享分类器的自适应教学（ATSC）。在 ATSC 中，预训练的教师网络会根据其能力进行自我调整，以更好地满足学生网络的学习需求，学生网络受益于共享分类器，从而提高其性能。此外，我们将 ATSC 扩展到有多个老师的环境。我们进行了广泛的实验，证明了所提出的 KD 方法的有效性。我们的方法在单教师和多教师场景中在 CIFAR-100 和 ImageNet 数据集上取得了最佳效果，所需模型参数数量仅略有增加。源代码可在 https://github.com/random2314235/ATSC 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2406.08528</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:56 GMT</pubDate>
    </item>
    </channel>
</rss>