<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 29 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>GarmentCodeData：带有缝纫图案的 3D 定制服装数据集</title>
      <link>https://arxiv.org/abs/2405.17609</link>
      <description><![CDATA[arXiv:2405.17609v1 公告类型：新
摘要：最近对基于学习的服装处理的研究兴趣，从虚拟试穿到生成和重建，偶然发现该领域缺乏高质量的公共数据。我们通过展示第一个带有缝纫图案的 3D 量身定制服装的大规模合成数据集及其生成流程，为解决这一需求做出了贡献。GarmentCodeData 包含 115,000 个数据点，涵盖了许多常见服装类别的各种设计：上衣、衬衫、连衣裙、连身裤、裙子、裤子等，适合从基于 CAESAR 的自定义统计身体模型中采样的各种体形，以及应用三种不同纺织材料的标准参考体形。为了能够创建如此复杂的数据集，我们引入了一套算法，用于自动根据采样的体形进行裁缝测量，以及缝纫图案设计的采样策略，并提出了一种基于快速 XPBD 模拟器的自动开源 3D 服装悬垂流水线，同时为碰撞解决和悬垂正确性贡献了几种解决方案，以实现可扩展性。
数据集：http://hdl.handle.net/20.500.11850/673889]]></description>
      <guid>https://arxiv.org/abs/2405.17609</guid>
      <pubDate>Thu, 30 May 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>多模态学习框架：联合建模模态间和模态内依赖关系</title>
      <link>https://arxiv.org/abs/2405.17613</link>
      <description><![CDATA[arXiv:2405.17613v1 公告类型：新
摘要：监督式多模态学习涉及将多种模态映射到目标标签。该领域的先前研究集中于单独捕获模态间依赖关系（不同模态与标签之间的关系）或模态内依赖关系（单个模态与标签内的关系）。我们认为，这些仅依赖于模态间或模态内依赖关系的传统方法通常可能不是最佳的。我们从生成模型的角度来看待多模态学习问题，其中我们将目标视为多种模态及其之间相互作用的来源。为此，我们提出了模态间和模态内建模 (I2M2) 框架，该框架捕获并集成模态间和模态内依赖关系，从而实现更准确的预测。我们使用现实世界的医疗保健和视觉与语言数据集以及最先进的模型来评估我们的方法，结果表明，与仅关注一种模态依赖关系的传统方法相比，该方法具有更优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.17613</guid>
      <pubDate>Thu, 30 May 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>ExtremeMETA：通过重塑多通道超材料成像仪实现高速轻量级图像分割模型</title>
      <link>https://arxiv.org/abs/2405.17568</link>
      <description><![CDATA[arXiv:2405.17568v1 公告类型：新
摘要：深度神经网络 (DNN) 严重依赖于 CPU 和 GPU 等传统计算单元。然而，这种传统方法带来了巨大的计算负担、延迟问题和高功耗，限制了它们的有效性。这引发了对 ExtremeC3Net 等轻量级网络的需求。另一方面，光学计算单元取得了显着进步，尤其是超材料，为以光速运行的节能神经网络提供了令人兴奋的前景。然而，超材料神经网络 (MNN) 的数字设计面临着精度、噪声和带宽等挑战，限制了它们在直观任务和低分辨率图像中的应用。在本文中，我们提出了一个大核轻量级分割模型 ExtremeMETA。基于 ExtremeC3Net，ExtremeMETA 通过探索更大的卷积核和多条处理路径来最大化第一个卷积层的能力。利用提出的大核卷积模型，我们将光学神经网络应用边界扩展到分割任务。为了进一步减轻数字处理部分的计算负担，采用了一组模型压缩方法来提高推理阶段的模型效率。在三个公开数据集上的实验结果表明，优化后的高效设计将分割性能从 mIoU 的 92.45 提高到 95.97，同时将计算 FLOPs 从 461.07 MMacs 降低到 166.03 MMacs。提出的大内核轻量级模型 ExtremeMETA 展示了混合设计在复杂任务上的能力。]]></description>
      <guid>https://arxiv.org/abs/2405.17568</guid>
      <pubDate>Thu, 30 May 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>GOI：使用可优化的开放词汇语义空间超平面查找感兴趣的 3D 高斯</title>
      <link>https://arxiv.org/abs/2405.17596</link>
      <description><![CDATA[arXiv:2405.17596v1 公告类型：新
摘要：3D 开放词汇场景理解对于增强现实和机器人应用的发展至关重要，它涉及根据自然语言指令解释和定位 3D 空间内的特定区域。为此，我们引入了 GOI，这是一个将 2D 视觉语言基础模型中的语义特征集成到 3D 高斯分布 (3DGS) 中的框架，并使用可优化的语义空间超平面识别感兴趣的 3D 高斯。我们的方法包括一种有效的压缩方法，该方法利用场景先验将嘈杂的高维语义特征压缩为紧凑的低维向量，然后将其嵌入 3DGS。在开放词汇查询过程中，我们采用了一种与现有方法不同的方法，现有方法依赖于手动设置的固定经验阈值，根据区域与查询文本嵌入的语义特征距离来选择区域。这种传统方法通常缺乏普遍的准确性，导致难以准确识别特定目标区域。相反，我们的方法将特征选择过程视为特征空间内的超平面划分，仅保留与查询高度相关的特征。我们利用现成的 2D 指称表达分割 (RES) 模型来微调语义空间超平面，从而更精确地区分目标区域和其他区域。这种微调大大提高了开放词汇查询的准确性，确保了相关 3D 高斯的精确定位。大量实验证明了 GOI 优于以前的最先进方法。我们的项目页面位于 https://goi-hyperplane.github.io/ 。]]></description>
      <guid>https://arxiv.org/abs/2405.17596</guid>
      <pubDate>Thu, 30 May 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>进化渲染模型</title>
      <link>https://arxiv.org/abs/2405.17531</link>
      <description><![CDATA[arXiv:2405.17531v1 公告类型：新
摘要：随着可微分渲染模型的最新进展，计算机图形学领域发生了重大变化。这些渲染模型通常依赖于启发式设计，这些设计可能与最终渲染目标不完全一致。我们通过开创 \textit{进化渲染模型} 来解决这一差距，这种方法中的渲染模型具有在整个渲染过程中动态发展和适应的能力。特别是，我们提出了一个全面的学习框架，可以优化三个主要渲染元素，包括规范变换、射线采样机制和原始组织。该框架的核心是开发这些渲染元素的可微分版本，从而实现从最终渲染目标的有效梯度反向传播。对梯度特性进行了详细分析，以促进稳定且面向目标的元素演变。我们进行了广泛的实验，证明了进化渲染模型在增强各个领域的渲染性能方面具有巨大潜力，包括静态和动态场景表示、生成建模和纹理映射。]]></description>
      <guid>https://arxiv.org/abs/2405.17531</guid>
      <pubDate>Thu, 30 May 2024 03:16:22 GMT</pubDate>
    </item>
    <item>
      <title>ClassDiffusion：通过明确的类别指导实现更加一致的个性化调整</title>
      <link>https://arxiv.org/abs/2405.17532</link>
      <description><![CDATA[arXiv:2405.17532v1 公告类型：新
摘要：最近的文本到图像定制工作已被证明能够通过对一些示例微调扩散模型来成功生成给定概念的图像。然而，这些方法往往会过度拟合概念，导致在多种条件下无法创建概念（例如，在生成“戴着耳机的狗”时缺少耳机）。有趣的是，我们注意到微调之前的基础模型表现出将基础概念与其他元素组合的能力（例如，戴着耳机的狗），这意味着只有在个性化调整后，组合能力才会消失。受这一观察的启发，我们提出了 ClassDiffusion，这是一种简单的技术，它利用语义保存损失在学习新概念时明确调节概念空间。尽管它很简单，但这有助于避免在对目标概念进行微调时出现语义漂移。大量的定性和定量实验表明，使用语义保存损失可以有效提高微调模型的组合能力。为了解决 CLIP-T 指标评估不力的问题，我们引入了 BLIP2-T 指标，这是针对该特定领域的更公平、更有效的评估指标。我们还提供了深入的实证研究和理论分析，以更好地理解所提损失的作用。最后，我们还将 ClassDiffusion 扩展到个性化视频生成，展示了其灵活性。]]></description>
      <guid>https://arxiv.org/abs/2405.17532</guid>
      <pubDate>Thu, 30 May 2024 03:16:22 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言模型的文化意识如何？</title>
      <link>https://arxiv.org/abs/2405.17475</link>
      <description><![CDATA[arXiv:2405.17475v1 公告类型：新
摘要：人们常说一张图片胜过千言万语，某些图像可以讲述丰富而有见地的故事。这些故事可以通过图像字幕来讲述吗？民间传说类型的图像，如神话、民间舞蹈、文化符号和象征，对每种文化都至关重要。我们的研究比较了四种流行的视觉语言模型（GPT-4V、Gemini Pro Vision、LLaVA 和 OpenFlamingo）在识别此类图像中的文化特定信息以及创建准确且文化敏感的图像字幕方面的表现。我们还提出了一种新的评估指标——文化意识分数 (CAS)，专门用于衡量图像字幕中的文化意识程度。我们提供了一个数据集 MOSAIC-1.5k，其中包含包含文化背景和上下文的图像的基本事实，以及一个带有指定文化意识分数的标记数据集，可用于看不见的数据。创建符合文化背景的图像说明对科学研究很有价值，对许多实际应用也有好处。我们设想，我们的工作将促进文化敏感性在全球范围内与人工智能应用的更深层次融合。通过向公众提供数据集和文化意识分数，我们旨在促进该领域的进一步研究，鼓励开发更具文化意识、尊重和赞美全球多样性的人工智能系统。]]></description>
      <guid>https://arxiv.org/abs/2405.17475</guid>
      <pubDate>Thu, 30 May 2024 03:16:21 GMT</pubDate>
    </item>
    <item>
      <title>语义全局概念的本地测试模型检测</title>
      <link>https://arxiv.org/abs/2405.17523</link>
      <description><![CDATA[arXiv:2405.17523v2 公告类型：新
摘要：确保黑盒深度神经网络 (DNN) 的质量变得越来越重要，尤其是在自动驾驶等安全关键领域。虽然全局概念编码通常使用户能够测试特定概念的模型，但将全局概念编码与单个网络输入的本地处理联系起来会揭示它们的优势和局限性。我们提出的框架全局到本地概念归因 (glCA) 使用来自本地（特定预测产生的原因）和全局（模型通常如何工作）可解释人工智能 (xAI) 的方法来本地测试 DNN 的预定义语义概念。该方法允许对编码为模型潜在空间中的线性方向的预定义语义概念进行局部、事后解释。关于全局概念使用的像素精确评分有助于测试人员进一步了解所选概念的单个数据点的模型处理。我们的方法的优点是完全覆盖语义概念的模型内部编码，并允许定位相关的概念相关信息。结果显示，个体全局概念编码的局部感知和使用存在很大差异，需要进一步研究以获得彻底的语义概念编码。]]></description>
      <guid>https://arxiv.org/abs/2405.17523</guid>
      <pubDate>Thu, 30 May 2024 03:16:21 GMT</pubDate>
    </item>
    <item>
      <title>基于扩散的生成记忆的无数据联邦类增量学习</title>
      <link>https://arxiv.org/abs/2405.17457</link>
      <description><![CDATA[arXiv:2405.17457v1 公告类型：新
摘要：联邦类增量学习 (FCIL) 是一个关键但尚未得到充分探索的问题，它涉及在联邦学习 (FL) 中动态合并新类。现有方法通常采用生成对抗网络 (GAN) 来生成合成图像以解决 FL 中的隐私问题。然而，GAN 表现出固有的不稳定性和高灵敏度，从而损害了这些方法的有效性。在本文中，我们引入了一种新型的无数据联邦类增量学习框架，该框架具有基于扩散的生成记忆 (DFedDGM)，通过扩散模型生成稳定、高质量的图像来缓解灾难性遗忘。我们设计了一个新的平衡采样器来帮助训练扩散模型以缓解 FL 中常见的非 IID 问题，并从信息论的角度引入了一种基于熵的样本过滤技术来提高生成样本的质量。最后，我们将知识蒸馏与基于特征的正则化项相结合，以实现更好的知识转移。与基线 FedAvg 方法相比，我们的框架不会产生额外的通信成本。在多个数据集上进行的大量实验表明，我们的方法明显优于现有基线，例如，Tiny-ImageNet 数据集上的平均准确率提高了 4% 以上。]]></description>
      <guid>https://arxiv.org/abs/2405.17457</guid>
      <pubDate>Thu, 30 May 2024 03:16:20 GMT</pubDate>
    </item>
    <item>
      <title>WeatherFormer：一种用于从小型数据集中学习稳健天气表征的预训练编码器模型</title>
      <link>https://arxiv.org/abs/2405.17455</link>
      <description><![CDATA[arXiv:2405.17455v1 公告类型：新
摘要：本文介绍了 WeatherFormer，这是一种基于 Transformer 编码器的模型，旨在从最少的观测中学习稳健的天气特征。它解决了从小型数据集建模复杂天气动态的挑战，这是农业、流行病学和气候科学中许多预测任务的瓶颈。WeatherFormer 在一个大型预训练数据集上进行了预训练，该数据集由 39 年的美洲卫星测量数据组成。凭借新颖的预训练任务和微调，WeatherFormer 在县级大豆产量预测和流感预报中取得了最先进的性能。技术创新包括一种独特的时空编码，可捕捉地理、年度和季节变化，使 Transformer 架构适应连续天气数据，以及一种预训练策略，用于学习对缺失天气特征具有鲁棒性的表示。本文首次展示了对大型 Transformer 编码器模型进行预训练的有效性，可用于跨多个领域的天气相关应用。]]></description>
      <guid>https://arxiv.org/abs/2405.17455</guid>
      <pubDate>Thu, 30 May 2024 03:16:19 GMT</pubDate>
    </item>
    <item>
      <title>使用基于扩散的图像生成来优化逆问题的线性测量</title>
      <link>https://arxiv.org/abs/2405.17456</link>
      <description><![CDATA[arXiv:2405.17456v1 公告类型：新
摘要：我们重新审视了从一小组线性测量中重建高维信号的问题，并结合了扩散概率模型中的图像先验。优化此类测量的成熟方法包括主成分分析 (PCA)、独立成分分析 (ICA) 和压缩感知 (CS)，所有这些方法都依赖于轴或子空间对齐的统计特征。但许多自然发生的信号，包括摄影图像，都包含更丰富的统计结构。为了利用这种结构，我们引入了一种通用方法来获得一组优化的线性测量，假设贝叶斯逆解利用训练用于执行去噪的神经网络中的先验隐含。我们证明这些测量与 PCA 和 CS 的测量不同，在最小化平方重建误差方面有显著的改进。此外，我们表明优化 SSIM 感知损失的测量会导致感知重建的改善。我们的结果强调了在设计有效的线性测量时融入自然信号的特定统计规律的重要性。]]></description>
      <guid>https://arxiv.org/abs/2405.17456</guid>
      <pubDate>Thu, 30 May 2024 03:16:19 GMT</pubDate>
    </item>
    <item>
      <title>基于图像的字符识别、用于解码寺庙铭文的文献系统</title>
      <link>https://arxiv.org/abs/2405.17449</link>
      <description><![CDATA[arXiv:2405.17449v1 公告类型：新
摘要：该项目对应用于 Brihadeeswarar 神庙墙壁上发现的 10 世纪古代泰米尔铭文的光学字符识别 OCR 方法进行训练和分析。所选的 OCR 方法包括 Tesseract，一种广泛使用的 OCR 引擎，使用现代 ICR 技术预处理原始数据，并使用框编辑软件微调我们的模型。使用 Tesseract 进行分析旨在评估它们在准确解读古代泰米尔字符细微差别方面的有效性。我们的模型对数据集的性能由它们的准确率决定，其中评估的数据集分为训练集和测试集。通过解决脚本的历史背景带来的独特挑战，本研究旨在为更广泛的 OCR 领域贡献有价值的见解，促进古代铭文的保存和解释]]></description>
      <guid>https://arxiv.org/abs/2405.17449</guid>
      <pubDate>Thu, 30 May 2024 03:16:18 GMT</pubDate>
    </item>
    <item>
      <title>下一帧预测对于学习物理定律的作用</title>
      <link>https://arxiv.org/abs/2405.17450</link>
      <description><![CDATA[arXiv:2405.17450v1 公告类型：新
摘要：下一帧预测是一种有用且强大的方法，可用于建模和理解视频数据的动态。受因果语言建模和语言建模中下一个标记预测的经验成功启发，我们探索了下一帧预测在多大程度上可以作为一种强大的基础学习策略（类似于语言建模）来引导对视觉世界的理解。为了量化下一帧预测引起的特定视觉理解，我们引入了六个诊断模拟视频数据集，这些数据集源自由重力和质量等不同物理常数创建的基本物理定律。我们证明，仅对下一帧预测进行训练的模型能够预测这些物理常数（例如重力）的值，而无需通过回归任务直接训练来学习这些常数。我们发现，生成训练阶段本身就能诱导出一种模型状态，这种模型状态能够比随机模型更好地预测物理常数，从而将损失降低 1.28 到 6.24 倍。我们得出结论，下一帧预测作为一种通用学习策略，在无需明确标记的情况下，有望帮助理解视觉领域的诸多“规律”。]]></description>
      <guid>https://arxiv.org/abs/2405.17450</guid>
      <pubDate>Thu, 30 May 2024 03:16:18 GMT</pubDate>
    </item>
    <item>
      <title>通过时空注意力网络实现基于梯度的时间序列解释</title>
      <link>https://arxiv.org/abs/2405.17444</link>
      <description><![CDATA[arXiv:2405.17444v1 公告类型：新
摘要：在本文中，我们探讨了使用基于变换器的时空注意网络 (STAN) 进行基于梯度的时间序列解释的可行性。首先，我们使用数据的全局和局部视图以及时间序列数据（即活动类型）上的弱监督标签训练 STAN 模型进行视频分类。然后，我们利用基于梯度的 XAI 技术（例如显着性图）来识别时间序列数据的显着帧。根据使用四个医学相关活动的数据集进行的实验，STAN 模型展示了其识别重要视频帧的潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.17444</guid>
      <pubDate>Thu, 30 May 2024 03:16:17 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 ViT 进行 OOD 检测</title>
      <link>https://arxiv.org/abs/2405.17447</link>
      <description><![CDATA[arXiv:2405.17447v1 公告类型：新
摘要：事实证明，VisionTransformers 在从公开可用的检查点进行微调后，对于 ImageNet 规模设置而言是强大的分布外检测器，通常在流行基准上优于其他模型类型。在这项工作中，我们通过分析大量模型来研究预训练和微调方案对 ViT 在该任务上的性能的影响。我们发现，确切的预训练类型对哪种方法效果良好以及总体 OOD 检测性能有很大影响。我们进一步表明，某些训练方案可能仅对特定类型的分布外有效，但对一般情况无效，并确定了最佳实践的训练方案。]]></description>
      <guid>https://arxiv.org/abs/2405.17447</guid>
      <pubDate>Thu, 30 May 2024 03:16:17 GMT</pubDate>
    </item>
    </channel>
</rss>