<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 04 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>CAPE：CAM 作为增强 DNN 解释的概率集成</title>
      <link>https://arxiv.org/abs/2404.02388</link>
      <description><![CDATA[arXiv:2404.02388v1 公告类型：新
摘要：深度神经网络（DNN）广泛用于视觉分类任务，但其复杂的计算过程和黑盒性质阻碍了决策的透明度和可解释性。类激活图 (CAM) 和最新变体提供了通过显示 DNN 的“注意力”热图来直观地解释 DNN 决策过程的方法。然而，CAM 解释仅提供相对的注意力信息，即在注意力热图上，我们可以解释哪个图像区域比其他区域更重要或不那么重要。然而，这些区域无法在类别之间进行有意义的比较，并且每个区域对模型类别预测的贡献也没有被揭示。为了解决这些挑战，最终带来更好的 DNN 解释，在本文中，我们提出了 CAPE，这是一种新颖的 CAM 重新表述，它为图像区域的贡献提供了统一且具有概率意义的评估。我们在 CUB 和 ImageNet 基准数据集上对 CAPE 与最先进的 CAM 方法进行定量和定性比较，以证明增强的可解释性。我们还对描述具有挑战性的慢性粒单核细胞白血病 (CMML) 诊断问题的细胞学成像数据集进行了测试。代码位于：https://github.com/AIML-MED/CAPE。]]></description>
      <guid>https://arxiv.org/abs/2404.02388</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>使用语言对图像进行语义增强</title>
      <link>https://arxiv.org/abs/2404.02353</link>
      <description><![CDATA[arXiv:2404.02353v1 公告类型：新
摘要：深度学习模型非常需要数据，并且需要非常大的标记数据集来进行监督学习。因此，这些模型经常会出现过度拟合，从而限制了它们推广到现实世界示例的能力。扩散模型的最新进展使得能够基于文本输入生成逼真的图像。利用用于训练这些扩散模型的大量数据集，我们提出了一种利用生成的图像来增强现有数据集的技术。本文探讨了有效数据增强的各种策略，以提高深度学习模型的域外泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2404.02353</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>使用具有眼睛注视模式的视觉和语言模型增强胸部 X 射线分析中的人机交互</title>
      <link>https://arxiv.org/abs/2404.02370</link>
      <description><![CDATA[arXiv:2404.02370v1 公告类型：新
摘要：计算机辅助诊断的最新进展在医学成像任务中显示出良好的性能，特别是在胸部 X 射线分析中。然而，这些模型和放射科医生之间的交互主要限于输入图像。这项工作提出了一种新方法，使用视觉语言模型 (VLM) 增强胸部 X 射线分析中的人机交互，通过将眼睛注视数据与文本提示结合起来，增强放射科医生的注意力。我们的方法利用从眼睛注视数据生成的热图，将其叠加到医学图像上，以突出显示放射科医生在胸部 X 射线评估期间重点关注的区域。我们在视觉问答、胸部 X 光报告自动化、错误检测和鉴别诊断等任务中评估这种方法。我们的结果表明，纳入眼睛注视信息可显着提高胸部 X 射线分析的准确性。此外，眼睛注视对微调的影响也得到了证实，因为它在除视觉问答之外的所有任务中都优于其他医学 VLM。这项工作标志着利用 VLM 的功能和放射科医生的领域知识来提高医学成像中人工智能模型的能力的潜力，为以人为中心的人工智能计算机辅助诊断铺平了一条新的道路。]]></description>
      <guid>https://arxiv.org/abs/2404.02370</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>一种噪音统治一切：具有普遍扰动的多视图对抗攻击</title>
      <link>https://arxiv.org/abs/2404.02287</link>
      <description><![CDATA[arXiv:2404.02287v1 公告类型：新
摘要：本文提出了一种新颖的通用扰动方法，用于在 3D 对象识别中生成鲁棒的多视图对抗示例。与仅限于单一视图的传统攻击不同，我们的方法在多个 2D 图像上运行，为增强模型的可扩展性和鲁棒性提供了实用且可扩展的解决方案。这种可推广的方法弥合了 2D 扰动和类 3D 攻击能力之间的差距，使其适合现实世界的应用。
  当图像经历光照、相机位置或自然变形等变化时，现有的对抗性攻击可能会变得无效。我们通过制作适用于各种对象视图的单一通用噪声扰动来应对这一挑战。对不同渲染 3D 对象的实验证明了我们方法的有效性。通用扰动成功地从多个姿势和视点为每个给定的 3D 对象渲染集识别出单个对抗性噪声。与单视图攻击相比，我们的通用攻击降低了多个视角的分类置信度，尤其是在低噪声水平下。 https://github.com/memoatwit/UniversalPerturbation 提供了示例实现。]]></description>
      <guid>https://arxiv.org/abs/2404.02287</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>GaitSTR：具有顺序两流细化的步态识别</title>
      <link>https://arxiv.org/abs/2404.02345</link>
      <description><![CDATA[arXiv:2404.02345v1 公告类型：新
摘要：步态识别旨在根据人的行走序列来识别人，作为一种有用的生物识别方式，因为它可以从远距离观察而无需主体的合作。在表示一个人的行走顺序时，轮廓和骨骼是使用的两种主要形式。当不同身体部位之间发生重叠并且受到携带的物体和衣服的影响时，剪影序列缺乏详细的部分信息。骨骼，包括关节和连接关节的骨骼，为不同节段提供更准确的部位信息；然而，它们对遮挡和低质量图像很敏感，导致序列中逐帧结果不一致。在本文中，我们探索使用骨骼的双流表示和轮廓来进行步态识别。通过融合轮廓和骨架的组合数据，我们通过图卷积中的自校正以及具有轮廓时间一致性的跨模式校正来细化双流骨架、关节和骨骼。我们证明，与没有额外注释的最先进方法相比，通过精细化的骨架，步态识别模型的性能可以在公共步态识别数据集上实现进一步的改进。]]></description>
      <guid>https://arxiv.org/abs/2404.02345</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>平滑深度显着性</title>
      <link>https://arxiv.org/abs/2404.02282</link>
      <description><![CDATA[arXiv:2404.02282v1 公告类型：新
摘要：在这项工作中，我们研究了减少来自卷积下采样的深度显着图中噪声的方法，目的是解释深度学习模型如何检测扫描的组织学组织样本中的肿瘤。这些方法使得所研究的模型对于在隐藏层中计算的基于梯度的显着性图更具可解释性。我们在 ImageNet1K 上进行图像分类训练的不同模型、Camelyon16 上进行肿瘤检测训练的模型以及染色组织样本的内部真实世界数字病理扫描训练的模型上测试我们的方法。我们的结果表明，梯度中的棋盘噪声减少了，从而导致更平滑，因此更容易解释显着图。]]></description>
      <guid>https://arxiv.org/abs/2404.02282</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>LP++：用于少样本 CLIP 的超强线性探头</title>
      <link>https://arxiv.org/abs/2404.02285</link>
      <description><![CDATA[arXiv:2404.02285v1 公告类型：新
摘要：在最近关于小样本 CLIP 适应的新兴文献中，线性探针（LP）经常被报道为弱基线。这促使人们深入研究建立复杂的即时学习或特征适应策略。在这项工作中，我们从凸优化的角度提出并检查了标准 LP 基线的泛化，其中线性分类器权重是文本嵌入的可学习函数，并具有混合图像和文本知识的类乘法器。由于我们的目标函数取决于两种类型的变量，即类视觉原型和可学习的混合参数，因此我们提出了一种计算高效的块坐标Majorize-Minimize（MM）下降算法。在我们的全批量 MM 优化器（我们称之为 LP++）中，步长是隐式的，这与标准梯度下降实践不同，在标准梯度下降实践中，学习率是在验证集上进行集中搜索的。通过检查损失的数学特性（例如，Lipschitz 梯度连续性），我们构建了产生数据驱动的学习率的主函数，并得出损失最小值的近似值，从而提供了变量的数据通知初始化。令人惊讶的是，我们的图像语言目标函数以及这些重要的优化见解和成分，产生了极具竞争力的少样本 CLIP 性能。此外，LP++ 在黑盒中运行，放松了对优化超参数的密集验证搜索，并且运行速度比最先进的少样本 CLIP 适应方法快几个数量级。我们的代码位于：\url{https://github.com/FereshteShakeri/FewShot-CLIP-Strong-Baseline.git}。]]></description>
      <guid>https://arxiv.org/abs/2404.02285</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>SnAG：可扩展且准确的视频接地</title>
      <link>https://arxiv.org/abs/2404.02257</link>
      <description><![CDATA[arXiv:2404.02257v1 公告类型：新
摘要：视频中文本描述的时间基础是视觉语言学习和视频理解的中心问题。现有的方法通常优先考虑准确性而不是可扩展性——它们已经过优化，只能在短视频中进行少量文本查询，并且无法扩展到具有数百个查询的长视频。在本文中，我们研究了跨模态融合对视频接地模型可扩展性的影响。我们的分析表明，后期融合对于具有许多文本查询的长格式视频来说是一种更具成本效益的融合方案。此外，它还为我们带来了一种新颖的、以视频为中心的采样方案，以实现高效的训练。基于这些发现，我们提出了 SnAG，这是一个用于可扩展且准确的视频接地的简单基线。 SnAG 没有任何附加功能，其准确度比 CONE 高 43%，速度快 1.5 倍，CONE 是基于具有挑战性的 MAD 数据集的最先进的长视频视频，同时在短视频上取得了极具竞争力的结果。]]></description>
      <guid>https://arxiv.org/abs/2404.02257</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>OFMPNet：城市环境中占用和流量预测的深度端到端模型</title>
      <link>https://arxiv.org/abs/2404.02263</link>
      <description><![CDATA[arXiv:2404.02263v1 公告类型：新
摘要：运动预测任务对于自动驾驶系统至关重要，它为选择车辆在周围环境中的行为策略提供了关键数据。现有的运动预测技术主要集中于利用其过去的轨迹数据来单独预测场景中每个智能体的未来轨迹。在本文中，我们介绍了一种端到端神经网络方法，旨在预测环境中所有动态对象的未来行为。这种方法利用了占用图和场景的运动流。我们正在研究构建称为 OFMPNet 的深度编码器-解码器模型的各种替代方案。该模型使用一系列鸟瞰道路图像、占用网格和先验运动流作为输入数据。模型的编码器可以包含变换器、基于注意力的单元或卷积单元。解码器考虑使用卷积模块和循环块。此外，我们提出了一种新颖的时间加权运动流损失，其应用表明端点误差大幅减少。我们的方法在 Waymo 占用和流量预测基准上取得了最先进的结果，在 Flow-Grounded Occupancy 上的软 IoU 为 52.1%，AUC 为 76.75%。]]></description>
      <guid>https://arxiv.org/abs/2404.02263</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>保存的检查点的线性组合使一致性和扩散模型更好</title>
      <link>https://arxiv.org/abs/2404.02241</link>
      <description><![CDATA[arXiv:2404.02241v1 公告类型：新
摘要：扩散模型（DM）和一致性模型（CM）是两种流行的生成模型，在各种任务上都具有良好的生成质量。在训练 DM 和 CM 时，没有充分利用中间权重检查点，仅使用最后一个收敛的检查点。在这项工作中，我们发现高质量的模型权重通常位于 SGD 无法达到的盆地中，但可以通过适当的检查点平均来获得。基于这些观察，我们提出了 LCSC，这是一种简单但有效且高效的方法，通过将训练轨迹上的检查点与进化搜索推导出的系数相结合来增强 DM 和 CM 的性能。我们通过两个用例展示了 LCSC 的价值： $\textbf{(a) 降低训练成本。}$ 使用 LCSC，我们只需要用更少的迭代次数和/或更低的批量大小来训练 DM/CM 即可获得可比较的样本经过充分训练的模型的质量。例如，LCSC 为 CM 实现了相当大的训练加速（CIFAR-10 上为 23$\times$，ImageNet-64 上为 15$\times$）。 $\textbf{(b) 增强预训练模型。}$ 假设已经完成完整训练，LCSC 可以进一步提高最终收敛模型的生成质量或速度。例如，在一致性蒸馏上，LCSC 使用 1 次函数评估 (NFE) 获得了比使用 2 个 NFE 的基本模型更好的性能，并将 DM 的 NFE 从 15 降低到 9，同时保持了 CIFAR-10 上的生成质量。我们的代码可在 https://github.com/imagination-research/LCSC 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.02241</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>通过对抗性学习实现稳健的 3D 姿势迁移</title>
      <link>https://arxiv.org/abs/2404.02242</link>
      <description><![CDATA[arXiv:2404.02242v1 公告类型：新
摘要：3D 姿态迁移旨在将所需姿态迁移到目标网格，是最具挑战性的 3D 生成任务之一。之前的尝试依赖于明确定义的参数化人体模型或骨骼关节作为驾驶姿势源。然而，为了获得这些干净的姿态源，繁琐但必要的预处理管道是不可避免的，这阻碍了实时应用程序的实现。这项工作的直觉是，可以通过在训练中引入对抗性样本来增强模型的鲁棒性，从而形成一个对噪声输入更无懈可击的模型，甚至可以进一步扩展到直接处理现实世界的数据，例如原始点云/扫描，无需中间处理。此外，我们提出了一种新颖的 3D 姿势 Masked Autoencoder (3D-PoseMAE)，这是一种定制的 MAE，可以有效地学习 3D 外部表示（即姿势）。 3D-PoseMAE 通过同时生成干扰模型的对抗样本并通过多尺度掩蔽策略学习任意原始噪声姿势，促进从外部属性方面进行学习。定性和定量研究都表明，我们的网络给出的转移网格会产生更好的质量。此外，我们还证明了我们的方法在各种姿势、不同领域甚至原始扫描上的强大通用性。实验结果还显示了有意义的见解，即训练中生成的中间对抗样本可以成功攻击现有的姿势迁移模型。]]></description>
      <guid>https://arxiv.org/abs/2404.02242</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>OOSTraj：利用视觉定位去噪进行视距外轨迹预测</title>
      <link>https://arxiv.org/abs/2404.02227</link>
      <description><![CDATA[arXiv:2404.02227v1 公告类型：新
摘要：轨迹预测是计算机视觉和自动驾驶的基础，特别是对于理解行人行为和实现主动决策而言。该领域的现有方法通常假设精确且完整的观测数据，忽略了与视野外物体相关的挑战以及由于相机范围有限、物理障碍物以及去噪传感器数据缺乏地面实况而导致的传感器数据固有的噪声。 。这种疏忽是严重的安全问题，因为它们可能会导致丢失重要的、不可见的物体。为了弥补这一差距，我们提出了一种利用视觉定位技术进行视线外轨迹预测的新颖方法。我们的方法以无监督的方式对噪声传感器观测进行降噪，并将基于传感器的视线外物体的轨迹精确地映射到视觉轨迹中。该方法在 Vi-Fi 和 JRDB 数据集上的视线外噪声传感器轨迹去噪和预测方面展示了最先进的性能。通过提高轨迹预测准确性并解决视线外物体的挑战，我们的工作为提高复杂环境下自动驾驶的安全性和可靠性做出了重大贡献。我们的工作代表了视线外轨迹预测（OOSTraj）的第一个举措，为未来的研究树立了新的基准。代码可在 \url{https://github.com/Hai-chao-Zhang/OOSTraj} 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.02227</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>视觉概念连接组（VCC）：开放世界概念发现及其深层模型中的层间连接</title>
      <link>https://arxiv.org/abs/2404.02233</link>
      <description><![CDATA[arXiv:2404.02233v1 公告类型：新
摘要：理解深度网络模型在其学习表示中捕获的内容是计算机视觉中的一个基本挑战。我们提出了一种理解此类视觉模型的新方法，即视觉概念连接组（VCC），它以完全无监督的方式发现人类可解释的概念及其层间连接。我们的方法同时揭示了一层的细粒度概念、所有层之间的连接权重，并且可修正网络结构的全局分析（例如，分层概念组件的分支模式）。以前的工作提出了从单层中提取可解释概念并检查其对分类的影响的方法，但没有提供跨整个网络架构的多层概念分析。定量和定性的实证结果表明了 VCC 在图像分类领域的有效性。此外，我们利用 VCC 进行故障模式调试应用，以揭示深层网络中出现错误的位置。]]></description>
      <guid>https://arxiv.org/abs/2404.02233</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>NeRFCodec：神经特征压缩与神经辐射场相结合，实现内存高效的场景表示</title>
      <link>https://arxiv.org/abs/2404.02185</link>
      <description><![CDATA[arXiv:2404.02185v1 公告类型：新
摘要：神经辐射场（NeRF）的出现极大地影响了 3D 场景建模和新颖视图合成。作为一种用于3D场景表示的视觉媒体，具有高率失真性能的压缩是永恒的目标。受神经压缩和神经场表示进步的推动，我们提出了 NeRFCodec，这是一种端到端 NeRF 压缩框架，集成了非线性变换、量化和熵编码，以实现内存高效的场景表示。由于直接在大规模 NeRF 特征平面上训练非线性变换是不切实际的，我们发现预先训练的神经 2D 图像编解码器可用于在添加特定于内容的参数时压缩特征。具体来说，我们重用神经二维图像编解码器，但修改其编码器和解码器头，同时保持预训练解码器的其他部分冻结。这使我们能够通过监督渲染损失和熵损失来训练完整的管道，通过更新特定于内容的参数来产生率失真平衡。在测试时，传输包含潜在代码、特征解码器头和其他辅助信息的比特流以进行通信。实验结果表明，我们的方法优于现有的 NeRF 压缩方法，能够以 0.5 MB 的内存预算实现高质量的新颖视图合成。]]></description>
      <guid>https://arxiv.org/abs/2404.02185</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>选择：多视图深度细化的对比假设选择</title>
      <link>https://arxiv.org/abs/2404.02225</link>
      <description><![CDATA[arXiv:2404.02225v1 公告类型：新
摘要：我们提出了 CHOSEN，一个简单而灵活、健壮且有效的多视图深度细化框架。它可用于任何现有的多视图立体管道，具有针对不同多视图捕获系统（例如相机相对定位和镜头）的直接泛化能力。给定初始深度估计，CHOSEN 迭代地重新采样并选择最佳假设，并自动适应捕获系统确定的不同度量或内在尺度。我们方法的关键是在适当的解决方案空间和精心设计的假设特征中应用对比学习，基于此可以有效区分正假设和负假设。与当前许多基于深度学习的多视图立体管道相比，CHOSEN 集成在简单的基线多视图立体管道中，在深度和正常精度方面提供了令人印象深刻的质量。]]></description>
      <guid>https://arxiv.org/abs/2404.02225</guid>
      <pubDate>Thu, 04 Apr 2024 06:17:08 GMT</pubDate>
    </item>
    </channel>
</rss>