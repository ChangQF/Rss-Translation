<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 03 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>有限标签卫星图像中农田边界分割的多区域迁移学习</title>
      <link>https://arxiv.org/abs/2404.00179</link>
      <description><![CDATA[arXiv:2404.00179v1 公告类型：新
摘要：田地边界描绘的目标是预测高空遥感图像（例如来自卫星或无人机）中各个作物田地的多边形边界和内部。自动划定田地边界是许多现实世界农业用例的一项必要任务，例如估计一个地区的耕地面积或预测田地的季末产量。场边界描绘可以被视为实例分割问题，但与用于实例分割的传统计算机视觉数据集相比，提出了独特的研究挑战。先前工作的实际适用性还受到以下假设的限制：在应用田界划分模型时可以获得足够大的标记数据集，但这对于大多数地区（特别是撒哈拉以南非洲等资源贫乏地区）来说并非现实）。我们提出了一种在缺乏标记数据的区域中对卫星图像中的作物田地边界进行分割的方法，该方法使用多区域迁移学习来调整目标区域的模型权重。我们表明，我们的方法优于现有方法，并且多区域迁移学习大大提高了多个模型架构的性能。我们的实施和数据集是公开的，以便最终用户能够使用该方法，并作为未来工作的基准。]]></description>
      <guid>https://arxiv.org/abs/2404.00179</guid>
      <pubDate>Wed, 03 Apr 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>关于主动视觉系统固有的对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2404.00185</link>
      <description><![CDATA[arXiv:2404.00185v1 公告类型：新
摘要：当前的深度神经网络很容易受到对抗性示例的影响，这些示例会通过添加精心设计的噪声来改变其预测。由于人眼对此类输入具有鲁棒性，因此该漏洞可能源于通过处理具有相同重要性的每个像素来一次性处理输入的标准方法。相比之下，神经科学表明，人类视觉系统可以通过以下方式区分显着特征：（1）在多个注视点（扫视）之间切换和（2）以不均匀的外部分辨率（注视点）处理周围环境。在这项工作中，我们主张将这种主动视觉机制集成到当前的深度学习系统中可以提供鲁棒性优势。具体来说，我们凭经验证明了两种主动视觉方法（GFNet 和 FALcon）在黑盒威胁模型下的固有鲁棒性。通过基于从输入内多个不同注视点获得的下采样瞥见进行学习和推理，我们表明，与标准被动卷积网络相比，这些主动方法在最先进的对抗性攻击下实现了 (2-3) 倍的鲁棒性。更重要的是，我们提供了说明性和可解释的可视化分析，展示了如何从不同的注视点执行推理使主动视觉方法不易受到恶意输入的影响。]]></description>
      <guid>https://arxiv.org/abs/2404.00185</guid>
      <pubDate>Wed, 03 Apr 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>通过反事实揭示大型视觉语言模型中的偏见</title>
      <link>https://arxiv.org/abs/2404.00166</link>
      <description><![CDATA[arXiv:2404.00166v1 公告类型：新
摘要：随着具有越来越令人印象深刻的功能的大型语言模型（LLM）的出现，人们提出了许多大型视觉语言模型（LVLM）来通过视觉输入来增强 LLM。此类模型在输入图像和文本提示上条件化生成的文本，从而实现各种用例，例如视觉问答和多模式聊天。虽然之前的研究已经考察了法学硕士生成的文本中包含的社会偏见，但这一主题在 LVLM 中相对未被探索。检查 LVLM 中的社会偏见尤其具有挑战性，因为文本和视觉模式中包含的信息引起的偏见具有混杂的影响。为了解决这个具有挑战性的问题，我们对不同 LVLM 在输入图像的反事实变化下生成的文本进行了大规模研究。具体来说，我们向 LVLM 提供相同的开放式文本提示，同时以来自不同反事实集合的图像为条件，其中每个集合包含的图像在描述共同主题（例如医生）时基本相同，但仅在以下方面有所不同：交叉社会属性（例如种族和性别）。我们全面评估了这种反事实生成环境下不同 LVLM 生成的文本，发现输入图像中描述的种族、性别和身体特征等社会属性可以显着影响毒性和能力相关词的生成。]]></description>
      <guid>https://arxiv.org/abs/2404.00166</guid>
      <pubDate>Wed, 03 Apr 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>动态城市环境的多级神经场景图</title>
      <link>https://arxiv.org/abs/2404.00168</link>
      <description><![CDATA[arXiv:2404.00168v1 公告类型：新
摘要：我们通过不同环境条件下的多个车辆捕获来估计大规模动态区域的辐射场。该领域之前的作品要么仅限于静态环境，要么无法扩展到多个短视频，要么难以单独表示动态对象实例。为此，我们针对动态城市环境提出了一种新颖的、可分解的辐射场方法。我们提出了一种多级神经场景图表示，可以从具有数百个快速移动对象的数十个序列中扩展到数千个图像。为了能够有效地训练和渲染我们的表示，我们开发了一种快速复合射线采样和渲染方案。为了在城市驾驶场景中测试我们的方法，我们引入了一种新的、新颖的视图合成基准。我们表明，我们的方法在已建立的基准和我们提出的基准上都明显优于现有技术，同时训练和渲染速度更快。]]></description>
      <guid>https://arxiv.org/abs/2404.00168</guid>
      <pubDate>Wed, 03 Apr 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>通过深度数据和深度度量学习进行通用牛识别</title>
      <link>https://arxiv.org/abs/2404.00172</link>
      <description><![CDATA[arXiv:2404.00172v1 公告类型：新
摘要：本文首次提出并评估了一种自上而下（背视图）、仅深度的深度学习系统，用于准确识别个体牛，并提供相关代码、数据集和训练权重以实现立即再现性。牛群规模的增加扭曲了农场的牛与人的比例，并使对个体的手动监控更具挑战性。因此，实时牛群识别对于农场来说至关重要，也是实现精准畜牧业的关键一步。在我们之前工作的基础上，本文介绍了一种使用现成 3D 相机的深度数据进行牛识别的深度度量学习方法。该方法依赖于 CNN 和 MLP 主干网络，从身体形状中学习通用的嵌入空间来区分个体——操作时既不需要特定物种的皮毛图案，也不需要特写枪口印记。网络嵌入使用简单的算法（例如 $k$-NN）进行聚类，以实现高度准确的识别，从而无需重新训练网络来注册新个体。我们评估了两种主干架构：ResNet（之前用于使用 RGB 图像识别荷斯坦弗里斯兰牛）和 PointNet（专门用于在 3D 点云上运行）。我们还推出了 CowDepth2023，这是一个新数据集，包含 99 头奶牛的 21,490 个同步颜色深度图像对，用于评估主干网。 ResNet 和 PointNet 架构分别使用深度图和点云，从而实现了与基于外套图案的主干网相当的高精度。]]></description>
      <guid>https://arxiv.org/abs/2404.00172</guid>
      <pubDate>Wed, 03 Apr 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>VSRD：用于弱监督 3D 对象检测的实例感知体积轮廓渲染</title>
      <link>https://arxiv.org/abs/2404.00149</link>
      <description><![CDATA[arXiv:2404.00149v1 公告类型：新
摘要：单目 3D 目标检测由于其在单目深度估计中固有的不适定性质，对 3D 场景理解提出了重大挑战。现有方法严重依赖使用丰富的 3D 标签的监督学习，这些标签通常是通过激光雷达点云上昂贵且劳动密集型的注释获得的。为了解决这个问题，我们提出了一种名为 VSRD（体积轮廓渲染检测）的新型弱监督 3D 对象检测框架，用于在没有任何 3D 监督而只有弱 2D 监督的情况下训练 3D 对象检测器。 VSRD 包括多视图 3D 自动标记以及使用自动标记阶段生成的伪标签对单目 3D 对象检测器进行后续训练。在自动标记阶段，我们将每个实例的表面表示为有符号距离场（SDF），并通过我们提出的实例感知体积轮廓渲染将其轮廓渲染为实例掩模。为了通过渲染直接优化 3D 边界框，我们将每个实例的 SDF 分解为长方体的 SDF 和表示长方体残差的残差距离场 (RDF)。这种机制使我们能够通过将渲染的实例掩码与地面实况实例掩码进行比较，以端到端的方式优化 3D 边界框。优化后的 3D 边界框可作为 3D 对象检测的有效训练数据。我们在 KITTI-360 数据集上进行了广泛的实验，证明我们的方法优于现有的弱监督 3D 对象检测方法。该代码可从 https://github.com/skmhrk1209/VSRD 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.00149</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>使用联合监督和对抗学习的 CT 呼吸运动合成</title>
      <link>https://arxiv.org/abs/2404.00163</link>
      <description><![CDATA[arXiv:2404.00163v1 公告类型：新
摘要：目的：四维计算机断层扫描 (4DCT) 成像是将 CT 采集重建为多个阶段，以跟踪内部器官和肿瘤运动。它通常用于放射治疗计划中以建立计划目标体积。然而，4DCT 增加了方案的复杂性，可能与治疗期间患者的呼吸不一致，并导致更高的辐射输送。方法：在本研究中，我们提出了一种深度合成方法，从静态图像生成伪呼吸 CT 相位，用于运动感知治疗计划。该模型通过基于外部患者表面估计的调节合成来生成患者特定的变形矢量场 (DVF)，模仿呼吸监测设备。一个关键的方法贡献是通过监督 DVF 训练来鼓励 DVF 真实性，同时不仅在扭曲图像上而且在 DVF 本身的大小上联合使用对抗性术语。这样，我们避免了通常通过深度无监督学习获得的过度平滑，并鼓励与呼吸幅度的相关性。主要结果：使用真实的 4DCT 采集来评估性能，肿瘤体积比之前报道的更小。结果首次证明，生成的伪呼吸 CT 相位可以捕获器官和肿瘤运动，其准确度与同一患者的重复 4DCT 扫描相似。对于真实 4DCT 相，平均扫描间肿瘤质心距离和 Dice 相似系数分别为 $1.97$mm 和 $0.63$，对于合成相，平均扫描间肿瘤质心距离和 $0.71$ 分别为 $2.35$mm 和 $0.71$，并且与状态相比毫不逊色。 -艺术技术（RMSim）。]]></description>
      <guid>https://arxiv.org/abs/2404.00163</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>FISBe：用于远程细丝状结构实例分割的真实世界基准数据集</title>
      <link>https://arxiv.org/abs/2404.00130</link>
      <description><![CDATA[arXiv:2404.00130v1 公告类型：新
摘要：神经系统体积光学显微镜图像中神经元的实例分割，通过促进细胞分辨率下神经回路的联合功能和形态学分析，实现了神经科学领域的突破性研究。然而，所述多神经元光学显微镜数据对于实例分割任务表现出极具挑战性的特性：单个神经元具有长距离、细丝状和广泛分支的形态，多个神经元紧密地交织在一起，以及部分体积效应、不均匀的照明和噪声光学显微镜固有的缺陷严重阻碍了局部解缠结以及单个神经元的远程追踪。这些属性反映了机器学习研究当前的一个关键挑战，即有效捕获数据中的远程依赖性。虽然各自的方法论研究很热门，但迄今为止的方法通常以合成数据集为基准。为了解决这一差距，我们发布了 FlyLight 实例分割基准 (FISBe) 数据集，这是第一个公开的具有像素级注释的多神经元光学显微镜数据集。此外，我们定义了一组用于基准测试的实例分段指标，我们设计这些指标对于下游分析有意义。最后，我们提供了三个基线来启动一场竞赛，我们希望这场竞赛能够推动机器学习领域有关捕获远程数据依赖性的方法的发展，并促进基础神经科学中的科学发现。]]></description>
      <guid>https://arxiv.org/abs/2404.00130</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>用于精确恢复和稀疏逼近的快速 OMP</title>
      <link>https://arxiv.org/abs/2404.00146</link>
      <description><![CDATA[arXiv:2404.00146v1 公告类型：新
摘要：正交匹配追踪（OMP）已成为稀疏信号恢复和逼近的有效方法。然而，当信号具有大量非零时，OMP 会遇到计算问题。本文在两个方面对 OMP 进行了改进：它提供了一种在每次迭代时对输入信号进行正交投影的快速算法，以及一种用于进行贪婪选择的新选择标准，从而减少了恢复信号所需的迭代次数。所提出的对 OMP 的修改直接降低了计算复杂度。实验结果表明，在计算时间上比经典的 OMP 有显着的改进。论文还为新的贪婪选择准则下的精确恢复提供了充分条件。对于可能没有稀疏表示的一般信号，论文提供了近似误差的界限。近似误差与 OMP 的量级相同，但在更少的迭代和更少的时间内获得。]]></description>
      <guid>https://arxiv.org/abs/2404.00146</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>通过正交融合和遮挡处理进行鲁棒整体人员重新识别</title>
      <link>https://arxiv.org/abs/2404.00107</link>
      <description><![CDATA[arXiv:2404.00107v1 公告类型：新
摘要：由于姿势的多样性和外观的变化，遮挡仍然是行人重新识别（ReID）的主要挑战之一。开发新颖的架构来提高遮挡感知行人重识别的鲁棒性需要新的见解，特别是在低分辨率边缘相机方面。我们提出了一种深度集成模型，利用 CNN 和 Transformer 架构来生成稳健的特征表示。为了在不需要手动标记遮挡区域的情况下实现鲁棒的 Re-ID，我们建议采用一种基于集成学习的方法，该方法源自任意形状的遮挡区域和鲁棒特征表示之间的类比。利用正交原理，我们开发的深度 CNN 模型利用掩模自动编码器 (MAE) 和全局局部特征融合来实现稳健的人员识别。此外，我们提出了一个部分遮挡感知转换器，能够学习对遮挡区域具有鲁棒性的特征空间。在几个 Re-ID 数据集上报告了实验结果，以显示我们开发的名为正交融合与遮挡处理（OFOH）的集成模型的有效性。与竞争方法相比，所提出的 OFOH 方法已实现了足够的 Rank-1 和 mAP 性能。]]></description>
      <guid>https://arxiv.org/abs/2404.00107</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>Deepfake Sentry：利用集成智能进行弹性检测和泛化</title>
      <link>https://arxiv.org/abs/2404.00114</link>
      <description><![CDATA[arXiv:2404.00114v1 公告类型：新
摘要：生成对抗网络（GAN）的最新进展使得高质量的逼真图像生成成为可能。然而，对此类生成媒体的恶意使用引起了人们对视觉错误信息的担忧。尽管深度伪造检测研究已经证明了高精度，但它很容易受到生成技术的进步和检测对策的对抗性迭代的影响。为了解决这个问题，我们提出了一种主动且可持续的 Deepfake 训练增强解决方案，将人工指纹引入模型中。我们通过采用集成学习方法来实现这一目标，该方法包含一组自动编码器，可以模仿 Deepfake 生成器模型引入的人工制品的效果。对三个数据集的实验表明，我们提出的基于集成自动编码器的数据增强学习方法在泛化性、对基本数据扰动（如噪声、模糊、锐度增强和仿射变换）的抵抗力、对常用有损压缩算法（如JPEG，并增强了对对抗性攻击的抵抗力。]]></description>
      <guid>https://arxiv.org/abs/2404.00114</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>AgileFormer：用于医学图像分割的空间敏捷 Transformer UNet</title>
      <link>https://arxiv.org/abs/2404.00122</link>
      <description><![CDATA[arXiv:2404.00122v1 公告类型：新
摘要：在过去的几十年中，深度神经网络，特别是卷积神经网络，在各种医学图像分割任务中取得了最先进的性能。最近，视觉变换器（ViT）的引入显着改变了深度分割模型的格局。由于 ViT 卓越的性能和可扩展性，人们越来越关注 ViT。然而，我们认为基于视觉变换器的 UNet (ViT-UNet) 分割模型的当前设计可能无法有效处理医学图像分割任务中感兴趣对象的异构外观（例如，不同的形状和大小）。为了应对这一挑战，我们提出了一种结构化方法，将空间动态组件引入 ViT-UNet。这种适应使模型能够有效地捕获具有不同外观的目标对象的特征。这是通过三个主要组件实现的： \textbf{(i)} 可变形补丁嵌入； \textbf{(ii)} 空间动态多头注意力； \textbf{(iii)} 可变形位置编码。这些组件被集成到一个新颖的架构中，称为 AgileFormer。 AgileFormer 是一个空间敏捷的 ViT-UNet，专为医学图像分割而设计。使用公开数据集进行的三个分割任务的实验证明了所提出方法的有效性。该代码可在 \href{https://github.com/sotiraslab/AgileFormer}{https://github.com/sotiraslab/AgileFormer} 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.00122</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>GDA：用于稳健测试时间适应的广义扩散</title>
      <link>https://arxiv.org/abs/2404.00095</link>
      <description><![CDATA[arXiv:2404.00095v2 公告类型：新
摘要：当遇到具有意外分布变化的分布外（OOD）样本时，机器学习模型很难泛化。对于视觉任务，最近的研究表明，采用扩散模型的测试时适应可以通过生成与模型域一致的新样本来实现 OOD 样本的最先进的精度改进，而无需修改模型的权重。不幸的是，这些研究主要集中在像素级损坏上，因此缺乏适应更广泛的 OOD 类型的泛化能力。我们引入了广义扩散适应（GDA），这是一种新颖的基于扩散的测试时间适应方法，对不同的 OOD 类型具有鲁棒性。具体来说，GDA 通过应用从模型导出的边际熵损失，结合反向采样过程中的风格和内容保留损失，迭代地引导扩散。换句话说，GDA将模型的输出行为与样本的语义信息作为一个整体来考虑，这可以减少生成过程中下游任务的歧义。对各种流行模型架构和 OOD 基准的评估表明，GDA 始终优于先前在扩散驱动适应方面的工作。值得注意的是，它实现了最高的分类精度改进，在 ImageNet-C 上提​​高了 4.4% 到 5.02%，在 Rendition、Sketch 和 Stylized 基准上提高了 2.5% 到 7.4%。这一性能凸显了 GDA 对更广泛的 OOD 基准的推广。]]></description>
      <guid>https://arxiv.org/abs/2404.00095</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>稀疏视图，近光：未校准点光光度立体的实用范例</title>
      <link>https://arxiv.org/abs/2404.00098</link>
      <description><![CDATA[arXiv:2404.00098v1 公告类型：新
摘要：神经方法在基于相机的重建方面取得了重大进展。但它们要么需要对视域进行相当密集的采样，要么需要对现有数据集进行预训练，从而限制了它们的通用性。相比之下，光度立体（PS）方法显示出在稀疏视点下实现高质量重建的巨大潜力。然而，它们是不切实际的，因为它们通常需要繁琐的实验室条件，仅限于暗室，并且通常是多阶段的，这使得它们容易出现累积的错误。为了解决这些缺点，我们提出了一种端到端的未校准多视图 PS 框架，用于重建从现实环境中的稀疏视点获取的高分辨率形状。我们放松了暗室假设，并允许静态环境照明和动态近 LED 照明的组合，从而可以在实验室外轻松捕获数据。实验验证证实，它在稀疏视点的情况下大大优于现有的基线方法。这样可以将高精度 3D 重建从暗室带到现实世界，同时保持合理的数据捕获复杂性。]]></description>
      <guid>https://arxiv.org/abs/2404.00098</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>DVIS-DAQ：通过动态锚点查询改进视频分割</title>
      <link>https://arxiv.org/abs/2404.00086</link>
      <description><![CDATA[arXiv:2404.00086v1 公告类型：新
摘要：现代视频分割方法采用对象查询来执行帧间关联，尽管存在大规模运动和瞬态遮挡，但在跟踪连续出现的对象方面表现出令人满意的性能。
  然而，它们在现实世界中常见的新出现和消失的对象上都表现不佳，因为它们试图通过具有显着特征差距的背景和前景查询之间的特征转换来建模对象的出现和消失。我们引入动态锚查询（DAQ），通过根据潜在候选者的特征动态生成锚查询来缩短锚查询和目标查询之间的转换差距。
  此外，我们还引入了查询级对象出现和消失模拟 (EDS) 策略，该策略无需任何额外成本即可释放 DAQ 的潜力。
  最后，我们将我们提出的 DAQ 和 EDS 与 DVIS~\cite{zhang2023dvis} 结合起来以获得 DVIS-DAQ。
  大量实验表明，DVIS-DAQ 在五个主流视频分割基准上实现了新的最先进 (SOTA) 性能。代码和模型可在 \url{https://github.com/SkyworkAI/DAQ-VS} 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.00086</guid>
      <pubDate>Wed, 03 Apr 2024 06:16:55 GMT</pubDate>
    </item>
    </channel>
</rss>