<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Wed, 27 Dec 2023 06:17:51 GMT</lastBuildDate>
    <item>
      <title>FineMoGen：细粒度时空运动生成和编辑。 （arXiv：2312.15004v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.15004</link>
      <description><![CDATA[文本驱动的动作生成已经取得了实质性进展
扩散模型的出现。然而，现有的方法仍然难以
生成与细粒度描述相对应的复杂运动序列，
描绘详细而准确的时空动作。这种缺乏精
可控性限制了运动生成对更多受众的使用。到
为了应对这些挑战，我们推出了 FineMoGen，一种基于扩散的运动
可以合成细粒度运动的生成和编辑框架，
时空组合向用户指示。具体来说，FineMoGen
基于扩散模型，采用一种称为
时空混合注意力（SAMI）。 SAMI 优化了生成
全局注意力模板从两个角度：1）显式建模
时空组成的约束； 2）利用稀疏激活
专家混合自适应地提取细粒度特征。为了方便
对这种新的细粒度运动生成任务的大规模研究，我们
贡献 HuMMan-MoGen 数据集，其中包含 2,968 个视频和 102,336 个视频
细粒度的时空描述。大量实验证实
FineMoGen 展现出超越最先进技术的卓越运动生成质量
方法。值得注意的是，FineMoGen 进一步实现了零镜头运动编辑
借助现代大语言模型（LLM）的能力，
通过细粒度的指令忠实地操纵运动序列。项目
页面：https://mingyuan-zhang.github.io/projects/FineMoGen.html
]]></description>
      <guid>http://arxiv.org/abs/2312.15004</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>SI-MIL：驯服深层 MIL 以实现十亿像素组织病理学的自我解释。 （arXiv：2312.15010v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.15010</link>
      <description><![CDATA[将可解释性和推理引入多实例学习
考虑到整个幻灯片图像（WSI）分析的（MIL）方法具有挑战性
十亿像素幻灯片的复杂性。传统上，MIL 可解释性有限
识别与下游任务相关的显着区域，提供
最终用户（病理学家）对背后的基本原理知之甚少
这些选择。为了解决这个问题，我们提出了自解释 MIL (SI-MIL)，
一种从一开始就本质上为可解释性而设计的方法。
SI-MIL 采用深层 MIL 框架来指导扎根的可解释分支
基于手工制作的病理特征，促进线性预测。超过
识别显着区域，SI-MIL 独特地提供特征级
植根于 WSI 病理学见解的解释。值得注意的是，SI-MIL 具有
其线性预测约束挑战了普遍存在的神话
模型可解释性和性能之间不可避免的权衡，
与最先进的方法相比，展示了具有竞争力的结果
三种癌症类型的 WSI 级别预测任务。此外，我们
对 SI-MIL 的本地和全球可解释性进行全面基准测试
统计分析、领域专家研究以及需求
可解释性，即用户友好性和忠实性。
]]></description>
      <guid>http://arxiv.org/abs/2312.15010</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>合成图像有助于识别人造艺术赝品。 （arXiv：2312.14998v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14998</link>
      <description><![CDATA[之前的研究表明人工智能能够
区分特定艺术家的真迹画作和人造画作
伪造品具有极高的准确性，并提供了足够的培训。然而，随着
现有的已知伪造品数量有限，增强方法
伪造检测是非常需要的。在这项工作中，我们研究了潜力
将合成艺术品纳入训练数据集以增强
伪造检测性能。我们的调查重点是绘画
Vincent van Gogh，为此我们发布了第一个专门用于
伪造检测。为了强化我们的结果，我们对
艺术家阿梅代奥·莫迪利亚尼和拉斐尔。我们训练一个分类器来区分
来自赝品的原创艺术品。为此，我们使用人造赝品
模仿著名艺术家的风格并增强我们的训练集
具有由稳定扩散和 StyleGAN 生成的类似风格的图像。我们
发现额外的合成赝品不断提高检测能力
的人造赝品。另外，我们发现，与之前的情况一致
研究中，将合成赝品纳入培训也使得
检测人工智能生成的伪造品，特别是使用类似的方法创建的伪造品
发电机。
]]></description>
      <guid>http://arxiv.org/abs/2312.14998</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>利用栖息地信息进行细粒度鸟类识别。 （arXiv：2312.14999v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14999</link>
      <description><![CDATA[传统的鸟类分类器大多依赖于鸟类的视觉特征
鸟类。一些先前的工作甚至训练分类器使其对
背景，完全抛弃了鸟类的生存环境。相反，我们
率先探索整合栖息地信息，这是四大领域之一
鸟类学家识别鸟类的线索，进入现代鸟类分类器。我们
重点关注两种领先的模型类型：(1) 在下游训练的 CNN 和 ViT
鸟类数据集； (2) 原创的多模式 CLIP。训练 CNN 和 ViT
栖息地增强数据导致改善高达 +0.83 和 +0.23
分别在 NABirds 和 CUB-200 上获得积分。同样，添加栖息地
CLIP 提示的描述符可显着提高准确度
NABirds 和 CUB-200 上分别为 +0.99 和 +1.1 点。我们发现
将栖息地特征集成到模型中后，准确性得到持续提高
图像增强过程并转化为视觉语言的文本描述符
CLIP 分类器。代码可在以下位置获取：
https://anonymous.4open.science/r/reasoning-8B7E/。
]]></description>
      <guid>http://arxiv.org/abs/2312.14999</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>Emage：非自回归文本到图像生成。 （arXiv：2312.14988v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14988</link>
      <description><![CDATA[自回归和扩散模型推动了最近的突破
文本到图像的生成。尽管他们取得了巨大的成功
高真实感图像，这些模型的一个共同缺点是它们的高
推理延迟 - 自回归模型运行超过一千次
依次生成图像标记和扩散模型转换高斯
通过数百个去噪步骤将噪声转化为图像。在这项工作中，我们
探索可有效生成文本到图像的非自回归模型
数百个图像标记并行。我们开发了许多模型变体
不同的学习和推理策略、初始化的文本编码器等。
与需要运行一千次的自回归基线相比，
我们的模型只运行 16 次就可以生成具有竞争质量的图像
推理延迟降低了一个数量级。我们的非自回归模型
346M个参数生成一张256$\times$256的图像，大约一秒
一个 V100 GPU。
]]></description>
      <guid>http://arxiv.org/abs/2312.14988</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>学习促进开放世界持续学习的知识转移。 （arXiv：2312.14990v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.14990</link>
      <description><![CDATA[本文研究了开放世界中的持续学习问题
场景，称为开放世界持续学习（OwCL）。猫头鹰CL是
不断上升，同时在两个方面极具挑战性：i）学习
任务顺序，而不忘记过去的已知信息，以及 ii）识别
未来的未知数（新的对象/类）。现有的 OwCL 方法受到影响
来自已知和未知之间任务感知边界的适应性，以及
不考虑知识转移的机制。在这项工作中，我们建议
Pro-KT，一种新颖的 OwCL 即时增强知识转移模型。 KT原
包括两个关键组件：(1) 一个提示库，用于编码和传输
任务通用和任务特定的知识，以及（2）任务感知的开放集
边界来识别新任务中的未知数。实验结果使用两个
现实世界的数据集表明，所提出的 Pro-KT 优于
在未知物检测和未知物检测方面均具有最先进的水平
显着地对已知进行分类。
]]></description>
      <guid>http://arxiv.org/abs/2312.14990</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>FoodLMM：使用大型多模态模型的多功能食品助手。 （arXiv：2312.14991v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14991</link>
      <description><![CDATA[大型多模态模型 (LMM) 在许多方面取得了令人瞩目的进展
视觉语言任务。尽管如此，一般 LMM 的性能
具体领域还远不能令人满意。本文提出了FoodLMM，
基于 LMM 的多功能食品助手，具有多种功能，包括
食物识别、成分识别、食谱生成、营养
估计、食物分割和多轮对话。为了方便
FoodLMM 为了处理纯文本输出之外的任务，我们引入了一系列
新颖的特定于任务的标记和头部，使模型能够预测食物
营养价值和多重分割掩模。我们采用两阶段
培训策略。第一阶段，我们利用多种公共食品
通过利用指令跟随范式来确定多任务学习的基准。
在第二阶段，我们构建多轮对话和推理
分割数据集来微调模型，使其能够进行
专业对话并根据复杂的情况生成分割掩模
食品领域的推理。我们经过微调的 FoodLMM 达到了最先进的水平
多个食品基准的结果。我们将制作我们的代码、模型和
公开可用的数据集。
]]></description>
      <guid>http://arxiv.org/abs/2312.14991</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:49 GMT</pubDate>
    </item>
    <item>
      <title>通过高度判别性的 LNT 特征增强边缘智能。 (arXiv:2312.14968v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.14968</link>
      <description><![CDATA[边缘人工智能算法需要更小的模型尺寸和更低的计算量
复杂。为了实现这些目标，我们采用绿色学习（GL）
范式而不是深度学习范式。 GL 具有三个模块：1)
无监督表示学习，2) 有监督特征学习，以及 3)
监督决策学习。我们重点关注这项工作的第二个模块。在
特别是，我们从适当的线性组合中得出新的判别特征
在第一个模块中获得的输入特征，用 x 表示。他们叫
分别是互补特征和原始特征。沿着这个思路，我们提出了一个
新颖的监督学习方法来产生高度判别性的互补
基于最小二乘正态变换（LNT）的特征。 LNT 由两个组成
脚步。首先，我们将 C 类分类问题转换为二元分类问题
分类问题。这两个类被分配0和1，
分别。接下来，我们根据以下公式制定最小二乘回归问题
N 维（N-D）特征空间到一维输出空间，并求解
最小二乘法线方程得到一个N维法线向量，记为a1。
由于一个法向量是由一次二元分割产生的，因此我们可以得到 M 个法向量
具有 M 个分割的向量。那么，Ax 被称为 x 的 LNT，其中变换矩阵 A
在 R^{M by N} 中通过堆叠 aj^T, j=1, ..., M，并且 LNT Ax 可以生成 M
新功能。新生成的互补特征显示出更多
比原始特征具有判别性。实验表明，分类
这些新功能可以提高性能。
]]></description>
      <guid>http://arxiv.org/abs/2312.14968</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>高斯和谐：在基于扩散的人脸生成模型中实现公平。 （arXiv：2312.14976v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14976</link>
      <description><![CDATA[扩散模型在人脸生成方面取得了巨大进步。然而，
这些模型放大了生成过程中的偏差，导致
年龄、性别和种族等敏感属性分布不平衡。
本文提出了一种通过平衡面部表情来解决这个问题的新颖方法。
生成图像的属性。我们通过本地化来减轻偏差
使用扩散模型的潜在空间中的面部属性的方法
高斯混合模型（GMM）。我们选择 GMM 而非其他的动机
聚类框架来自于扩散的灵活潜在结构
模型。由于扩散模型中的每个采样步骤都遵循高斯分布
分布，我们表明拟合 GMM 模型可以帮助我们定位
子空间负责生成特定的属性。此外，我们的
方法不需要重新训练，我们而是即时本地化子空间
并减轻生成公平数据集的偏差。我们评估我们的方法
在多个人脸属性数据集上展示我们的有效性
方法。我们的结果表明我们的方法可以带来更公平的数据
在保持质量的同时，在代表性公平性方面进行一代
生成的样本数。
]]></description>
      <guid>http://arxiv.org/abs/2312.14976</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>UniHuman：用于在野外编辑人类图像的统一模型。 （arXiv：2312.14985v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14985</link>
      <description><![CDATA[人类图像编辑包括改变一个人的姿势、他们的
服装，或根据文字提示编辑图像。然而，之前的工作
经常单独处理这些任务，忽视了共同的利益
通过共同学习来强化。在本文中，我们提出了 UniHuman，一个
解决人类图像编辑多个方面的统一模型
现实世界的设置。提高模型的生成质量和
泛化能力，我们利用人类视觉编码器的指导和
引入一个轻量级的姿势扭曲模块，可以利用不同的姿势
表示，容纳看不见的纹理和图案。此外，为了
弥合现有人类编辑基准与现实世界之间的差距
数据，我们策划了 40 万个高质量的人类图像文本对用于训练和
收集了 2K 人类图像进行域外测试，两者都包含不同的内容
服装风格、背景和年龄段。域内和域内实验
域外测试集表明 UniHuman 优于特定任务
模型大幅领先。在用户研究中，UniHuman 受到
平均 77% 的情况下都有用户。
]]></description>
      <guid>http://arxiv.org/abs/2312.14985</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Transformer 进行动态大脑连接组的大规模图表示学习。 （arXiv：2312.14939v1 [q-bio.NC]）</title>
      <link>http://arxiv.org/abs/2312.14939</link>
      <description><![CDATA[Graph Transformers 最近在各种图表中取得了成功
表示学习任务，提供了许多优于
消息传递图神经网络。利用图形转换器
学习大脑功能连接网络的表示是
也获得了兴趣。然而，迄今为止的研究忽视了时间
功能连接的动态，随着时间的推移而波动。在这里，我们
提出一种学习动态泛函表示的方法
与图形转换器的连接。具体来说，我们定义连接组
嵌入，保存了位置、结构和时间信息
功能连接图，并使用 Transformers 学习其表示
跨越时间。我们使用超过 50,000 个静息态 fMRI 样本进行实验
从三个数据集获得，这是使用最多的 fMRI 数据
研究到目前为止。实验结果表明我们提出的方法
在性别分类和年龄方面优于其他竞争基线
基于从功能磁共振成像中提取的功能连接的回归任务
数据。
]]></description>
      <guid>http://arxiv.org/abs/2312.14939</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>用于评估学生在手术打结模拟中表现的级联神经网络系统。 （arXiv：2312.14952v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14952</link>
      <description><![CDATA[作为培训的一部分，所有医学生和住院医生都必须通过
基本的手术任务，例如打结、穿针和缝合。他们的
评估通常由外科医师在手术室进行
学生的错误和失败会增加操作时间，
成本。该评估是定量的，误差范围很小。模拟
已成为一种具有成本效益的选择，但缺乏评估或要求
用于评估的额外昂贵硬件。提供培训视频的应用程序
学生可以进行外科打结尝试，但没有人进行评估。我们
提出一种级联神经网络架构来评估学生的
表演仅来自他们模拟手术打结的视频
任务。我们的模型将视频帧图像转换为特征向量
预先训练的深度卷积网络，然后对帧序列进行建模
与时间网络。我们获得了医学生和住院医师的视频
来自罗伯特伍德约翰逊医院在标准化的打结
模拟套件。我们手动注释每个视频并继续进行
对它们进行五重交叉验证研究。我们的模型达到了中位数
准确率、召回率和 F1 分数分别为 0.71、0.66 和 0.65
确定打结和推结相关任务的级别。我们的
不同概率阈值的平均精度得分为 0.8。
我们的 F1 分数和平均精度分数比
最近发表了针对同一问题的研究。我们期望我们的准确性
随着我们向模型添加更多训练视频，模型会进一步增加
使其成为学生可以用来评估自己的实用解决方案。
]]></description>
      <guid>http://arxiv.org/abs/2312.14952</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>揭示扩散模型中 Unet 的时间动力学。 （arXiv：2312.14965v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14965</link>
      <description><![CDATA[扩散模型已经引起了极大的关注，因为它们可以
有效地学习复杂的多元高斯分布，从而得到
多样化、高质量的成果。他们将高斯噪声引入训练中
数据并迭代重建原始数据。此迭代的核心
process 是一个单一的 Unet，跨时间步骤进行调整以促进生成。
最近的工作揭示了这种情况下的合成和去噪阶段的存在
生成过程中，提出了有关 Unets 不同角色的问题。我们的研究
深入研究 Unets 在去噪扩散中的动态行为
概率模型（DDPM），专注于（反）卷积块和跳过
跨时间步长的连接。我们提出了一种分析方法
系统地评估时间步长和核心 Unet 组件对
最终输出。该方法消除了研究因果关系的成分
研究它们对产出变化的影响。主要目的是
了解时间动态并识别期间的潜在捷径
推理。我们的研究结果为不同世代提供了宝贵的见解
推理过程中的各个阶段，并揭示了 Unets 的使用模式
这些阶段。利用这些见解，我们确定了 GLIDE（一个
改进了 DDPM）并将推理时间缩短了约 27%，并且性能退化最小化
输出质量。我们的最终目标是指导更明智的优化
推理和影响新模型设计的策略。
]]></description>
      <guid>http://arxiv.org/abs/2312.14965</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>AS-XAI：CNN 的自监督自动语义解释。 （arXiv：2312.14935v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14935</link>
      <description><![CDATA[可解释的人工智能（XAI）旨在开发透明的人工智能
“黑盒”深度学习模型的解释方法。然而，它仍然
现有方法很难实现三个关键的权衡
可解释性标准，即可靠性、因果关系和可用性，
阻碍了它们的实际应用。在本文中，我们提出了一个
自监督 自动 语义 可解释 可解释 人工
智能（AS-XAI）框架，利用透明正交
嵌入语义提取空间和以行为中心的主成分
分析（PCA），用于模型决策的全局语义解释
没有人为干扰，无需额外的计算成本。在
另外，利用滤波器​​特征高阶分解的不变性
评估模型对不同语义概念的敏感性。广泛的
实验表明鲁棒且正交的语义空间可以
由AS-XAI自动提取，提供更有效的全局
卷积神经网络 (CNN) 的可解释性和生成
人类可以理解的解释。所提出的方法提供了广泛的
细粒度可扩展的实际应用，包括共享语义
分配外 (OOD) 类别下的解释，辅助
对难以区分的物种的解释，以及
从不同角度进行分类解释。
]]></description>
      <guid>http://arxiv.org/abs/2312.14935</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>SC-GS：用于可编辑动态场景的稀疏控制高斯泼溅。 （arXiv：2312.14937v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14937</link>
      <description><![CDATA[动态场景的新颖视图合成仍然是一个具有挑战性的问题
计算机视觉和图形学。最近，高斯泼溅作为一种
强大的技术来表示静态场景并实现高质量和
实时小说视图合成。基于这项技术，我们提出了一种新的
明确分解动态的运动和外观的表示
场景分别分为稀疏控制点和密集高斯。我们的钥匙
想法是使用稀疏控制点，其数量明显少于
高斯，学习紧凑的 6 DoF 变换基，可以是局部的
通过学习的插值权重进行插值以产生运动场
3D 高斯。我们采用变形 MLP 来预测时变 6 DoF
每个控制点的转换，这降低了学习复杂性，
增强学习能力，有利于获得时间和空间
连贯的运动模式。然后，我们共同学习 3D 高斯，
控制点的规范空间位置，以及变形 MLP
重建 3D 场景的外观、几何形状和动态。期间
通过学习，自适应调整控制点的位置和数量
适应不同区域不同的运动复杂性以及 ARAP 损失
遵循尽可能严格的原则来加强空间
学习动作的连续性和局部刚性。最后，感谢
显式稀疏运动表示及其从外观的分解，
我们的方法可以启用用户控制的运动编辑，同时保留
高保真外观。大量的实验表明我们的方法
在具有高渲染能力的新颖视图合成方面优于现有方法
速度并支持新颖的保留外观的运动编辑应用程序。
]]></description>
      <guid>http://arxiv.org/abs/2312.14937</guid>
      <pubDate>Wed, 27 Dec 2023 06:17:46 GMT</pubDate>
    </item>
    </channel>
</rss>