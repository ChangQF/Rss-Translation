<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 05 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>HandDiff：在图像点云上进行扩散的 3D 手部姿势估计</title>
      <link>https://arxiv.org/abs/2404.03159</link>
      <description><![CDATA[arXiv:2404.03159v1 公告类型：新
摘要：从输入的手部帧中提取关键点位置（称为 3D 手部姿势估计）是各种人机交互应用中的一项关键任务。本质上，3D 手部姿势估计可以被视为以输入帧为条件的 3D 点子集生成问题。由于最近基于扩散的生成模型取得了重大进展，手部姿势估计也可以受益于扩散模型来高质量地估计关键点位置。然而，直接部署现有的扩散模型来解决手部姿势估计并不简单，因为它们无法实现复杂的排列映射和精确定位。基于这个动机，本文提出了 HandDiff，一种基于扩散的手部姿势估计模型，该模型可以迭代地对手形图像点云条件下的准确手部姿势进行去噪。为了恢复关键点排列和准确定位，我们进一步引入联合条件和局部细节条件。实验结果表明，所提出的 HandDiff 在四个具有挑战性的手部姿势基准数据集上显着优于现有方法。代码和预训练模型可在 https://github.com/cwc1260/HandDiff 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.03159</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>用于零样本多标签分类的多样化和定制图像生成</title>
      <link>https://arxiv.org/abs/2404.03144</link>
      <description><![CDATA[arXiv:2404.03144v1 公告类型：新
摘要：最近，零样本多标签分类因其无需人工注释即可对看不见的标签进行预测的能力而引起了广泛关注。然而，流行的方法经常使用可见的类作为看不见的类的不完美代理，从而导致性能不佳。从文本到图像生成模型在生成逼真图像方面的成功中汲取灵感，我们提出了一种创新的解决方案：生成合成数据来构建专门针对看不见的标签进行无代理训练的训练集。我们的方法引入了一种新颖的图像生成框架，该框架可以生成未见过的类的多标签合成图像以进行分类器训练。为了增强生成图像的多样性，我们利用预先训练的大型语言模型来生成不同的提示。采用预训练的多模态 CLIP 模型作为鉴别器，我们评估生成的图像是否准确地表示目标类别。这可以自动过滤不准确生成的图像，从而保持分类器的准确性。为了细化文本提示以更精确、更有效地生成多标签对象，我们引入了基于 CLIP 分数的判别损失来微调扩散模型中的文本编码器。此外，为了增强目标任务的视觉特征，同时保持原始特征的泛化并减轻因微调整个视觉编码器而导致的灾难性遗忘，我们提出了一种受变压器注意机制启发的特征融合模块。该模块有助于更有效地捕获多个对象之间的全局依赖关系。大量的实验结果验证了我们的方法的有效性，证明了我们的方法比最先进的方法有显着的改进。]]></description>
      <guid>https://arxiv.org/abs/2404.03144</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>DreamWalk：使用扩散引导的风格空间探索</title>
      <link>https://arxiv.org/abs/2404.03145</link>
      <description><![CDATA[arXiv:2404.03145v1 公告类型：新
摘要：文本条件扩散模型可以生成令人印象深刻的图像，但在细粒度控制方面存在不足。与 Photoshop 等直接编辑工具不同，文本条件模型要求艺术家执行“即时工程”，构建特殊的文本句子来控制输出图像中存在的特定主题的风格或数量。我们的目标是对提示指定的样式和内容提供细粒度的控制，例如调整图像不同区域的样式强度（图 1）。我们的方法是将文本提示分解为概念元素，并在单个扩散过程中为每个元素应用单独的指导术语。我们引入引导尺度函数来控制扩散过程中的何时以及图像中的\emph{where}进行干预。由于该方法仅基于调整扩散引导，因此不需要微调或操作扩散模型神经网络的内部层，并且可以与 LoRA 或 DreamBooth 训练的模型结合使用（图 2）。项目页面：https://mshu1.github.io/dreamwalk.github.io/]]></description>
      <guid>https://arxiv.org/abs/2404.03145</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>LVLM-Intrepret：大型视觉语言模型的可解释性工具</title>
      <link>https://arxiv.org/abs/2404.03118</link>
      <description><![CDATA[arXiv:2404.03118v1 公告类型：新
摘要：在快速发展的人工智能领域，多模态大语言模型正在成为一个重要的关注领域。这些结合了各种形式的数据输入的模型正变得越来越流行。然而，了解其内部机制仍然是一项复杂的任务。可解释性工具和机制领域已经取得了许多进展，但仍有很多值得探索的地方。在这项工作中，我们提出了一种新颖的交互式应用程序，旨在理解大型视觉语言模型的内部机制。我们的界面旨在增强图像块的可解释性，这有助于生成答案，并评估语言模型将其输出基于图像的有效性。通过我们的应用程序，用户可以系统地研究模型并发现系统限制，为增强系统功能铺平道路。最后，我们提出了一个案例研究，说明我们的应用程序如何帮助理解流行的大型多模态模型中的故障机制：LLaVA。]]></description>
      <guid>https://arxiv.org/abs/2404.03118</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>利用计算机视觉连续监测实验小鼠的疫苗副作用</title>
      <link>https://arxiv.org/abs/2404.03121</link>
      <description><![CDATA[arXiv:2404.03121v1 公告类型：新
摘要：对提高疫苗安全性评估的效率和准确性的需求日益增加。在这里，我们探索应用计算机视觉技术来自动监测实验小鼠疫苗接种后潜在的副作用。传统的观测方法是劳动密集型的并且缺乏连续监测的能力。通过部署计算机视觉系统，我们的研究旨在提高疫苗安全评估的效率和准确性。该方法涉及根据疫苗接种前后小鼠行为的带注释视频数据训练机器学习模型。初步结果表明，计算机视觉可以有效识别细微的变化，并发出可能的副作用信号。因此，我们的方法有可能显着增强动物疫苗试验的监测过程，为人类观察的局限性提供实用的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2404.03121</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>与辅助边保持不连续性的法向积分</title>
      <link>https://arxiv.org/abs/2404.03138</link>
      <description><![CDATA[arXiv:2404.03138v1 公告类型：新
摘要：许多表面重建方法都结合了法线积分，这是从表面梯度获得深度图的过程。在此过程中，输入可能表示具有不连续性的表面，例如由于自遮挡。为了从输入法线贴图重建精确的深度图，必须处理因跳跃而产生的隐藏表面梯度。为了正确地模拟这些跳跃，我们为正态积分域设计了一种新颖的离散化方案。我们的关键思想是引入辅助边，它在域中的分段平滑补丁之间架起桥梁，以便可以明确表达隐藏跳跃的幅度。使用辅助边缘，我们设计了一种新颖的算法来优化输入法线图的不连续性和深度图。我们的方法通过使用迭代重新加权最小二乘法和辅助边缘上跳跃幅度的迭代过滤的组合来优化不连续性，以提供强稀疏性正则化。与之前仅隐式模拟跳跃幅度的保持不连续性的正态积分方法相比，我们的方法由于我们对跳跃的显式表示允许强稀疏性正则化，因此能够准确地重建细微的不连续性。]]></description>
      <guid>https://arxiv.org/abs/2404.03138</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>SalFoM：使用视频基础模型进行动态显着性预测</title>
      <link>https://arxiv.org/abs/2404.03097</link>
      <description><![CDATA[arXiv:2404.03097v1 公告类型：新
摘要：视频显着性预测（VSP）的最新进展与人类视觉系统相比显示出了有希望的性能，其模拟是 VSP 的主要目标。然而，当前最先进的模型采用基于有限数据量训练的时空变换器，阻碍了对下游任务的通用性适应。视觉基础模型的优势为改进 VSP 流程提供了潜在的解决方案。然而，将图像基础模型适应视频领域在场景动态建模和捕获时间信息方面提出了重大挑战。为了应对这些挑战，并且作为第一个基于视频基础模型设计 VSP 模型的举措，我们引入了 SalFoM，一种新颖的编码器-解码器视频转换器架构。我们的模型采用 UnMasked Teacher (UMT) 作为特征提取器，并提出一种异构解码器，该解码器具有局部感知时空变换器，并从不同角度集成局部和全局时空信息以生成最终的显着性图。我们对 DHF1K、Hollywood-2 和 UCF-Sports 等具有挑战性的 VSP 基准数据集进行的定性和定量实验证明了我们提出的模型与最先进的方法相比的优越性。]]></description>
      <guid>https://arxiv.org/abs/2404.03097</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>使用自回归扩散模型的多对多图像生成</title>
      <link>https://arxiv.org/abs/2404.03109</link>
      <description><![CDATA[arXiv:2404.03109v1 公告类型：新
摘要：图像生成领域的最新进展取得了重大进展，但现有模型在广泛的背景下感知和生成任意数量的相互关联图像方面存在局限性。随着多媒体平台的扩展，对多图像场景（例如多视图图像和视觉叙事）的需求不断增长，这种限制变得越来越严重。本文介绍了一种用于多对多图像生成的领域通用框架，能够从给定的图像集生成相互关联的图像系列，提供可扩展的解决方案，消除了跨不同多图像场景的特定任务解决方案的需要。为了实现这一目标，我们提出了 MIS，这是一种新颖的大规模多图像数据集，包含 12M 合成多图像样本，每个样本都有 25 个互连图像。利用稳定扩散和各种潜在噪声，我们的方法从单个标题生成一组互连图像。利用 MIS，我们学习 M2M，这是一种用于多对多生成的自回归模型，其中每个图像都在扩散框架内建模。在合成 MIS 的整个训练过程中，该模型擅长从先前图像（合成图像或真实图像）中捕获风格和内容，并根据捕获的模式生成新颖的图像。此外，通过特定于任务的微调，我们的模型展示了其对各种多图像生成任务的适应性，包括新颖视图合成和视觉过程生成。]]></description>
      <guid>https://arxiv.org/abs/2404.03109</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>用于稳健多目标跟踪的自我运动感知目标预测模块</title>
      <link>https://arxiv.org/abs/2404.03110</link>
      <description><![CDATA[arXiv:2404.03110v1 公告类型：新
摘要：多目标跟踪（MOT）是计算机视觉中自动驾驶应用中的一项突出任务，负责同时跟踪多个目标轨迹。基于检测的多对象跟踪 (DBT) 算法使用独立的对象检测器来检测对象并预测每个目标的即将位置。 DBT 中的传统预测方法利用卡尔曼滤波器（KF）通过假设恒速运动模型来推断即将到来的帧中的目标位置。由于相机运动剧烈或检测不可用，这些方法在自动驾驶应用中尤其受到阻碍。这种限制会导致跟踪失败，表现为大量身份切换和轨迹中断。在本文中，我们介绍了一种基于 KF 的新型预测模块，称为自我运动感知目标预测（EMAP）模块，重点关注相机运动和深度信息与物体运动模型的集成。我们提出的方法通过重新制定卡尔曼滤波器，将相机旋转和平移速度的影响与物体轨迹解耦。这种重新表述使我们能够拒绝相机运动引起的干扰，并最大限度地提高对象运动模型的可靠性。我们将我们的模块与四种最先进的基本 MOT 算法集成，即 OC-SORT、Deep OC-SORT、ByteTrack 和 BoT-SORT。特别是，我们对 KITTI MOT 数据集的评估表明，EMAP 将 OC-SORT 和 Deep OC-SORT 的身份转换 (IDSW) 数量分别显着降低了 73% 和 21%。同时，它将 HOTA 等其他性能指标提升了 5% 以上。我们的源代码可在 https://github.com/noyzzz/EMAP 获取。]]></description>
      <guid>https://arxiv.org/abs/2404.03110</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>粗线形状物体位置和宽度计算的线性锚定高斯混合模型</title>
      <link>https://arxiv.org/abs/2404.03043</link>
      <description><![CDATA[arXiv:2404.03043v1 公告类型：新
摘要：在道路交通中的 X 射线成像、遥感和车道标记检测等许多敏感的现实应用中，线性物体中心线的精确检测是一个具有挑战性的课题。使用霍夫和拉东变换的基于模型的方法经常被使用，但不建议用于粗线检测，而基于图像导数的方法需要进一步的逐步处理，使其效率取决于每个步骤的结果。在本文中，我们的目标是通过将图像灰度级的 3D 表示视为统计分布的有限混合模型来检测图像中发现的线性结构。后者，我们将其命名为线性锚定高斯分布，可以通过描述线性结构厚度的尺度值 {\sigma} 和线方程进行参数化，而线方程又通过半径 \r{ho} 和方向角 {\ theta}，描述线性结构中心线位置。期望最大化（EM）算法用于混合模型参数估计，提出了一种使用背景减法进行似然函数计算的新范式。对于 EM 算法，使用两种 {\theta} 参数初始化方案：第一个是基于 {\theta} 向量的第一个分量的随机选择，而第二个是基于图像 Hessian 并同时计算混合物模型成分编号。对真实世界图像和因模糊和加性噪声而损坏的合成图像进行的实验表明，所提出的方法具有良好的性能，其中使用背景减法和基于 Hessian 的 {\theta} 初始化的算法提供了出色的线性结构检测精度，尽管图像不规则背景以及模糊和噪音的存在。]]></description>
      <guid>https://arxiv.org/abs/2404.03043</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>面纱背后：通过遮挡表面完成增强室内 3D 场景重建</title>
      <link>https://arxiv.org/abs/2404.03070</link>
      <description><![CDATA[arXiv:2404.03070v1 公告类型：新
摘要：在本文中，我们提出了一种新颖的室内 3D 重建方法，在给定一系列深度读数的情况下，具有遮挡表面补全功能。现有的最先进（SOTA）方法仅关注场景中可见区域的重建，忽略了由于遮挡而导致的不可见区域，例如家具、遮挡的墙壁和地板之间的接触面。我们的方法解决了完成被遮挡的场景表面的任务，从而产生完整的 3D 场景网格。我们方法的核心思想是先从各种完整场景中学习 3D 几何形状，然后仅从深度测量中推断出未见场景的被遮挡几何形状。我们设计了一种粗细分层八叉树表示，与双解码器架构（即 Geo-decoder 和 3D Inpainter）相结合，共同重建完整的 3D 场景几何形状。具有精细级别详细表示的地理解码器针对每个场景进行在线优化，以重建可见表面。具有粗略抽象表示的 3D Inpainter 使用各种场景进行离线训练，以完成遮挡的表面。因此，虽然地理解码器专门用于单个场景，但 3D Inpainter 可以普遍应用于不同的场景。我们在 3D 完整房间场景 (3D-CRS) 和 iTHOR 数据集上评估了所提出的方法，在 3D 重建的完整性方面明显优于 SOTA 方法，分别提高了 16.8% 和 24.2%。项目网页提供了 3D-CRS 数据集，包括每个场景的完整 3D 网格。]]></description>
      <guid>https://arxiv.org/abs/2404.03070</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>ASAP：大规模人工智能生成图像模式的可解释分析和总结</title>
      <link>https://arxiv.org/abs/2404.02990</link>
      <description><![CDATA[arXiv:2404.02990v1 公告类型：新
摘要：生成图像模型已成为一种有前景的生成逼真图像的技术。尽管有潜在的好处，但人们对其滥用的担忧与日俱增，特别是在生成可能引发重大道德、法律和社会问题的欺骗性图像方面。因此，越来越需要让用户能够有效辨别和理解人工智能生成图像的模式。为此，我们开发了 ASAP，这是一种交互式可视化系统，可以自动提取人工智能生成图像的不同模式，并允许用户通过各种视图交互式地探索它们。为了发现虚假模式，ASAP 引入了一种改编自 CLIP 的新型图像编码器，它将图像转换为紧凑的“蒸馏”表示，并富含用于区分真假图像的信息。这些表示生成梯度，传播回 CLIP 变压器块的注意力图。这个过程量化了每个像素对图像真实性或虚假性的相对重要性，暴露了关键的欺骗模式。 ASAP 通过多个协调的可视化来实现对这些模式的大规模交互式分析。这包括具有创新单元字形的表示概述，以帮助探索和定性评估大量图像中的虚假图案，以及显示图像中指示真实性的图案并量化其影响的图案视图。 ASAP 支持使用最新架构分析尖端生成模型，包括基于 GAN 的模型（如 proGAN）和扩散模型（如潜在扩散模型）。我们通过使用多个虚假图像检测基准数据集的两个使用场景展示了 ASAP 的实用性，揭示了其识别和理解人工智能生成图像中隐藏模式的能力，特别是在检测基于扩散的技术产生的虚假人脸方面。]]></description>
      <guid>https://arxiv.org/abs/2404.02990</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>DPFT：用于基于摄像头雷达的物体检测的双视角融合变压器</title>
      <link>https://arxiv.org/abs/2404.03015</link>
      <description><![CDATA[arXiv:2404.03015v1 公告类型：新
摘要：自动驾驶汽车必须高效、稳健且具有成本效益。然而，摄像头在恶劣天气条件下的鲁棒性较差，激光雷达传感器价格昂贵，基于雷达的感知性能仍然不如其他传感器。相机-雷达融合方法已经被提出来解决这个问题，但这些方法受到雷达点云典型稀疏性的限制，并且通常是为没有高程信息的雷达而设计的。我们提出了一种新颖的相机雷达融合方法，称为双视角融合变压器（DPFT），旨在克服这些限制。我们的方法利用较低级别的雷达数据（雷达立方体）而不是处理后的点云来保留尽可能多的信息，并在摄像机和地平面中采用投影来有效地使用具有高程信息的雷达并简化与摄像机数据的融合。因此，DPFT 在 K-Radar 数据集上展示了最先进的性能，同时在恶劣天气条件下表现出卓越的稳健性并保持较低的推理时间。该代码作为开源软件在 https://github.com/TUMFTM/DPFT 下提供。]]></description>
      <guid>https://arxiv.org/abs/2404.03015</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>AWOL：使用语言进行分析而不进行综合</title>
      <link>https://arxiv.org/abs/2404.03042</link>
      <description><![CDATA[arXiv:2404.03042v1 公告类型：新
摘要：存在许多经典的参数化 3D 形状模型，但使用此类模型创建新颖的形状需要对其参数的专业知识。例如，想象一下使用程序图形创建特定类型的树或根据统计形状模型创建新型动物。我们的关键想法是利用语言来控制现有模型以产生新颖的形状。这涉及学习视觉语言模型的潜在空间和 3D 模型的参数空间之间的映射，我们使用一小组形状和文本对来完成此操作。我们的假设是，从语言到参数的映射允许我们为训练期间从未见过的对象生成参数。如果语言和参数之间的映射足够平滑，那么语言中的插值或泛化应该适当地转化为新颖的 3D 形状。我们使用两种截然不同类型的参数形状模型（四足动物和树木）来测试我们的方法。我们使用学习的四足动物统计形状模型，并表明我们可以使用文本来生成训练期间不存在的新动物。特别是，我们展示了最先进的 3D 狗形状估计。这项工作也构成了第一个用于生成 3D 树的语言驱动方法。最后，将图像嵌入 CLIP 潜在空间使我们能够直接从图像生成动物和树木。]]></description>
      <guid>https://arxiv.org/abs/2404.03042</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>星系图像的缩放定律</title>
      <link>https://arxiv.org/abs/2404.02973</link>
      <description><![CDATA[arXiv:2404.02973v1 公告类型：新
摘要：我们首次在类似 ImageNet 的环境之外对星系图像的监督缩放定律进行了系统研究。我们使用 Galaxy Zoo 志愿者提供的 84 万张星系图像和超过 1 亿条注释，其规模与 Imagenet-1K 相当。我们发现，添加带注释的星系图像可以在所有架构和所有任务中提供幂律性能改进，而添加可训练参数仅对某些（通常更具主观挑战性）任务有效。然后，我们比较单独在 ImageNet-12k 上预训练的微调模型与在我们的星系图像上额外预训练的微调模型的下游性能。我们在 5 项具有科学意义的下游任务中实现了平均相对错误率降低了 31%。我们的微调模型具有更高的标签效率，并且与 ImageNet-12k 预训练模型不同，通常可以实现与端到端微调相同的线性传输性能。我们发现缩放模型大小带来的额外下游好处相对较小，这意味着仅缩放不足以解决我们的领域差距，并建议具有不同质量图像的从业者可能会从域内适应和有针对性的下游标记中受益更多。]]></description>
      <guid>https://arxiv.org/abs/2404.02973</guid>
      <pubDate>Fri, 05 Apr 2024 06:17:08 GMT</pubDate>
    </item>
    </channel>
</rss>