<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://arxiv.org/rss/</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 02 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>咖啡馆场景的多模态实体交互代理</title>
      <link>https://arxiv.org/abs/2402.00290</link>
      <description><![CDATA[随着大型语言模型发展的激增，具身智能越来越受到人们的关注。然而，先前关于具身智能的研究通常以视觉或语言的单模态方式对场景或历史记忆进行编码，这使得模型的行动规划与具身控制的协调变得复杂。为了克服这一限制，我们引入了多模式嵌入式交互代理（MEIA），它能够将以自然语言表达的高级任务转换为一系列可执行动作。具体来说，我们提出了一种新颖的多模态环境记忆（MEM）模块，通过场景的视觉语言记忆促进体现控制与大型模型的集成。此功能使 MEIA 能够根据不同的要求和机器人的功能生成可执行的行动计划。我们在动态的虚拟咖啡馆环境中进行实验，通过零样本学习利用多个大型模型，并针对各种情况精心设计场景。实验结果展示了我们的 MEIA 在各种具体交互任务中的良好表现。]]></description>
      <guid>https://arxiv.org/abs/2402.00290</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:03 GMT</pubDate>
    </item>
    <item>
      <title>大视觉语言模型中幻觉的调查</title>
      <link>https://arxiv.org/abs/2402.00253</link>
      <description><![CDATA[最近，大型视觉语言模型（LVLM）的发展因其实际实施潜力而在人工智能领域引起了越来越多的关注。然而，“幻觉”，或者更具体地说，实际视觉内容与相应文本生成之间的不一致，对使用 LVLM 提出了重大挑战。在这项综合调查中，我们剖析了 LVLM 相关的幻觉，试图建立一个概述并促进未来的缓解。我们的审查从澄清 LVLM 幻觉的概念开始，呈现各种幻觉症状并强调 LVLM 幻觉固有的独特挑战。随后，我们概述了专门为评估 LVLM 特有的幻觉而定制的基准和方法。此外，我们还深入研究了这些幻觉的根本原因，包括来自训练数据和模型组件的见解。我们还严格审查现有的减轻幻觉的方法。讨论了与 LVLM 内幻觉相关的悬而未决的问题和未来方向，以结束本次调查。]]></description>
      <guid>https://arxiv.org/abs/2402.00253</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:02 GMT</pubDate>
    </item>
    <item>
      <title>了解使用向量空间和逆映射进行图像分析的神经网络系统</title>
      <link>https://arxiv.org/abs/2402.00261</link>
      <description><![CDATA[人们对开发可用于理解图像分析中使用的复杂神经网络的数学方法有着浓厚的兴趣。在本文中，我们介绍了线性代数技术，将神经网络层建模为信号空间之间的映射。首先，我们演示如何使用信号空间来可视化权重空间和卷积层内核。我们还演示了如何使用残差向量空间来进一步可视化每层丢失的信息。其次，我们介绍可逆网络的概念以及用于计算产生特定输出的输入图像的算法。我们在两个可逆网络和 ResNet18 上展示了我们的方法。]]></description>
      <guid>https://arxiv.org/abs/2402.00261</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:02 GMT</pubDate>
    </item>
    <item>
      <title>通过空间动作单元线索引导可解释的面部表情识别</title>
      <link>https://arxiv.org/abs/2402.00281</link>
      <description><![CDATA[虽然最先进的面部表情识别 (FER) 分类器具有很高的准确性，但它们缺乏可解释性，而这对最终用户来说是一个重要方面。为了识别基本的面部表情，专家们求助于将一组空间动作单元与面部表情相关联的密码本。在本文中，我们遵循相同的专家脚步，并提出了一种学习策略，使我们能够明确地将空间动作单元（aus）线索纳入分类器的训练中，以构建深度可解释模型。特别是，使用此 aus 代码本、输入图像表情标签和面部标志，构建单个动作单元热图来指示图像中与面部表情相关的最具辨别力的感兴趣区域。我们利用这一宝贵的空间线索来训练 FER 的深度可解释分类器。这是通过将分类器的空间层特征限制为与\aus图相关来实现的。使用复合损失，训练分类器正确分类图像，同时产生与 aus 地图相关的可解释的视觉分层注意力，模拟专家的决策过程。这是仅使用图像类表达式作为监督来实现的，无需任何额外的手动注释。此外，我们的方法是通用的。它可以应用于任何基于 CNN 或 Transformer 的深度分类器，无需更改架构或增加大量训练时间。我们对两个公共基准 RAFDB 和 AFFECTNET 数据集的广泛评估表明，我们提出的策略可以在不降低分类性能的情况下提高分层可解释性。此外，我们探索了一种依赖于类激活映射方法（CAM）的常见类型的可解释分类器，并且我们表明我们的训练技术提高了 CAM 可解释性。]]></description>
      <guid>https://arxiv.org/abs/2402.00281</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:02 GMT</pubDate>
    </item>
    <item>
      <title>使用智能制造对象检测进行产能约束分析</title>
      <link>https://arxiv.org/abs/2402.00243</link>
      <description><![CDATA[基于深度学习 (DL) 的目标检测 (OD) 方法及其实际应用的日益普及，为智能制造开辟了新的领域。冠状病毒病 (COVID-19) 后，传统行业受到产能限制，需要采用非侵入性方法进行深入运营分析，以优化和增加收入。在本研究中，我们最初开发了基于卷积神经网络（CNN）的 OD 模型来解决这个问题。该模型经过训练，可以准确识别生产车间中椅子和个人的存在。然后，识别出的对象被传递到基于 CNN 的跟踪器，该跟踪器在工作站中跟踪它们的整个生命周期。提取的元数据通过容量约束分析的新颖框架进一步处理。我们发现 C 站在 6 个月内的生产率仅为 70.6%。此外，还会记录并汇总每个对象在每个站点花费的时间。事实证明，这些数据有助于进行年度审计以及随着时间的推移有效管理劳动力和材料。]]></description>
      <guid>https://arxiv.org/abs/2402.00243</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:01 GMT</pubDate>
    </item>
    <item>
      <title>LRDif：用于屏下摄像头情绪识别的扩散模型</title>
      <link>https://arxiv.org/abs/2402.00250</link>
      <description><![CDATA[本研究引入了 LRDif，这是一种新型的基于扩散的框架，专为屏下摄像头 (UDC) 背景下的面部表情识别 (FER) 而设计。为了解决 UDC 图像退化带来的固有挑战，例如清晰度降低和噪声增加，LRDif 采用两阶段训练策略，集成压缩初步提取网络（FPEN）和敏捷变压器网络（UDCformer）来有效识别情感标签来自 UDC 图像。通过利用扩散模型 (DM) 强大的分布映射功能和变压器的空间依赖性建模能力，LRDif 有效克服了 UDC 环境中固有的噪声和失真障碍。对标准 FER 数据集（包括 RAF-DB、KDEF 和 FERPlus、LRDif）的综合实验展示了最先进的性能，强调了其在推进 FER 应用方面的潜力。这项工作不仅通过解决 FER 中的 UDC 挑战来弥补文献中的重大空白，而且还为该领域的未来研究树立了新的基准。]]></description>
      <guid>https://arxiv.org/abs/2402.00250</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:01 GMT</pubDate>
    </item>
    <item>
      <title>CMRNext：相机与 LiDAR 的野外匹配，用于定位和外部校准</title>
      <link>https://arxiv.org/abs/2402.00129</link>
      <description><![CDATA[LiDAR 广泛用于动态环境中的测绘和定位。然而，它们的高成本限制了它们的广泛采用。另一方面，使用廉价相机在激光雷达地图中进行单目定位是大规模部署的一种经济高效的替代方案。然而，大多数现有方法很难推广到新的传感器设置和环境，需要重新训练或微调。在本文中，我们提出了 CMRNext，这是一种相机-激光雷达匹配的新颖方法，该方法独立于传感器特定参数、可推广，并且可在野外用于激光雷达地图中的单目定位和相机-激光雷达外在校准。 CMRNext 利用深度神经网络的最新进展来匹配跨模态数据和标准几何技术来实现稳健的姿态估计。我们将点像素匹配问题重新表述为光流估计问题，并根据所得对应关系解决透视 n 点问题，以找到相机和 LiDAR 点云之间的相对位姿。我们在六个不同的机器人平台上广泛评估 CMRNext，包括三个公开可用的数据集和三个内部机器人。我们的实验评估表明，CMRNext 在这两项任务上都优于现有方法，并以零样本方式有效地推广到以前未见过的环境和传感器设置。我们在 http://cmrnext.cs.uni-freiburg.de 公开提供代码和预训练模型。]]></description>
      <guid>https://arxiv.org/abs/2402.00129</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:00 GMT</pubDate>
    </item>
    <item>
      <title>通过超分辨率技术提高足球中的物体检测质量</title>
      <link>https://arxiv.org/abs/2402.00163</link>
      <description><![CDATA[这项研究探讨了超分辨率技术在提高足球运动中物体检测准确性方面的潜力。鉴于这项运动的快节奏性质以及精确对象（例如球、运动员）跟踪对于分析和广播的至关重要性，超分辨率可以提供显着的改进。我们研究了通过超分辨率进行的先进图像处理如何影响处理足球比赛镜头的对象检测算法的准确性和可靠性。
  我们的方法包括将最先进的超分辨率技术应用于 SoccerNet 的各种足球比赛视频，然后使用 Faster R-CNN 进行对象检测。这些算法的性能，无论是否具有超分辨率增强，都在检测精度方面进行了严格评估。
  结果表明，应用超分辨率预处理时，目标检测精度显着提高。通过集成超分辨率技术来改进目标检测会带来显着的好处，特别是对于低分辨率场景，在 IoU（交并集）范围为 0.50 时，平均精度 (mAP) 显着提高 12%：使用 RLFN 将分辨率提高四倍时，320x240 尺寸图像为 0.95。随着尺寸的增加，改进的幅度变得更加减弱；然而，检测质量的明显改善始终是显而易见的。此外，我们还讨论了这些发现对实时体育分析、球员跟踪和整体观看体验的影响。该研究通过展示将超分辨率技术集成到足球分析和广播中的实际好处和局限性，为不断发展的体育技术领域做出了贡献。]]></description>
      <guid>https://arxiv.org/abs/2402.00163</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:00 GMT</pubDate>
    </item>
    <item>
      <title>从 ImageNet 中的野外图像生成几何感知 3D</title>
      <link>https://arxiv.org/abs/2402.00225</link>
      <description><![CDATA[生成准确的 3D 模型是一个具有挑战性的问题，传统上需要使用监督学习从 3D 数据集进行显式学习。尽管最近的进展显示出从 2D 图像学习 3D 模型的前景，但这些方法通常依赖于结构良好的数据集，其中包含每个实例的多视图图像或相机姿态信息。此外，这些数据集通常包含形状简单的干净背景，使得它们获取成本昂贵并且难以泛化，这限制了这些方法的适用性。为了克服这些限制，我们提出了一种在没有相机姿态信息的情况下从多样化且非结构化的 Imagenet 数据集重建 3D 几何的方法。我们使用高效的三平面表示从 2D 图像中学习 3D 模型，并修改基于 StyleGAN2 的生成器主干的架构以适应高度多样化的数据集。为了防止模式崩溃并提高不同数据上的训练稳定性，我们建议使用多视图辨别。经过训练的生成器可以生成类条件 3D 模型以及任意视点的渲染。类条件生成结果表明比当前最先进的方法有显着改进。此外，使用 PTI，我们可以从单视图图像有效地重建整个 3D 几何结构。]]></description>
      <guid>https://arxiv.org/abs/2402.00225</guid>
      <pubDate>Fri, 02 Feb 2024 18:17:00 GMT</pubDate>
    </item>
    <item>
      <title>深度伪造检测的常识推理</title>
      <link>https://arxiv.org/abs/2402.00126</link>
      <description><![CDATA[最先进的方法依赖于通过神经网络提取的基于图像的特征来进行深度伪造检测二元分类。虽然这些在监督意义上训练的方法提取了可能的虚假特征，但它们可能无法代表不自然的“非物理”语义面部属性——模糊的发际线、双眉毛、僵硬的瞳孔或不自然的皮肤阴影。然而，此类面部属性通常很容易被人类通过常识推理感知。此外，通过显着图提供视觉解释的基于图像的特征提取方法可能很难被人类解释。为了应对这些挑战，我们建议使用常识推理来建模 Deepfake 检测，并将其扩展到 Deepfake 检测 VQA (DD-VQA) 任务，旨在模拟人类直觉，解释将图像标记为真实图像背后的原因或假的。为此，我们引入了一个新的数据集，它提供了与图像真实性相关的问题的答案及其相应的解释。我们还为 DD-VQA 任务提出了一个基于视觉和语言转换器的框架，结合了文本和图像感知特征对齐公式。最后，我们根据深度伪造检测的性能和生成的解释的质量来评估我们的方法。我们希望这项任务能够激励研究人员探索新的途径，以增强深度伪造检测领域基于语言的可解释性和跨模态应用。]]></description>
      <guid>https://arxiv.org/abs/2402.00126</guid>
      <pubDate>Fri, 02 Feb 2024 18:16:59 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶的实时交通物体检测</title>
      <link>https://arxiv.org/abs/2402.00128</link>
      <description><![CDATA[随着计算机视觉的最新进展，自动驾驶似乎迟早会成为现代社会的一部分。然而，仍有大量问题需要解决。尽管现代计算机视觉技术表现出卓越的性能，但它们倾向于优先考虑准确性而不是效率，而效率是实时应用程序的一个重要方面。大型物体检测模型通常需要更高的计算能力，这是通过使用更复杂的机载硬件来实现的。对于自动驾驶来说，这些要求会导致燃料成本增加，并最终导致行驶里程减少。此外，尽管存在计算需求，但现有的物体检测器远非实时。在这项研究中，我们评估了之前提出的高效行人检测器 LSFM 在完善的自动驾驶基准上的鲁棒性，包括不同的天气条件和夜间场景。此外，我们将 LSFM 模型扩展用于一般目标检测，以实现交通场景中的实时目标检测。我们评估其在交通对象检测数据集上的性能、低延迟和通用性。此外，我们讨论了自动驾驶背景下物体检测系统当前使用的关键性能指标的不足，并提出了一种更合适的替代方案，其中包含了实时要求。]]></description>
      <guid>https://arxiv.org/abs/2402.00128</guid>
      <pubDate>Fri, 02 Feb 2024 18:16:59 GMT</pubDate>
    </item>
    <item>
      <title>LF-ViT：减少 Vision Transformer 中的空间冗余以实现高效图像识别</title>
      <link>https://arxiv.org/abs/2402.00033</link>
      <description><![CDATA[视觉变换器 (ViT) 在处理高分辨率图像时具有出色的准确性，但它面临着大量空间冗余的挑战，导致计算和内存需求增加。为了解决这个问题，我们提出了定位和焦点视觉转换器（LF-ViT）。该模型的运作方式是在不影响性能的情况下战略性地减少计算需求。在定位阶段，处理分辨率降低的图像；如果最终的预测仍然难以捉摸，我们开创性的邻里全局阶级关注 (NGCA) 机制就会被触发，根据初步发现有效识别和突出阶级歧视区域。随后，在聚焦阶段，使用原始图像中的这个指定区域来增强识别。独特的是，LF-ViT 在两个阶段都采用一致的参数，确保无缝的端到端优化。我们的实证测试证实了 LF-ViT 的实力：它显着地将 Deit-S 的 FLOP 降低了 63%，同时将吞吐量提高了一倍。该项目的代码位于https://github.com/edgeai1/LF-ViT.git。]]></description>
      <guid>https://arxiv.org/abs/2402.00033</guid>
      <pubDate>Fri, 02 Feb 2024 18:16:58 GMT</pubDate>
    </item>
    <item>
      <title>用于飞机安全滑行的跑道目标分类器的鲁棒性评估</title>
      <link>https://arxiv.org/abs/2402.00035</link>
      <description><![CDATA[随着深度神经网络 (DNN) 正在成为许多计算问题的重要解决方案，航空业寻求探索其在减轻飞行员工作量和提高运营安全方面的潜力。然而，在此类安全关键型应用中使用 DNN 需要彻底的认证过程。这种需求可以通过形式验证来解决，形式验证提供了严格的保证——例如，通过证明不存在某些错误预测。在本案例研究论文中，我们使用空中客车公司目前正在开发的图像分类器 DNN 来演示此过程，该图像分类器旨在用于飞机滑行阶段。我们使用形式化方法来评估该 DNN 对三种常见图像扰动类型的鲁棒性：噪声、亮度和对比度以及它们的一些组合。这个过程需要多次调用底层验证器，这可能会导致计算成本高昂；因此，我们提出了一种利用这些鲁棒性属性的单调性以及过去验证查询的结果的方法，以便将所需的验证查询总数减少近 60%。我们的结果表明了所研究的 DNN 分类器所实现的鲁棒性水平，并表明它更容易受到噪声的影响，而不是亮度或对比度扰动。]]></description>
      <guid>https://arxiv.org/abs/2402.00035</guid>
      <pubDate>Fri, 02 Feb 2024 18:16:58 GMT</pubDate>
    </item>
    <item>
      <title>遥感场景分类中卷积神经网络的克罗内克产品特征融合</title>
      <link>https://arxiv.org/abs/2402.00036</link>
      <description><![CDATA[遥感场景分类是一个具有挑战性和有价值的研究课题，其中卷积神经网络（CNN）发挥了至关重要的作用。 CNN可以从遥感图像中提取分层卷积特征，不同层的特征融合可以增强CNN的性能。某些最先进的 CNN 算法中采用了两种成功的特征融合方法：Add 和 Concat。在本文中，我们提出了一种新颖的特征融合算法，该算法使用克罗内克积（KPFF）统一了上述方法，并讨论了与该算法相关的反向传播过程。为了验证所提出方法的有效性，设计并进行了一系列实验。结果证明了其在提高 CNN 在遥感场景分类中的准确性方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.00036</guid>
      <pubDate>Fri, 02 Feb 2024 18:16:58 GMT</pubDate>
    </item>
    <item>
      <title>使用多时态 Sentinel-1 和 Sentinel-2 数据进行水体测绘</title>
      <link>https://arxiv.org/abs/2402.00023</link>
      <description><![CDATA[气候变化正在加剧极端天气事件，造成水资源短缺和严重的降雨不可预测性，并对可持续发展、生物多样性以及水和卫生设施的获取构成威胁。本文旨在为不同气象条件下的综合水资源监测提供有价值的见解。提出了 SEN2DWATER 数据集的扩展，以增强其流域分割的能力。通过将 Sentinel-1 数据中的时间和空间对齐的雷达信息与现有的多光谱 Sentinel-2 数据相集成，生成了一个新颖的多源和多时态数据集。对增强数据集进行基准测试涉及土壤水分指数 (SWI) 和归一化水分指数 (NDWI) 等指数的应用，以及无监督机器学习 (ML) 分类器（k 均值聚类）。这项研究取得了有希望的结果，并探讨了未来潜在的发展和应用。]]></description>
      <guid>https://arxiv.org/abs/2402.00023</guid>
      <pubDate>Fri, 02 Feb 2024 18:16:57 GMT</pubDate>
    </item>
    </channel>
</rss>