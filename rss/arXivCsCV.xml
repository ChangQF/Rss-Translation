<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 14 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>ADLDA：一种减少数据增强中数据分布偏移危害的方法</title>
      <link>https://arxiv.org/abs/2405.06893</link>
      <description><![CDATA[arXiv:2405.06893v1 公告类型：新
摘要：本研究介绍了一种新型数据增强技术 ADLDA，旨在减轻计算机视觉任务中数据增强过程引起的数据分布变化的负面影响。 ADLDA 将增强数据划分为不同的子域，并结合域标签，结合域适应技术，以优化模型特征空间中的数据表示。实验结果表明，ADLDA 显着增强了跨多个数据集的模型性能，特别是在具有复杂特征提取层的神经网络架构中。此外，ADLDA 提高了模型定位和识别关键特征的能力，展示了在对象识别和图像分割任务中的潜力。本文的贡献为计算机视觉领域提供了一种有效的数据增强正则化方法，有助于增强深度学习模型的鲁棒性和准确性。]]></description>
      <guid>https://arxiv.org/abs/2405.06893</guid>
      <pubDate>Tue, 14 May 2024 06:20:41 GMT</pubDate>
    </item>
    <item>
      <title>Logical：面向无监督异常定位的逻辑异常综合</title>
      <link>https://arxiv.org/abs/2405.06875</link>
      <description><![CDATA[arXiv:2405.06875v1 公告类型：新
摘要：异常定位是提高工业生产线效率的实用技术。由于异常现象多种多样且难以收集，现有的无监督研究通常配备异常合成方法。然而，大多数都偏向于结构缺陷综合，而忽略了底层的逻辑约束。为了填补这一空白并提高异常定位性能，我们提出了一种基于边缘操作的异常合成框架，名为 Logical，它可以产生逼真的逻辑和结构异常。我们引入了一种善于打破逻辑约束的逻辑异常生成策略和一种补充结构缺陷合成的结构异常生成策略。我们通过在网络结构中引入边缘重建来进一步提高异常定位性能。在 MVTecLOCO、MVTecAD、VisA 和 MADsim 数据集上进行的大量实验验证了所提出的 LogAL 在逻辑和结构异常定位方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2405.06875</guid>
      <pubDate>Tue, 14 May 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>FineParser：用于以人为中心的动作质量评估的细粒度时空动作解析器</title>
      <link>https://arxiv.org/abs/2405.06887</link>
      <description><![CDATA[arXiv:2405.06887v1 公告类型：新
摘要：现有的动作质量评估（AQA）方法主要学习视频级别的深度表示，以对不同的动作进行评分。由于缺乏对视频中动作的细粒度理解，它们严重缺乏可信度和可解释性，因此不足以满足严格的应用，例如奥运会跳水比赛。我们认为，对动作的细粒度理解需要模型在时间和空间上感知和解析动作，这也是 AQA 技术可信性和可解释性的关键。基于这一见解，我们提出了一种新的细粒度时空动作解析器，名为 \textbf{FineParser}。它通过关注每个帧内的目标动作区域并利用它们在时间和空间上的细粒度对齐来学习以人为中心的前景动作表示，以最大程度地减少评估过程中无效背景的影响。此外，我们为 FineDiving 数据集构建了以人类为中心的前景动作蒙版的细粒度注释，称为 \textbf{FineDiving-HM}。通过对不同目标动作程序的精细注释，FineDiving-HM 可以促进现实世界 AQA 系统的开发。通过大量的实验，我们证明了 FineParser 的有效性，它超越了最先进的方法，同时支持更多细粒度动作理解的任务。数据和代码可在 \url{https://github.com/PKU-ICST-MIPL/FineParser_CVPR2024} 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.06887</guid>
      <pubDate>Tue, 14 May 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>破坏视频图像的风格模仿攻击</title>
      <link>https://arxiv.org/abs/2405.06865</link>
      <description><![CDATA[arXiv:2405.06865v1 公告类型：新
摘要：生成式人工智能模型通常用于执行模仿攻击，其中预训练的模型在少量图像样本上进行微调，以学习模仿感兴趣的特定艺术家。虽然研究人员引入了多种反模仿保护工具（Mist、Glaze、Anti-Dreambooth），但最近的证据表明，使用视频作为训练数据源的模仿模型的趋势正在不断增长。本文介绍了我们探索破坏视频图像风格模仿的技术的经验。我们首先通过对从视频中提取的各个帧进行训练来验证模仿攻击是否可以成功。我们表明，虽然反模仿工具在应用于单个帧时可以提供保护，但这种方法很容易受到自适应对策的影响，该对策通过利用连续（几乎相同）帧的优化结果中的随机性来消除保护。我们开发了一个新的与工具无关的框架，该框架根据帧级相似性将视频分割成短场景，并使用每个场景的优化基线来消除帧间随机化，同时降低计算成本。我们通过图像级别指标和端到端用户研究表明，由此产生的保护可以恢复针对模仿的保护（包括对策）。最后，我们开发了另一种自适应对策，并发现它不符合我们的框架。]]></description>
      <guid>https://arxiv.org/abs/2405.06865</guid>
      <pubDate>Tue, 14 May 2024 06:20:39 GMT</pubDate>
    </item>
    <item>
      <title>eCAR：边缘辅助协作增强现实框架</title>
      <link>https://arxiv.org/abs/2405.06872</link>
      <description><![CDATA[arXiv:2405.06872v1 公告类型：新
摘要：我们提出了一种新型的大型室内环境中的边缘辅助多用户协作增强现实框架。在协作增强现实中，同步虚拟对象的数据通信具有较大的网络流量和较高的网络延迟。由于漂移，没有连续数据通信进行坐标系对齐的 CAR 应用程序会出现虚拟对象不一致的情况。此外，随着协作设备数量的增加，在线虚拟对象更新的同步消息具有较高的延迟。为了解决这个问题，我们实现了名为 eCAR 的 CAR 框架，它利用边缘计算以更少的网络流量持续匹配设备的坐标系。此外，我们扩展了边缘服务器的共同可见性图，通过同步本地图来维持相邻设备中的虚拟对象时空一致性。我们在公共数据集和物理室内环境中对系统进行定量和定性评估。 eCAR 以更少的网络流量和延迟来传输数据，以实现边缘服务器和设备之间的坐标系对齐。此外，协作增强现实同步算法可以快速准确地托管和解析虚拟对象。所提出的系统不断地将坐标系与大型室内环境中的多个设备对齐并共享增强现实内容。通过我们的系统，用户可以与虚拟对象进行交互，并与邻近的用户分享增强现实体验。]]></description>
      <guid>https://arxiv.org/abs/2405.06872</guid>
      <pubDate>Tue, 14 May 2024 06:20:39 GMT</pubDate>
    </item>
    <item>
      <title>CasCalib：稀疏不同步相机运动捕捉的级联校准</title>
      <link>https://arxiv.org/abs/2405.06845</link>
      <description><![CDATA[arXiv:2405.06845v1 公告类型：新
摘要：现在可以使用现成的 3D 姿势估计器从单目图像估计 3D 人体姿势。然而，许多实际应用需要细粒度的绝对姿态信息，为此需要多视图线索和相机校准。这种多视图记录非常费力，因为它们需要手动校准，并且在使用专用硬件时价格昂贵。我们的目标是完全自动化，包括时间同步以及内部和外部相机校准。这是通过使用场景中的人作为校准对象来完成的。现有方法要么仅解决同步或校准问题，假设前者之一作为输入，要么具有很大的局限性。一个常见的限制是他们只考虑单身人士，这简化了信件查找。我们通过将高维时间和校准空间划分为级联子空间并引入定制算法来高效、鲁棒地优化每个子空间来实现这种通用性。结果是我们发布了一个易于使用、灵活且强大的运动捕捉工具箱，以实现科学应用，并在不同的多视图基准测试中进行了演示。项目网站：https://github.com/jamestang1998/CasCalib。]]></description>
      <guid>https://arxiv.org/abs/2405.06845</guid>
      <pubDate>Tue, 14 May 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>GreedyViG：高效视觉 GNN 的动态轴图构建</title>
      <link>https://arxiv.org/abs/2405.06849</link>
      <description><![CDATA[arXiv:2405.06849v1 公告类型：新
摘要：视觉图神经网络（ViG）为计算机视觉的探索提供了新的途径。 ViG 的一个主要瓶颈是用于图构建的低效 k-近邻 (KNN) 操作。为了解决这个问题，我们提出了一种设计 ViG 的新方法，即动态轴图构建（DAGC），它比 KNN 更有效，因为它限制了图像内所考虑的图连接的数量。此外，我们提出了一种新颖的 CNN-GNN 架构 GreedyViG，它使用 DAGC。大量实验表明，GreedyViG 在图像分类、对象检测、实例分割和语义分割任务的准确性、GMAC 和参数方面击败了现有的 ViG、CNN 和 ViT 架构。我们最小的模型 GreedyViG-S 在 ImageNet-1K 上实现了 81.1% 的 top-1 准确率，比 Vision GNN 高 2.9%，比 Vision HyperGraph Neural Network (ViHGNN) 高 2.2%，并且 GMAC 更少，参数数量相似。我们最大的模型 GreedyViG-B 获得了 83.9% 的 top-1 准确率，比 Vision GNN 高 0.2%，参数减少了 66.6%，GMAC 减少了 69%。 GreedyViG-B 也获得了与 ViHGNN 相同的精度，参数减少了 67.3%，GMAC 减少了 71.3%。我们的工作表明，混合 CNN-GNN 架构不仅为设计高效模型提供了新途径，而且还可以超越当前最先进模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.06849</guid>
      <pubDate>Tue, 14 May 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>弥合差距：公平一致的影响分析协议</title>
      <link>https://arxiv.org/abs/2405.06841</link>
      <description><![CDATA[arXiv:2405.06841v1 公告类型：新
摘要：机器学习算法在日常生活中的日益集成凸显了其部署中对公平和公正的迫切需求。由于这些技术在决策中发挥着关键作用，因此解决不同亚群体（包括年龄、性别和种族）之间的偏见变得至关重要。自动情感分析在生理学、心理学和机器学习的交叉领域取得了显着的发展。然而，现有的数据库和方法缺乏统一性，导致评估存在偏差。这项工作通过分析六个情感数据库、注释人口统计属性并提出数据库分区的通用协议来解决这些问题。注重评价的公平性。使用基线和最先进方法进行的广泛实验证明了这些变化的影响，揭示了先前评估的不足。研究结果强调了在影响分析研究中考虑人口统计属性的重要性，并为更公平的方法论提供了基础。我们的注释、代码和预训练模型可在以下位置获取：https://github.com/dkollias/Fair-Concient-Affect-Analysis]]></description>
      <guid>https://arxiv.org/abs/2405.06841</guid>
      <pubDate>Tue, 14 May 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>用于医疗材料自主分类、绘图和量化的同步对象检测</title>
      <link>https://arxiv.org/abs/2405.06821</link>
      <description><![CDATA[arXiv:2405.06821v1 公告类型：新
摘要：循环经济范式作为减少材料供应不确定性和废物产生的解决方案越来越受到人们的关注。主要挑战之一是监控材料，因为一般来说，不进行测量的东西就无法有效管理。在本文中，我们提出了实时同步对象检测，以同时实现报废医疗材料的自主分类、绘图和量化。数据集、代码和演示视频是公开的。]]></description>
      <guid>https://arxiv.org/abs/2405.06821</guid>
      <pubDate>Tue, 14 May 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>G-FARS：用于 3D 零件分组的基于梯度场的自回归采样</title>
      <link>https://arxiv.org/abs/2405.06828</link>
      <description><![CDATA[arXiv:2405.06828v1 公告类型：新
摘要：本文提出了一个名为“3D 零件分组”的新任务。假设有一个混合集，其中包含各种形状的分散部分。这项任务需要算法找出所有部分之间的每种可能的组合。为了应对这一挑战，我们提出了专为 3D 零件分组任务量身定制的所谓基于梯度场的自回归采样框架 (G-FARS)。在我们的框架中，我们设计了一个基于梯度场的选择图神经网络（GNN）来学习零件选择方面的对数条件概率密度的梯度，其中条件是给定的混合零件集。这种创新方法通过基于梯度场的选择 GNN 实现，有效捕获输入中所有部分之间的复杂关系。训练过程完成后，我们的框架能够利用经过训练的基于梯度场的选择 GNN 获得的知识，通过从混合零件集中迭代选择 3D 零件来自主对 3D 零件进行分组。我们的代码位于：https://github.com/J-F-Cheng/G-FARS-3DPartGrouping。]]></description>
      <guid>https://arxiv.org/abs/2405.06828</guid>
      <pubDate>Tue, 14 May 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>GraphRelate3D：具有对象间关系图的上下文相关 3D 对象检测</title>
      <link>https://arxiv.org/abs/2405.06782</link>
      <description><![CDATA[arXiv:2405.06782v1 公告类型：新
摘要：准确有效的3D物体检测对于确保自动驾驶汽车的行驶安全至关重要。最近，最先进的两级 3D 物体探测器表现出了令人鼓舞的性能。然而，这些方法单独细化提案，忽略了邻居提案之间的对象关系中丰富的上下文信息。在本研究中，我们引入了一个由图生成器和图神经网络 (GNN) 组成的对象关系模块，用于从某些模式中学习空间信息以改进 3D 对象检测。具体来说，我们通过图生成器基于框架中的提案创建对象间关系图，以将每个提案与其邻居提案连接起来。随后，GNN 模块从生成的图中提取边缘特征，并使用捕获的边缘特征迭代地细化提议特征。最终，我们利用细化的特征作为检测头的输入来获得检测结果。我们的方法在 KITTI 验证集上针对简单、中等和困难难度级别的汽车类别的基线 PV-RCNN 分别改进了 0.82%、0.74% 和 0.58%。此外，我们的方法在测试服务器上的中等和困难级别 BEV AP 下的性能优于基线 1% 以上。]]></description>
      <guid>https://arxiv.org/abs/2405.06782</guid>
      <pubDate>Tue, 14 May 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>双任务视觉转换器可在 CT 图像上快速准确地进行脑出血分类</title>
      <link>https://arxiv.org/abs/2405.06814</link>
      <description><![CDATA[arXiv:2405.06814v1 公告类型：新
【摘要】：脑出血(ICH)是一种因脑血管破裂引起的严重突发疾病,可导致脑组织永久性损伤,常导致患者功能残疾或死亡。 ICH 的诊断和分析通常依赖于脑部 CT 成像。鉴于脑出血病情的紧迫性，早期治疗至关重要，需要快速分析 CT 图像以制定量身定制的治疗计划。然而，ICH CT 图像的复杂性和专业放射科医生的经常短缺带来了重大挑战。因此，我们建立了一个用于 ICH 和正常分类的数据集，以及基于出血位置的三种类型的 ICH 图像分类，即深部、皮质下和脑叶。此外，我们提出了一种双任务视觉转换器（DTViT），用于 ICH 图像的自动分类和诊断。该神经网络利用 ViT 的编码器，采用注意力机制从 CT 图像中提取特征。我们在网络中加入了两个基于多层感知 (MLP) 的解码器，以同时识别 ICH 的存在并对三种类型的出血位置进行分类。实验结果表明，我们提出的多分类网络在构建的真实测试数据集上表现良好。本研究的代码和数据集将在论文接受后公开发布：https://github.com/Jialiangfan/ICH-classification。]]></description>
      <guid>https://arxiv.org/abs/2405.06814</guid>
      <pubDate>Tue, 14 May 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>增强和评估空对空视觉目标检测鲁棒性的常见损坏</title>
      <link>https://arxiv.org/abs/2405.06765</link>
      <description><![CDATA[arXiv:2405.06765v1 公告类型：新
摘要：实现完全自主飞行的主要障碍在于飞机自主导航。管理非合作流量是这个问题中最重要的挑战。处理非合作流量的最有效策略是通过深度学习模型进行单目视频处理。这项研究通过调查环境和硬件条件引起的数据损坏对这些方法有效性的影响，为基于视觉的深度学习飞机检测和跟踪文献做出了贡献。更具体地说，考虑到现实世界的飞行条件，我们为相机输入设计了 7 美元类型的常见损坏。通过将这些损坏应用于机载物体跟踪 (AOT) 数据集，我们构建了第一个用于空对空空中物体检测的鲁棒性基准数据集，名为 AOT-C。该数据集中包含的损坏涵盖了各种具有挑战性的条件，例如恶劣天气和传感器噪声。这封信的第二个主要贡献是提出了一项广泛的实验评估，涉及 8 美元的不同目标检测器，以探索在腐败程度不断升级（域转移）的情况下性能的下降。根据评估结果，出现的关键观察结果如下：1）YOLO 系列的一级检测器表现出更好的鲁棒性，2）基于 Transformer 的多级检测器（如 Faster R-CNN）极易受到损坏， 3）抗腐败的鲁棒性与模型的泛化能力有关。第三个主要贡献是提出对我们的增强合成数据进行微调可以提高现实世界飞行实验中物体探测器的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2405.06765</guid>
      <pubDate>Tue, 14 May 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散模型生成形状条件人体运动</title>
      <link>https://arxiv.org/abs/2405.06778</link>
      <description><![CDATA[arXiv:2405.06778v1 公告类型：新
摘要：人体运动合成是计算机图形学和计算机视觉中的一项重要任务。虽然关注文本、动作类或音频等各种条件信号来指导生成过程，但大多数现有方法都利用基于骨架的姿势表示，需要额外的蒙皮来生成可渲染的网格。鉴于人体运动是骨骼、关节和肌肉的复杂相互作用，仅考虑生成骨骼可能会忽略它们固有的相互依赖性，这会限制生成结果的可变性和精度。为了解决这个问题，我们提出了一种形状条件运动扩散模型（SMD），它可以直接以网格格式生成运动序列，以指定的目标网格为条件。在 SMD 中，使用图拉普拉斯将输入网格转换为谱系数，以有效地表示网格。随后，我们提出了一种谱时自动编码器（STAE）来利用谱域内的跨时依赖性。大量的实验评估表明，与最先进的方法相比，SMD 不仅可以产生生动逼真的动作，而且在文本到动作和动作到动作任务中实现了有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.06778</guid>
      <pubDate>Tue, 14 May 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>确保无人机安全：通过目标检测、跟踪和距离估计避免碰撞的纯视觉实时框架</title>
      <link>https://arxiv.org/abs/2405.06749</link>
      <description><![CDATA[arXiv:2405.06749v1 公告类型：新
【摘要】：近二十年来,无人机(UAV)因其在军事和民用领域的广泛应用而引起了越来越多的关注。高效检测非合作飞行器并准确估计碰撞对于实现完全自主飞机和促进先进空中机动性 (AAM) 至关重要。本文提出了一种深度学习框架，利用光学传感器来检测、跟踪和估计非合作飞行器的距离。在实施这个综合传感框架时，深度信息的可用性对于自动驾驶飞行器感知和绕过障碍物至关重要。在这项工作中，我们提出了一种仅使用单目相机的输入来实时估计检测到的空中物体的距离信息的方法。为了训练我们的深度学习组件来执行对象检测、跟踪和深度估计任务，我们利用 Amazon 机载对象跟踪 (AOT) 数据集。与之前将深度估计模块集成到对象检测器中的方法相比，我们的方法将问题表述为图像到图像的转换。我们采用单独的轻量级编码器-解码器网络来实现高效且稳健的深度估计。简而言之，物体检测模块识别并定位障碍物，将该信息传递给用于监视障碍物移动的跟踪模块和用于计算距离的深度估计模块。我们的方法在机载物体跟踪（AOT）数据集上进行评估，该数据集是最大的（据我们所知）空对空机载物体数据集。]]></description>
      <guid>https://arxiv.org/abs/2405.06749</guid>
      <pubDate>Tue, 14 May 2024 06:20:33 GMT</pubDate>
    </item>
    </channel>
</rss>