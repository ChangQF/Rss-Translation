<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Tue, 19 Dec 2023 06:17:48 GMT</lastBuildDate>
    <item>
      <title>NM-FlowGAN：使用基于标准化流和生成对抗网络的混合方法对 sRGB 噪声进行建模。 （arXiv：2312.10112v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10112</link>
      <description><![CDATA[建模和合成真实的 sRGB 噪声对于各种低级噪声至关重要
视觉任务。真实 sRGB 噪声的分布非常复杂，并且
受多种因素的影响，使得其建模的精确度极高
具有挑战性的。因此，最近的研究提出了采用的方法
数据驱动的生成模型，例如生成对抗网络（GAN）
和标准化流程。这些研究实现了更准确的 sRGB 建模
噪声与传统噪声建模方法的比较。然而，有
由于每个生成器的固有特征而导致性能限制
模型。为了解决这个问题，我们提出了 NM-FlowGAN，这是一种混合方法
利用 GAN 和标准化流的优势。我们同时
采用基于归一化流的逐像素噪声建模网络，以及
基于GAN的空间相关建模网络。在我们的实验中，我们的
NM-FlowGAN 在 sRGB 噪声合成任务上优于其他基线。
此外，用合成图像对训练的去噪神经网络
从我们的模型来看，与其他基线相比，也显示出卓越的性能。
我们的代码位于：https://github.com/YoungJooHan/NM-FlowGAN
]]></description>
      <guid>http://arxiv.org/abs/2312.10112</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>专注于您的指令：通过注意力调制进行细粒度和多指令图像编辑。 （arXiv：2312.10113v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10113</link>
      <description><![CDATA[最近，基于扩散的方法，如 InstructPix2Pix (IP2P)，已经实现了
有效的基于指令的图像编辑，仅需要自然语言
来自用户的指示。然而，这些方法往往会在不经意间改变
意外的区域并与多指令编辑作斗争，导致
结果受到损害。为了解决这些问题，我们引入了“关注您的”
指令（FoI），一种旨在确保精确和谐编辑的方法
无需额外的训练或测试时间优化即可跨越多个指令。
在FoI中，我们主要强调两个方面：（1）精准提取
每条指令的感兴趣区域以及 (2) 指导去噪过程
集中于这些感兴趣的区域。为了第一个目标，我们
从交叉注意力中识别IP2P的隐式接地能力
在指令和图像之间，然后开发有效的掩模提取
方法。对于第二个目标，我们引入交叉注意力调制
用于粗略隔离目标编辑区域和不相关区域的模块。
此外，我们引入了一种掩模引导的解缠采样策略
进一步确保明确的区域隔离。实验结果表明
FoI 在定量和定性方面都超越了现有方法
评估，特别是在多指令编辑任务中表现出色。
]]></description>
      <guid>http://arxiv.org/abs/2312.10113</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>启发您的声音：当多模态遇到零次低光图像增强时。 （arXiv：2312.10109v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10109</link>
      <description><![CDATA[低光图像增强是一项至关重要的视觉任务，许多无监督的任务
方法往往会忽视弱光下可见信息的退化
场景，这对互补信息的融合产生不利影响
阻碍了令人满意的结果的产生。为了解决这个问题，我们的研究
推出了“Enlighten-Your-Voice”，这是一个多模式增强框架，
通过语音和文本命令创新地丰富了用户交互。这
这种方法不仅意味着技术上的飞跃，而且代表了一种
用户参与的范式转变。我们的型号配备了双
协作注意力模块 (DCAM) 精心迎合不同的需求
内容和颜色差异，从而促进细致入微的增强。
作为补充，我们引入了语义特征融合（SFM）即插即用
将语义上下文与低光增强操作协同作用的模块，
提高算法的效率。至关重要的是，“启发你的声音”
在无监督的零样本场景中展示了卓越的泛化能力。这
源代码可以从
https://github.com/zhangbaijin/Enlighten-Your-Voice
]]></description>
      <guid>http://arxiv.org/abs/2312.10109</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>Plasticine3D：带有文本引导的非刚性 3D 编辑。 （arXiv：2312.10111v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10111</link>
      <description><![CDATA[借助分数蒸馏采样（SDS）的快速发展
各种可训练的 3D 表示、文本到图像 (T2I) 扩散模型
已应用于3D生成任务并取得了可观的成果。
还有一些尝试利用 3D 对象编辑任务
这个文本到 3D 管道。然而，目前大多数方法都集中在添加
额外的几何形状、覆盖纹理或两者兼而有之。但他们中很少有人能
执行 3D 对象的非刚性变换。对于那些有能力表演的人
另一方面，非严格编辑则面临分辨率低、缺乏
保真度和灵活性较差。为了解决这些问题，我们提出：
Plasticine3D，通用、高保真、逼真、可控
非严格的编辑管道。首先，我们的工作将编辑过程分为
几何编辑阶段和纹理编辑阶段以实现更详细的效果
和照片般真实的结果；其次，为了实现非刚性
转型的结果可控，同时保持对目标的忠诚度
在原始3D模型的同时，我们提出了多视图嵌入（MVE）
优化策略，确保扩散模型学习整体
原始对象的特征和嵌入融合（EF）来控制
通过调整融合率的值来编辑程度。我们还设计了一个
在优化基础几何图形之前的几何处理步骤以应对
各种编辑任务的不同需求。此外，要充分发挥
原始 3D 对象的几何先验，我们提供可选的替换
分数蒸馏抽样称为分数投影抽样（SPS）
使我们能够在大多数情况下直接从原始 3D 网格执行优化
常见的中值非刚性编辑场景。我们证明了以下方法的有效性
我们的方法适用于非刚性 3D 编辑任务和一般 3D 编辑任务。
]]></description>
      <guid>http://arxiv.org/abs/2312.10111</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>ICD-LM：通过语言建模配置视觉语言上下文演示。 （arXiv：2312.10104v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10104</link>
      <description><![CDATA[本文研究了如何配置强大的上下文演示（ICD）
用于解决视觉语言问题的大型视觉语言模型 (LVLM) 的序列
通过情境学习（ICL）完成任务。观察配置后
ICD 序列是造句的镜像过程，即，就像
句子可以通过语言模型逐字组成，ICD 序列可以
也可以一一配置。因此，我们引入了 ICD 语言模型
(ICD-LM) 专门设计用于生成有效的 ICD 序列。这
涉及为各种查询创建手工制作的 ICD 序列数据集
样本并用它来训练 ICD-LM。我们的方法不同于
NLP 中单独选择和订购 ICD 的传统方法能够
同时学习如何选择和订购 ICD，增强治疗效果
序列。此外，在数据构建过程中，我们使用了用于ICL的LVLM
实施来验证每个 ICD 序列的强度，从而产生
模型特定的数据集和由该数据集训练的 ICD-LM 也是
特定于型号。我们通过视觉实验验证我们的方法
问答和图像标题，确认使用
ICD 配置的语言模型。我们全面的消融研究
进一步探讨各种数据集构建和ICD-LM的影响
结果的发展设置。代码给出在
https://github.com/ForJadeForest/ICD-LM。
]]></description>
      <guid>http://arxiv.org/abs/2312.10104</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>锻造令牌以提高存储效率的培训。 （arXiv：2312.10105v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10105</link>
      <description><![CDATA[深度神经网络 (DNN) 模型的最新进展显着
提高了计算机视觉任务的性能。然而，实现高度
可推广和高性能的视觉模型需要广泛的数据集，
导致大量的存储需求。这一存储挑战带来了严峻的挑战
扩大视觉模型的瓶颈。受离散技术成功的激励
表示，SeiT 建议使用矢量量化（VQ）特征向量
（即标记）作为视觉分类的网络输入。然而，申请
由于输入域的原因，传统的令牌数据增强面临挑战
转移。为了解决这个问题，我们引入TokenAdapt和ColorAdapt，简单
然而有效的基于代币的增强策略。 TokenAdapt 重新调整代币
嵌入空间以与空间增强兼容，保留
模型的效率无需微调。此外，色彩适应
解决了受自适应实例启发的基于颜色的标记增强
标准化（AdaIN）。我们在各种场景中评估我们的方法，
包括存储高效的 ImageNet-1k 分类、细粒度
分类、稳健性基准和 ADE-20k 语义分割。
实验结果表明，不同领域的性能得到了一致的提高
实验。代码可在 https://github.com/naver-ai/tokenadapt 获取。
]]></description>
      <guid>http://arxiv.org/abs/2312.10105</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>隐私意识文档视觉问答。 （arXiv：2312.10108v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10108</link>
      <description><![CDATA[文档视觉问答（DocVQA）是一个快速发展的分支
文档理解。尽管文件包含敏感或
受版权保护的信息，当前的 DocVQA 方法都没有提供强大的
隐私保证。

在这项工作中，我们首次探索 DocVQA 领域的隐私。
我们强调最先进的多模式法学硕士模型中的隐私问题
DocVQA，并探索可能的解决方案。

具体来说，我们将发票处理用例视为现实的、
广泛使用的文档理解场景，并提出了大规模
DocVQA 数据集包含发票文档和相关问题以及
答案。我们采用联邦学习方案，反映现实生活
在不同的业务中分发文档，我们探索用例
其中发票开具人ID为敏感信息
受保护。

我们证明非私有模型倾向于记住可以记住的行为
导致私人信息泄露。然后我们评估基线训练
在这方面采用联邦学习和差异隐私的方案
多模态场景，敏感信息可能通过以下方式暴露
两种输入方式中的任何一种：视觉（文档图像）或语言（OCR）
代币）。

最后，我们设计了一种利用模型记忆效应的攻击，
并证明其在探索不同 DocVQA 模型中的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2312.10108</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>ADA-YOLO：YOLOv8 和自适应头的动态融合，用于精确图像检测和诊断。 （arXiv：2312.10099v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10099</link>
      <description><![CDATA[目标检测和定位是生物医学图像的关键任务
分析，特别是在血液学领域，其中检测和
血细胞的识别对于诊断和治疗决策至关重要。
虽然基于注意力的方法在对象方面取得了显着进展
各个领域的检测，它们在医疗对象检测中的应用
由于医学成像数据集带来的独特挑战而受到限制。到
为了解决这个问题，我们提出了 ADA-YOLO，一种轻量级但有效的方法
用于将基于注意力的机制与
YOLOv8 架构。我们提出的方法利用动态特征
计算机视觉任务的定位和并行回归
\textit{自适应头} 模块。进行了实证实验
用于评估有效性的血细胞计数和检测 (BCCD) 数据集
ADA-YOLO。结果表明ADA-YOLO在mAP方面优于YOLOv8模型
（平均平均精度）在 BCCD 数据集上使用 3 倍以上的更少
比 YOLOv8 空间大。这表明我们提出的方法是有效的。
此外，我们提出的方法的轻量级性质使其适用于
在资源受限的环境（例如移动设备或边缘）中部署
计算系统。这最终可能导致诊断的改进和
血液学领域的治疗结果。
]]></description>
      <guid>http://arxiv.org/abs/2312.10099</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>GSVA：通过多模式大语言模型进行广义细分。 （arXiv：2312.10103v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10103</link>
      <description><![CDATA[广义引用表达分段 (GRES) 扩展了范围
经典 RES 在一个表达式中引用多个对象或识别
图像中不存在的空目标。 GRES 给建模带来了挑战
图像中实例的复杂空间关系和识别
不存在的参照物。最近，多模态大语言模型（MLLM）已经
在这些复杂的视觉语言任务中显示出巨大的进步。
连接大型语言模型 (LLM) 和视觉模型，MLLM 精通
通过视觉输入理解上下文。其中，LISA作为
代表，采用特殊的[SEG]标记来提示分段掩码
解码器，例如 SAM，在 RES 任务中启用 MLLM。然而，现有的
由于当前的分割 MLLM，GRES 的解决方案仍然不能令人满意
无法正确处理用户可能引用多个主题的情况
以单一提示或提供与任何图像目标不一致的描述。
在本文中，我们提出了广义分割视觉助手（GSVA）
解决这一差距。具体来说，GSVA 重用 [SEG] 令牌来提示
同时支持多个掩模参考的分割模型
并创新地学习生成 [REJ] 令牌来拒绝空目标
明确地。实验验证了 GSVA 解决 GRES 问题的功效，
显着增强并在 GRES 基准上创下新记录
gRefCOCO 数据集。 GSVA 在各种经典推荐中也被证明是有效的
表达分割和理解任务。
]]></description>
      <guid>http://arxiv.org/abs/2312.10103</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>Gaussian-SLAM：具有高斯泼溅的照片级真实感密集 SLAM。 （arXiv：2312.10070v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10070</link>
      <description><![CDATA[我们提出了一种新的密集同步定位与建图（SLAM）方法
使用高斯图作为场景表示。新的代表
实现交互式时间重建和照片级真实感渲染
真实世界和合成场景。我们提出了新的播种策略和
优化高斯splats以扩展其在多视图离线场景中的使用
到顺序单目 RGBD 输入数据设置。此外，我们扩展高斯
splats 对几何体进行编码并尝试针对该场景进行跟踪
表示。我们的方法在两者上都实现了最先进的渲染质量
真实世界和合成数据集，同时在重建方面具有竞争力
性能和运行时间。
]]></description>
      <guid>http://arxiv.org/abs/2312.10070</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>公平医疗成像人工智能的局限性。 （arXiv：2312.10083v1 [cs.CY]）</title>
      <link>http://arxiv.org/abs/2312.10083</link>
      <description><![CDATA[随着人工智能 (AI) 迅速接近人类水平的表现
医学成像，重要的是它不会加剧或传播
医疗保健差距。先前的研究已经确立了人工智能的推理能力
来自胸部 X 光的人口统计数据，导致了一个关键问题：模型是否使用
人口统计捷径对亚人群的预测不公平？在这个
研究中，我们对医疗人工智能的程度进行了彻底的调查
利用人口统计编码，重点关注潜在的公平性差异
在分布内训练集和外部测试集中。我们的分析
涵盖三个关键的医学影像学科：放射学、皮肤病学和
眼科，并整合了来自六个全球胸部 X 射线数据集的数据。我们
确认医学成像人工智能利用了疾病中的人口捷径
分类。同时有效地通过算法纠正捷径
解决公平差距，在原始模型中创建“局部最优”模型
数据分布，这种最优性在新的测试设置中并不成立。
令人惊讶的是，我们发现人口统计属性编码较少的模型
通常是最“全局最优”的，在模型过程中表现出更好的公平性
在新的测试环境中进行评估。我们的工作为以下方面建立了最佳实践
保持其性能和公平性的医学成像模型
部署超出了最初的训练环境，强调了关键
跨人群和地点的人工智能临床部署的考虑因素。
]]></description>
      <guid>http://arxiv.org/abs/2312.10083</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>视听语音识别中丢失视频的鲁棒性。 （arXiv：2312.10088v1 [eess.AS]）</title>
      <link>http://arxiv.org/abs/2312.10088</link>
      <description><![CDATA[事实证明，学习视听特征可以提高
语音识别性能优于纯音频功能，尤其是对于嘈杂的情况
演讲。然而，在许多常见的应用中，视觉特征部分被忽视。
或完全缺失，例如，扬声器可能会移出屏幕。多模式模型
需要稳健：丢失视频帧不应降低性能
视听模型比单模态音频模型更差
模型。尽管人们在构建稳健模型方面进行了许多尝试，但
关于如何评估稳健性几乎没有达成共识。为了解决这个问题，我们
引入一个框架，允许在一个框架中评估有关稳健性的声明
精确且可测试的方式。我们还进行了系统的实证研究
常见视听语音识别架构在一系列
声学噪声条件和测试套件。最后，我们证明了一个
基于级联的架构无关的解决方案可以一致地实现
即使在现有技术无法处理的情况下，也能对丢失视频保持鲁棒性
像 dropout 这样的鲁棒性不够。
]]></description>
      <guid>http://arxiv.org/abs/2312.10088</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>基于内容的图像检索的进展：相关性反馈技术的综合调查。 （arXiv：2312.10089v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10089</link>
      <description><![CDATA[基于内容的图像检索（CBIR）系统已成为以下领域的重要工具：
计算机视觉领域，允许基于视觉内容的图像搜索
而不是仅仅依赖元数据。这份调查报告提出了
CBIR 的全面概述，强调其在物体检测和
它具有根据内容识别和检索视觉上相似图像的潜力
特征。 CBIR 系统面临的挑战，包括语义差距和
讨论了可扩展性以及潜在的解决方案。它详细阐述了
语义差距，由低级特征之间的差异引起
和高级语义概念，并探索弥合这一差距的方法。
一个值得注意的解决方案是集成相关性反馈 (RF)，从而增强
用户对检索到的图像提供反馈并优化搜索结果
迭代地。该调查涵盖长期和短期学习
利用 RF 提高 CBIR 准确性和相关性的方法。这些
方法侧重于权重优化和主动学习的利用
选择用于训练分类器的样本的算法。此外，论文
研究机器学习技术和深度学习的利用
和卷积神经网络来增强 CBIR 性能。本次调查
论文在增进对 CBIR 和 RF 的理解方面发挥着重要作用
技术。它指导研究人员和实践者理解现有的
在培养知识的同时，方法论、挑战和潜在的解决方案
传播并确定研究差距。通过解决未来的研究
方向，它为 CBIR 的进步奠定了基础，这将增强
各种应用中的检索准确性、可用性和有效性
域。
]]></description>
      <guid>http://arxiv.org/abs/2312.10089</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉深度度量学习：简要概述。 （arXiv：2312.10046v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.10046</link>
      <description><![CDATA[优化深度神经网络的目标函数在以下方面发挥着至关重要的作用：
创建输入数据的增强特征表示。虽然
基于交叉熵的损失公式已广泛应用于各种领域
在有监督的深度学习应用中，这些方法往往较少
当类内方差较大且类间方差较小时足够
在输入数据分布中。深度度量学习寻求开发方法
旨在通过学习表示来衡量数据样本之间的相似性
将这些数据样本映射到代表性嵌入空间的函数。它
利用精心设计的采样策略和损失函数来帮助
优化判别性嵌入空间的生成，甚至对于
具有低类间方差和高类内方差的分布。在这个
本章中，我们将概述该领域的最新进展，
讨论最先进的深度度量学习方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.10046</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>了解用于具体代理规划的辅助损失预训练表示。 （arXiv：2312.10069v1 [cs.RO]）</title>
      <link>http://arxiv.org/abs/2312.10069</link>
      <description><![CDATA[大规模视觉模型的预训练表示提高了
下游体现政策学习的绩效。我们希望了解
对探索轨迹进行额外的自我监督预训练是否可以
以这些通用视觉表示为基础来更好地支持
体现在现实环境中的规划。我们评估了四种常见的辅助
体现人工智能的损失，两个基于事后诸葛亮的损失，以及一个标准的模仿
学习损失，通过预训练代理的视觉压缩模块和状态
每个目标的信念表示并使用 CLIP 作为代表
视觉支柱。然后将学习到的表示形式冻结以供下游使用
对两个目标导向任务的多步骤评估。令人惊讶的是，我们发现
在这些探索轨迹上的模仿学习优于所有其他
即使勘探轨迹不同，也会产生辅助损失
来自下游任务。这表明探索的模仿可能是
构建强大的规划表示所需的“一切”。此外，
我们发现流行的辅助损失可以从简单的修改中受益
提高对下游规划的支撑能力。
]]></description>
      <guid>http://arxiv.org/abs/2312.10069</guid>
      <pubDate>Tue, 19 Dec 2023 06:17:43 GMT</pubDate>
    </item>
    </channel>
</rss>