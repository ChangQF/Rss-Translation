<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 25 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>如何了解更多？探索 Kolmogorov-Arnold 网络用于高光谱图像分类</title>
      <link>https://arxiv.org/abs/2406.15719</link>
      <description><![CDATA[arXiv:2406.15719v1 公告类型：新
摘要：卷积神经网络 (CNN) 和视觉变换器 (ViT) 在复杂高光谱图像 (HSI) 分类中表现出色。然而，这些模型需要大量的训练数据，并且需要大量的计算资源。另一方面，现代多层感知器 (MLP) 表现出了很强的分类能力。与 CNN 和 ViT 相比，这些基于 MLP 的现代模型需要的训练数据要少得多，但却能达到最先进的分类精度。最近，Kolmogorov-Arnold 网络 (KAN) 被提议作为 MLP 的可行替代方案。由于 KAN 的内部与样条相似，外部与 MLP 相似，因此除了能够学习新特征外，它还能够以惊人的精度优化学习到的特征。因此，在本研究中，我们评估了 KAN 对复杂 HSI 数据分类的有效性。此外，为了提高 KAN 获得的 HSI 分类准确率，我们开发并提出了一种利用 1D、2D 和 3D KAN 的混合架构。为了证明所提出的 KAN 架构的有效性，我们在三个新创建的 HSI 基准数据集上进行了广泛的实验：QUH-Pingan、QUH-Tangdaowan 和 QUH-Qingyun。结果强调了开发的基于混合 KAN 的模型在这些基准数据集上的能力优于其他几种基于 CNN 和 ViT 的算法，包括 1D-CNN、2DCNN、3D CNN、VGG-16、ResNet-50、EfficientNet、RNN 和 ViT。代码可在以下网址公开获取 (https://github.com/aj1365/HSIConvKAN)]]></description>
      <guid>https://arxiv.org/abs/2406.15719</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:29 GMT</pubDate>
    </item>
    <item>
      <title>用于医学图像分割的自监督对齐学习</title>
      <link>https://arxiv.org/abs/2406.15699</link>
      <description><![CDATA[arXiv:2406.15699v1 公告类型：新
摘要：最近，自监督学习 (SSL) 方法已用于预训练 2D 和 3D 医学图像的分割模型。这些方法大多基于重建、对比学习和一致性正则化。然而，3D 医学图像中 2D 切片的空间对应关系尚未得到充分利用。在本文中，我们提出了一种新颖的自监督对齐学习框架来预训练用于医学图像分割的神经网络。所提出的框架由新的局部对齐损失和全局位置损失组成。我们观察到在同一个 3D 扫描中，两个相近的 2D 切片通常包含相似的解剖结构。因此，提出局部对齐损失以使匹配结构的像素级特征彼此接近。实验结果表明，在有限注释的设置下，所提出的对齐学习与 CT 和 MRI 数据集上现有的自监督预训练方法具有竞争力。]]></description>
      <guid>https://arxiv.org/abs/2406.15699</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>视频-SALMONN：语音增强视听大型语言模型</title>
      <link>https://arxiv.org/abs/2406.15704</link>
      <description><![CDATA[arXiv:2406.15704v1 公告类型：新
摘要：语音理解作为使用视听大型语言模型 (av-LLM) 进行更通用的视频理解的一个元素，是一个至关重要但研究不足的方面。本文提出了 video-SALMONN，一种用于视频处理的单一端到端 av-LLM，它不仅可以理解视觉帧序列、音频事件和音乐，还可以理解语音。为了获得语音理解所需的细粒度时间信息，同时保持对其他视频元素的效率，本文提出了一种新颖的多分辨率因果 Q-Former (MRC Q-Former) 结构来连接预训练的视听编码器和骨干大型语言模型。此外，提出了包括多样性损失和不成对的视听混合训练方案在内的专用训练方法，以避免帧或模态占主导地位。在引入的语音-音频-视频评估基准上，video-SALMONN 在视频问答任务上实现了超过 25% 的绝对准确率提升，在人类语音的音频视频问答任务上实现了超过 30% 的绝对准确率提升。此外，video-SALMONN 在其他 av-LLM 未曾实现的任务上表现出卓越的视频理解和推理能力。我们的训练代码和模型检查点可在 \texttt{\url{https://github.com/bytedance/SALMONN/}} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.15704</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>psPRF：用于广义 3D 重建卫星图像的全色锐化平面神经辐射场</title>
      <link>https://arxiv.org/abs/2406.15707</link>
      <description><![CDATA[arXiv:2406.15707v1 公告类型：新 
摘要：目前大多数用于卫星的 NeRF 变体都是针对特定场景设计的，无法推广到新的几何图形。此外，RGB 图像需要全色锐化作为独立的预处理步骤。本文介绍了 psPRF，这是一种平面神经辐射场，专为使用有理多项式相机 (RPC) 的卫星传感器的成对低分辨率 RGB (LR-RGB) 和高分辨率全色 (HR-PAN) 图像而设计。为了从 LR-RGB 和 HR-PAN 图像中捕获跨模态先验，对于 Unet 形架构，我们使用显式光谱到空间卷积 (SSConv) 调整编码器以增强多模态表示能力。为了支持 psRPF 跨场景的泛化能力，我们采用投影损失来确保强大的几何自监督。使用多场景 WorldView-3 LR-RGB 和 HR-PAN 对对所提出的方法进行了评估，并取得了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.15707</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>分割死海古卷碎片以获取科学图像集</title>
      <link>https://arxiv.org/abs/2406.15692</link>
      <description><![CDATA[arXiv:2406.15692v1 公告类型：新
摘要：本文介绍了一种定制的流程，用于从以色列文物管理局 (IAA) 策划的图像中分割手稿碎片。由于标尺、颜色和车牌号条的存在，以及类似于墨水和不同背衬基材的黑色背景，这些图像对标准分割方法提出了挑战。所提出的流程由四个步骤组成，通过使用定制方法隔离和解决每个困难来解决这些挑战。此外，从概念的角度来看，使用多步骤流程肯定会对其他图像分割项目有所帮助，这些项目在应用任何更常用的分割技术时都会遇到难以解决的问题。此外，我们创建了一个包含条形检测和碎片分割基本事实的数据集，并在其上定性和定量评估流程步骤。该数据集是公开的，以支持该领域的发展。它旨在解决缺乏标准的碎片图像集和评估指标的问题，并使研究人员能够以可靠和可重复的方式评估他们的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.15692</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>用于通用遥感变化检测的单时间监督学习</title>
      <link>https://arxiv.org/abs/2406.15694</link>
      <description><![CDATA[arXiv:2406.15694v1 公告类型：新
摘要：双时态监督学习范式始终主导使用大量标记双时态图像对的遥感变化检测，尤其是对于高空间分辨率 (HSR) 遥感图像。然而，在大规模双时态 HSR 遥感图像对中标记变化区域非常昂贵且劳动密集。在本文中，我们从利用不成对图像之间的变化作为监督信号的新视角提出了用于通用遥感变化检测的单时态监督学习 (STAR)。STAR 使我们能够仅使用不成对的标记图像来训练高精度变化检测器，并且可以推广到现实世界的双时态图像对。为了展示 STAR 的灵活性和可扩展性，我们设计了一个简单而统一的变化检测器，称为 ChangeStar2，能够在一个架构中解决二进制变化检测、对象变化检测和语义变化检测。 ChangeStar2 在八个公开遥感变化检测数据集上取得了最佳性能，涵盖了上述两种监督设置、多种变化类型、多种场景。代码可在 https://github.com/Z-Zheng/pytorch-change-models 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.15694</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>高效人体姿势估计：利用 MediaPipe 的先进技术</title>
      <link>https://arxiv.org/abs/2406.15649</link>
      <description><![CDATA[arXiv:2406.15649v1 公告类型：新
摘要：本研究展示了使用 MediaPipe 框架对人体姿势估计的重大改进。该研究的重点是通过全面优化底层算法来提高准确性、计算效率和实时处理能力。引入了新的修改，大大提高了具有挑战性的场景（例如动态运动和部分遮挡）中的姿势估计准确性。改进后的框架与传统模型进行了对比，显示出相当大的精度和计算速度提升。这些进步在增强现实、体育分析和医疗保健领域有着广泛的应用，可实现更身临其境的体验、更精细的性能分析和先进的患者监测。该研究还探讨了这些增强功能在移动和嵌入式系统中的集成，以满足对计算效率和更广泛可访问性的需求。这项研究的意义为实时人体姿势估计技术树立了新的标杆，并为该领域的未来创新铺平了道路。本文的实现代码可在 https://github.com/avhixd/Human_pose_estimation 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.15649</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:26 GMT</pubDate>
    </item>
    <item>
      <title>TorchSpatial：空间表征学习的位置编码框架和基准</title>
      <link>https://arxiv.org/abs/2406.15658</link>
      <description><![CDATA[arXiv:2406.15658v1 公告类型：新
摘要：空间表示学习 (SRL) 旨在从各种类型的空间数据（例如点、折线、多边形、网络、图像等）的原始格式中学习通用神经网络表示。学习良好的空间表示是各种下游应用的基本问题，例如物种分布建模、天气预报、轨迹生成、地理问答等。尽管 SRL 已成为几乎所有地理空间人工智能 (GeoAI) 研究的基础，但我们尚未看到为开发广泛的深度学习框架和基准以支持 SRL 模型开发和评估而做出的重大努力。为了填补这一空白，我们提出了 TorchSpatial，这是一个用于位置（点）编码的学习框架和基准，它是空间表示学习最基本的数据类型之一。 TorchSpatial 包含三个关键组件：1）一个统一的位置编码框架，整合了 15 个公认的位置编码器，确保实现的可扩展性和可重复性；2）LocBench 基准测试任务涵盖 7 个地理感知图像分类和 4 个地理感知图像回归数据集；3）一套全面的评估指标，用于量化地理感知模型的整体性能及其地理偏差，并采用新颖的 Geo-Bias Score 指标。最后，我们对不同位置编码器的模型性能和地理偏差进行了详细的分析和见解。我们相信 TorchSpatial 将促进 GeoAI 研究中空间表征学习和空间公平性的未来发展。TorchSpatial 模型框架、LocBench 和 Geo-Bias Score 评估框架可在 https://github.com/seai-lab/TorchSpatial 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.15658</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:26 GMT</pubDate>
    </item>
    <item>
      <title>驯服 3DGS：使用有限资源实现高质量辐射场</title>
      <link>https://arxiv.org/abs/2406.15643</link>
      <description><![CDATA[arXiv:2406.15643v1 公告类型：新
摘要：3D 高斯分层 (3DGS) 以其快速、可解释和高保真渲染改变了新视图合成。然而，它的资源需求限制了它的可用性。特别是在受限的设备上，训练性能会迅速下降，并且由于模型的内存消耗过多而经常无法完成。该方法以无限数量的高斯函数收敛——其中许多是多余的——使得渲染不必要地缓慢，并阻止其在需要固定大小输入的下游任务中使用。为了解决这些问题，我们解决了在预算内训练和渲染 3DGS 模型的挑战。我们使用引导式、纯建设性的致密化过程，将致密化引导到提高重建质量的高斯函数上。模型大小以受控的方式不断增加，朝着精确的预算方向发展，使用基于分数的高斯致密化和训练时间先验来衡量它们的贡献。我们进一步解决了训练速度障碍：在仔细分析了 3DGS 的原始流程后，我们得出了更快、数值等效的梯度计算和属性更新解决方案，包括用于高效反向传播的替代并行化。我们还提出了在适当的情况下保持质量的近似值，以进一步缩短训练时间。总而言之，这些增强功能产生了一个强大、可扩展的解决方案，缩短了训练时间，降低了计算和内存要求，并且质量很高。我们的评估表明，在预算范围内，我们通过 3DGS 获得了具有竞争力的质量指标，同时将模型大小和训练时间缩短了 4-5 倍。在预算更充裕的情况下，我们测量的质量超过了他们。这些进步为受限环境（例如移动设备）中的新视图合成打开了大门。]]></description>
      <guid>https://arxiv.org/abs/2406.15643</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:25 GMT</pubDate>
    </item>
    <item>
      <title>VigilEye——基于人工智能的实时驾驶员疲劳检测</title>
      <link>https://arxiv.org/abs/2406.15646</link>
      <description><![CDATA[arXiv:2406.15646v1 公告类型：新
摘要：本研究提出了一种新型驾驶员困倦检测系统，该系统将深度学习技术与 OpenCV 框架相结合。该系统利用从驾驶员面部提取的面部特征作为卷积神经网络的输入，该神经网络经过训练可识别困倦模式。OpenCV 的集成实现了实时视频处理，使该系统适合实际实施。对各种数据集进行的大量实验表明，该系统在检测困倦方面具有很高的准确性、灵敏度和特异性。该系统有可能通过及时发出警报来防止驾驶员疲劳造成的事故，从而提高道路安全性。这项研究有助于推进实时驾驶员监控系统，并对汽车安全和智能交通系统产生影响。深度学习技术在此背景下的成功应用为未来驾驶员监控和车辆安全研究开辟了新途径。本文的实现代码可在 https://github.com/LUFFY7001/Driver-s-Drowsiness-Detection 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.15646</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:25 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态引导的开放词汇时间动作定位</title>
      <link>https://arxiv.org/abs/2406.15556</link>
      <description><![CDATA[arXiv:2406.15556v1 公告类型：新
摘要：开放词汇时间动作定位 (OVTAL) 使模型能够识别视频中任何所需的动作类别，而无需明确整理所有类别的训练数据。然而，这种灵活性带来了重大挑战，因为模型不仅必须识别训练期间看到的动作类别，还必须识别推理时指定的新类别。与标准时间动作定位不同，在标准时间动作定位中，训练和测试类别是预先确定的，而 OVTAL 需要理解揭示新类别语义的上下文线索。为了应对这些挑战，我们引入了 OVFormer，这是一个新颖的开放词汇框架，它扩展了 ActionFormer，具有三个关键贡献。首先，我们将任务特定提示作为大型语言模型的输入，以获得丰富的类特定动作类别描述。其次，我们引入了一种交叉注意机制来学习类表示和帧级视频特征之间的对齐，从而促进多模态引导特征。第三，我们提出了一种两阶段训练策略，包括使用更大的词汇数据集进行训练，并对下游数据进行微调以推广到新类别。OVFormer 将现有的 TAL 方法扩展到开放词汇设置。对 THUMOS14 和 ActivityNet-1.3 基准的全面评估证明了我们方法的有效性。代码和预训练模型将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2406.15556</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:24 GMT</pubDate>
    </item>
    <item>
      <title>利用共同外观线索进行看不见的物体推理</title>
      <link>https://arxiv.org/abs/2406.15565</link>
      <description><![CDATA[arXiv:2406.15565v1 公告类型：新
摘要：本文介绍了一种开放世界识别 (OWR) 的创新方法，我们利用从已知物体获得的知识来解决以前看不见的物体的识别问题。传统的物体建模方法依赖于具有严格闭集假设的监督学习，假设在推理过程中遇到的物体在训练阶段已经为人所知。然而，由于无法解释物体的巨大多样性，这种假设被证明不适用于现实世界场景。我们的假设认为物体外观可以表示为“可共享”中级特征的集合，这些特征以星座形式排列以形成物体实例。通过采用这个框架，我们可以根据它们的外观线索有效地剖析和表示已知和未知的物体。我们的论文介绍了一种简单而优雅的方法来建模新颖或看不见的物体，利用既定的外观线索并考虑固有的不确定性。这种表示不仅能够检测出分布不均的对象或未知对象中的新类别，而且还有助于更深层次的推理，从而能够识别未知实例所属的超类。这种新颖的方法有望在各种应用中推进开放世界识别。]]></description>
      <guid>https://arxiv.org/abs/2406.15565</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:24 GMT</pubDate>
    </item>
    <item>
      <title>用于多类 OOD 和异常检测的具有跨级特征引导解码器的特征纯化 Transformer</title>
      <link>https://arxiv.org/abs/2406.15396</link>
      <description><![CDATA[arXiv:2406.15396v1 公告类型：新
摘要：重建网络由于独立于标记异常数据而广泛用于无监督异常和分布外 (OOD) 检测。然而，在多类数据集中，异常检测的有效性通常会受到模型广义重建能力的影响，这会导致异常融入因添加类别而扩大的正态性边界内，从而降低检测准确性。我们引入了 FUTUREG 框架，它包含两个创新模块：特征纯化模块 (FPM) 和 CFG 解码器。FPM 将正态性边界限制在潜在空间内以有效过滤掉异常特征，而 CFG 解码器使用分层编码器表示来指导过滤特征的重建，从而保留细粒度细节。这些模块一起增强了异常的重建误差，确保了正常样本的高质量重建。我们的结果表明，FUTUREG 在多类 OOD 设置中实现了最先进的性能，并且在工业异常检测场景中保持竞争力。]]></description>
      <guid>https://arxiv.org/abs/2406.15396</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:23 GMT</pubDate>
    </item>
    <item>
      <title>基于深度视觉的气候变化影响和海岸线适应下的沿海洪水预测框架</title>
      <link>https://arxiv.org/abs/2406.15451</link>
      <description><![CDATA[arXiv:2406.15451v1 公告类型：新 
摘要：鉴于气候变化和海平面上升 (SLR) 带来的威胁日益严重，对计算效率高的方法来估计和分析潜在的沿海洪水灾害的需求变得越来越迫切。数据驱动的监督学习方法是有希望的候选方法，可以大大加快这一过程，从而消除与传统基于物理的流体动力学模拟器相关的计算瓶颈。然而，准确可靠的沿海洪水预测模型的开发，特别是基于深度学习 (DL) 技术的模型，一直受到两个主要问题的困扰：(1) 训练数据的稀缺和 (2) 详细淹没地图所需的高维输出。为了消除这一障碍，我们提出了一个系统框架，用于在低数据环境中训练高保真基于深度视觉的沿海洪水预测模型。我们在不同的现有视觉模型上测试了所提出的工作流程，包括完全基于 Transformer 的架构和具有附加注意门的卷积神经网络 (CNN)。此外，我们还引入了一种专门针对当前沿海洪水预测问题的深度 CNN 架构。该模型的设计特别注重其紧凑性，以适应资源受限的场景和可访问性方面。开发的 DL 模型的性能通过常用的地理统计回归方法和传统机器学习 (ML) 方法进行了验证，表明预测质量有了显着提高。最后，我们通过提供使用基于物理的流体动力学模拟器制作的阿布扎比​​海岸合成洪水淹没地图的精心策划的数据集来总结贡献，这可以作为评估未来沿海洪水预测模型的基准。]]></description>
      <guid>https://arxiv.org/abs/2406.15451</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:23 GMT</pubDate>
    </item>
    <item>
      <title>通过变分自动编码器和轨迹预测进行以人为中心的视频异常检测的探索性研究</title>
      <link>https://arxiv.org/abs/2406.15395</link>
      <description><![CDATA[arXiv:2406.15395v1 公告类型：新
摘要：视频异常检测 (VAD) 是计算机视觉领域一项具有挑战性且突出的研究任务。近年来，基于姿势的视频异常检测 (PAD) 引起了研究界的广泛关注，因为尽管偶尔会出现性能不佳的情况，但与基于像素的方法相比，它具有几个固有的优势。具体而言，PAD 的特点是计算复杂度降低、内在隐私保护以及缓解与特定人口群体的歧视和偏见相关的担忧。本文介绍了 TSGAD，一种利用变分自动编码器 (VAE) 和轨迹预测的新型以人为中心的双流图改进异常检测。TSGAD 旨在探索利用 VAE 作为基于姿势的以人为中心的 VAD 的新方法的可能性，同时具有轨迹预测的好处。我们通过对基准数据集的全面实验证明了 TSGAD 的有效性。 TSGAD 展示了与最先进方法相当的结果，展示了采用变分自动编码器的潜力。这为未来的研究工作指明了一个有希望的方向。这项工作的代码库可在 https://github.com/TeCSAR-UNCC/TSGAD 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.15395</guid>
      <pubDate>Tue, 25 Jun 2024 06:20:22 GMT</pubDate>
    </item>
    </channel>
</rss>