<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Wed, 03 Jan 2024 03:14:56 GMT</lastBuildDate>
    <item>
      <title>ScatterFormer：具有分散线性注意力的高效体素变换器。 （arXiv：2401.00912v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00912</link>
      <description><![CDATA[窗式变压器已展现出强大的大规模能力
通过捕获上下文感知表示来理解点云
以更本地化的方式进行负担得起的注意力计算。然而，因为
由于点云的稀疏性，每个窗口的体素数量各不相同
显著地。当前的方法将每个窗口中的体素划分为
多个相同大小的子集，这会在排序和处理方面花费昂贵的开销
填充体素，使它们比基于稀疏卷积的运行速度慢
方法。在本文中，我们提出了 ScatterFormer，它首次
我们最好的知识，可以直接对体素集进行关注
可变长度。 ScatterFormer 的关键在于创新的 Scatter Linear
注意力（SLA）模块，利用线性注意力机制
并行处理分散在不同窗口中的所有体素。利用
GPU 的分层计算单元和矩阵分块算法，我们
将所提出的 SLA 模块的延迟减少到中等程度时少于 1 毫秒
GPU。此外，我们开发了跨窗口交互模块，以同时
增强本地代表性并允许信息跨窗口流动，
消除了车窗移动的需要。我们建议的 ScatterFormer
在大规模 Waymo 开放数据集上展示了 73 mAP (L2)，在
NuScenes 数据集，以 28 FPS 的出色检测率运行。代码
可以在 https://github.com/skyhehe123/ScatterFormer 上找到
]]></description>
      <guid>http://arxiv.org/abs/2401.00912</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:55 GMT</pubDate>
    </item>
    <item>
      <title>Skeleton2vec：一种具有骨架序列情境化目标表示的自监督学习框架。 （arXiv：2401.00921v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00921</link>
      <description><![CDATA[自监督预训练范式已在
基于骨架的动作识别领域。特别是，基于的方法
屏蔽预测将预训练的性能推向了一个新的高度。
然而，这些方法采用低级特征，例如原始关节坐标
或时间运动，作为掩模区域的预测目标，即
次优。在本文中，我们展示了使用高级情境化
作为预测目标的特征可以获得优异的性能。具体来说，
我们提出了 Skeleton2vec，一种简单高效的自监督 3D 动作
表示学习框架，利用基于变压器的教师
编码器将未屏蔽的训练样本作为输入来创建潜在的
上下文表示作为预测目标。受益于
自注意力机制，教师产生的潜在表征
编码器可以合并整个训练样本的全局上下文，
从而使训练任务更加丰富。此外，考虑到高时间
骨架序列中的相关性，我们提出了一种运动感知管掩蔽
将骨架序列分为几个管子并执行的策略
基于运动先验的每个管内的持久掩蔽，从而迫使
模型建立远程时空连接并专注于
动作语义更丰富的区域。在 NTU-60、NTU-120 和
PKU-MMD 数据集证明我们提出的 Skeleton2vec 性能优于
以前的方法并取得了最先进的结果。
]]></description>
      <guid>http://arxiv.org/abs/2401.00921</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:55 GMT</pubDate>
    </item>
    <item>
      <title>基于Deformable-DETR和多级特征融合的白细胞精准检测辅助血液疾病诊断。 （arXiv：2401.00926v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00926</link>
      <description><![CDATA[在标准的医院血液检测中，传统流程要求医生
使用以下方法从患者血液的显微图像中手动分离白细胞
显微镜。然后通过自动对这些分离的白细胞进行分类
白细胞分类器确定不同类型的比例和体积
血液样本中存在的白细胞，有助于疾病诊断。这
方法不仅费时费力，而且还具有一定的局限性。
由于图像质量等因素，很容易出现错误
环境条件，这可能会导致不正确的后续
分类和误诊。为了解决这些问题，本文提出
白细胞检测的创新方法：多级特征融合和
可变形自注意力DETR（MFDS-DETR）。解决白细胞问题
针对规模差异，我们设计了高级筛选特征融合金字塔
（HS-FPN），实现多级融合。该模型使用高级特征作为
通过通道注意模块过滤低级特征信息的权重
然后将筛选出的信息与高级特征进行合并，从而
增强模型的特征表达能力。此外，我们还解决了
通过结合多尺度可变形解决白细胞特征稀缺问题
编码器中的自注意力模块并使用自注意力和
解码器中的交叉可变形注意机制，这有助于
提取白细胞特征图的全局特征。这
所提出的 MFDS-DETR 的有效性、优越性和普遍性
通过与其他尖端白细胞的比较证实了该方法
使用私有 WBCDD、公共 LISC 和 BCCD 数据集的检测模型。我们的
源代码和私有 WBCCD 数据集可在以下位置获取
https://github.com/JustlfC03/MFDS-DETR。
]]></description>
      <guid>http://arxiv.org/abs/2401.00926</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:55 GMT</pubDate>
    </item>
    <item>
      <title>Video-GroundingDINO：迈向开放词汇时空视频接地。 （arXiv：2401.00901v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00901</link>
      <description><![CDATA[视频接地旨在定位视频中的时空部分
对应于输入文本查询。本文解决了一个关键问题
通过引入一种方法来限制当前视频接地方法
开放词汇时空视频基础任务。与流行不同
由于以下原因，封闭集方法难以应对开放词汇场景
有限的训练数据和预定义词汇，我们的模型利用
来自基础空间接地模型的预训练表示。这
使其能够有效地弥合自然语言和语言之间的语义差距
多样化的视觉内容，在封闭场景中表现出色
开放词汇设置。我们的贡献包括一种新颖的时空
视频接地模型，超越封闭组中最先进的结果
对多个数据集进行评估并展示出卓越的性能
开放词汇场景。值得注意的是，所提出的模型优于
VidSTG 上封闭集设置中最先进的方法（声明式和
疑问）和 HC-STVG（V1 和 V2）数据集。此外，在
对 HC-STVG V1 和 YouCook-Interactions（我们的模型）进行开放词汇评估
超过最近表现最好的模型 $4.26$ m_vIoU 和 $1.83\%$
准确性，证明其在处理不同语言和视觉方面的功效
提高视频理解的概念。我们的代码将发布于
https://github.com/TalalWasim/Video-GroundingDINO。
]]></description>
      <guid>http://arxiv.org/abs/2401.00901</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>文本转 3D 生成的乐谱蒸馏中的驯服模式崩溃。 （arXiv：2401.00909v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00909</link>
      <description><![CDATA[尽管乐谱蒸馏在文本转 3D 方面表现出色
一代，此类技术众所周知地遭受视图不一致问题，
也称为“Janus”工件，其中生成的对象用以下方式伪造每个视图
多个正面。尽管经验上有效的方法已接近
这个问题可以通过分数去偏或即时工程来解决，更严格的
解释和解决这个问题的观点仍然难以捉摸。在本文中，
我们揭示了现有的基于分数蒸馏的文本到 3D 生成
框架退化为独立地寻求每个视图的最大似然
从而遭受模式崩溃问题，表现为 Janus
实践中的神器。为了抑制模式崩溃，我们通过以下方式改进分数蒸馏
在相应的变分目标中以熵项重新建立，
应用于渲染图像的分布。最大化熵
鼓励生成的 3D 资产中不同视图之间的多样性，从而
缓解 Janus 问题。基于这个新目标，我们得出一个新的
3D 分数蒸馏的更新规则，称为熵分数蒸馏
（静电放电）。我们从理论上揭示了 ESD 可以通过以下方式简化和实现：
只是在变分数上采用无分类器指导技巧
蒸馏。尽管直白得令人尴尬，但我们广泛的
实验成功证明 ESD 可以有效治疗
乐谱蒸馏中的两面神器。
]]></description>
      <guid>http://arxiv.org/abs/2401.00909</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>WoodScape 自动驾驶运动分割——CVPR 2023 OmniCV 研讨会挑战。 （arXiv：2401.00910v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00910</link>
      <description><![CDATA[运动分割是自动驾驶中一项复杂但不可或缺的任务
驾驶。相机的自我运动带来的挑战，径向
鱼眼镜头的畸变以及对时间一致性的需求使得
任务更加复杂，渲染传统和标准的卷积神经网络
网络 (CNN) 方法效果较差。随之而来的费力数据
标签、多样化和不常见场景的表示以及广泛的数据
捕获要求强调了合成数据对于改进的必要性
机器学习模型的性能。为此，我们采用 PD-WoodScape
由 Parallel Domain 与 WoodScape 鱼眼一起开发的合成数据集
数据集。因此，我们提出了 WoodScape 鱼眼运动分割挑战
自动驾驶，作为 CVPR 2023 研讨会的一部分举行
全向计算机视觉（OmniCV）。作为首批比赛之一
专注于鱼眼运动分割，我们的目标是探索和评估
在该领域利用合成数据的潜力和影响。在本文中，
我们对吸引了众多参赛者的比赛进行了详细的分析
全球 112 个团队参与，共提交 234 份作品。这项研究
描述了运动分割任务固有的复杂性，
强调鱼眼数据集的重要性，阐明鱼眼数据集的必要性
合成数据集及其产生的领域差距，概述了
设计成功解决方案的基础蓝图。随后，我们
深入研究基线实验和获胜方法的细节
评估他们的定性和定量结果，提供有用的
见解。
]]></description>
      <guid>http://arxiv.org/abs/2401.00910</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>TrailBlazer：基于扩散的视频生成的轨迹控制。 （arXiv：2401.00896v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00896</link>
      <description><![CDATA[在最近的文本到视频 (T2V) 生成方法中，实现了
合成视频的可控性通常是一个挑战。通常，这
通过以以下形式提供低级别的每帧指导来解决问题
边缘图、深度图或要更改的现有视频。然而，该过程
获得此类指导可能需要大量劳动力。本文重点讨论
通过使用简单的方法增强视频合成的可控性
边界框以各种方式引导主题，所有这些都不需要
神经网络训练、微调、推理时优化或使用
预先存在的视频。我们的算法 TrailBlazer 是基于
预训练（T2V）模型，且易于实现。该主题由一个
通过提出的空间和时间注意图编辑来确定边界框。
此外，我们引入了关键帧的概念，允许主体
轨迹和整体外观由移动边界框引导
以及相应的提示，无需提供详细的掩码。这
方法是有效的，相对于
底层预训练模型。尽管边界框很简单
指导，产生的运动出奇地自然，具有紧急效果
包括朝向虚拟相机的视角和移动作为盒子的大小
增加。
]]></description>
      <guid>http://arxiv.org/abs/2401.00896</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:53 GMT</pubDate>
    </item>
    <item>
      <title>视觉及其他领域自监督表征学习的掩蔽建模。 （arXiv：2401.00897v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00897</link>
      <description><![CDATA[随着深度学习革命的推进，自监督学习已经
近年来因其出色的表现而受到越来越多的关注
表示学习能力和对标记数据的低依赖性。之中
这些不同的自我监督技术，掩模建模已经成为一种
独特的方法涉及预测原始数据的部分
在训练期间按比例屏蔽。这种范式使深度模型能够
学习稳健的表示并在以下方面表现出卓越的表现
计算机视觉、自然语言处理和其他领域的背景
方式。在这项调查中，我们对蒙版进行了全面的审查
建模框架及其方法。我们详细阐述了以下细节
掩蔽建模中的技术，包括不同的掩蔽策略，
恢复目标、网络架构等。然后，我们系统地
研究其跨领域的广泛应用。此外，我们还
探讨掩蔽建模方法之间的共性和差异
不同的领域。在本文的最后，我们通过讨论
当前技术的局限性，并指出了几种潜在的途径
推进蒙版建模研究。本次调查的论文列表项目是
可以在 \url{https://github.com/Lupin1998/Awesome-MIM} 获取。
]]></description>
      <guid>http://arxiv.org/abs/2401.00897</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:53 GMT</pubDate>
    </item>
    <item>
      <title>提高内容一致超分辨率扩散模型的稳定性。 (arXiv:2401.00877v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2401.00877</link>
      <description><![CDATA[预训练的潜在扩散模型的生成先验具有
表现出增强图像感知质量的巨大潜力
超分辨率（SR）结果。不幸的是，现有的扩散
基于先验的 SR 方法遇到一个常见问题，即它们往往会生成
对于具有不同噪声的相同低分辨率图像，输出相当不同
样品。这种随机性是文本到图像生成任务所需要的，但是
对于 SR 任务来说是有问题的，图像内容预计会很好
保存下来。为了提高基于扩散先验的 SR 的稳定性，我们建议
使用扩散模型来细化图像结构，同时使用
生成对抗训练以增强图像细节。具体来说，我们
提出一种非均匀时间步长学习策略来训练紧凑扩散
网络，具有高效稳定的再现图像主
结构，并对变分自动编码器的预训练解码器进行微调
（VAE）通过对抗性训练来增强细节。广泛的实验
表明我们提出的方法，即内容一致的超分辨率
（CCSR），可以显着降低基于扩散先验的SR的随机性，
提高SR输出的内容一致性并加速图像
生成过程。代码和型号可以在以下位置找到：
{https://github.com/csslc/CCSR}。
]]></description>
      <guid>http://arxiv.org/abs/2401.00877</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:52 GMT</pubDate>
    </item>
    <item>
      <title>来自以自我为中心的立体视频的 3D 人体姿势感知。 （arXiv：2401.00889v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00889</link>
      <description><![CDATA[虽然头戴式设备变得越来越紧凑，但它们提供了以自我为中心的
设备用户有明显自我遮挡的视图。因此，现有的
方法通常无法准确估计以自我为中心的复杂 3D 姿势
意见。在这项工作中，我们提出了一个新的基于变压器的框架来改进
以自我为中心的立体 3D 人体姿势估计，利用场景
以自我为中心的立体视频的信息和时间背景。具体来说，我们
利用 1) 我们的 3D 场景重建模块的深度特征
以自我为中心的立体框架的均匀采样窗口，以及 2) 人体关节
通过视频输入的时间特征增强查询。我们的方法能够
即使在具有挑战性的场景中也能准确估计人体姿势，例如
蹲着和坐着。此外，我们引入了两个新的基准数据集，
即 UnrealEgo2 和 UnrealEgo-RW (RealWorld)。拟议的数据集提供了
大量以自我为中心的立体视图以及更广泛的人类
运动比现有数据集，允许综合评估
现有的和即将推出的方法。我们广泛的实验表明，所提出的
该方法明显优于以前的方法。我们将发布
UnrealEgo2、UnrealEgo-RW 以及我们项目页面上的训练模型。
]]></description>
      <guid>http://arxiv.org/abs/2401.00889</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:52 GMT</pubDate>
    </item>
    <item>
      <title>通过跨模态渗透平衡多模态联邦学习。 （arXiv：2401.00894v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.00894</link>
      <description><![CDATA[联邦学习 (FL) 支撑隐私保护方面的进步
通过协作训练神经网络进行分布式计算，无需
暴露客户的原始数据。当前的 FL 范式主要集中在单模态
数据，同时利用分布式多模式数据中的知识仍然
很大程度上尚未探索。现有的多模态FL（MFL）解决方案主要设计
然而，对于来自输入端的统计或模态异质性，有
分布式的“模态不平衡”这一根本问题还没有解决
条件，这可能导致信息利用不充分和
不同模态的异构知识聚合。在本文中，我们
提出一种新颖的跨模态渗透联邦学习（FedCMI）框架
有效缓解模态不平衡和知识异质性
来自全球主导模式的知识转移。为避免损失
由于仅仅模仿行为而导致的弱模态信息
主导模式，我们设计了两个投影仪模块来集成
来自主导模式的知识，同时仍弘扬当地特色
利用弱模态。此外，我们还引入了一个基于类的
温度适应方案以在不同的环境中实现公平的性能
类。对流行数据集进行了广泛的实验，并为我们提供了
令人欣慰地确认了为充分探索
MFL 中每种模态的信息。
]]></description>
      <guid>http://arxiv.org/abs/2401.00894</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:52 GMT</pubDate>
    </item>
    <item>
      <title>PlanarNeRF：利用神经辐射场在线学习平面基元。 （arXiv：2401.00871v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00871</link>
      <description><![CDATA[从视觉数据中识别空间完整的平面基元是一个
计算机视觉中的关键任务。先前的方法主要限于
2D 段恢复或简化 3D 结构，即使具有广泛的平面
注释。我们提出了 PlanarNeRF，一种能够检测
通过在线学习制作密集的 3D 平面。利用神经场
代表中，PlanarNeRF带来了三大贡献。首先，它增强了
具有并发外观和几何知识的 3D 平面检测。第二，一个
提出了轻量级平面拟合模块来估计平面参数。
第三，具有更新机制的新颖的全局存储体结构是
引入，确保一致的跨框架对应。灵活的
PlanarNeRF 的架构使其能够在 2D 监督和
自我监督的解决方案，它可以有效地从中学习
训练信号稀疏，显着提高训练效率。通过
大量的实验，我们证明了 PlanarNeRF 在
各种场景以及对现有作品的显着改进。
]]></description>
      <guid>http://arxiv.org/abs/2401.00871</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:51 GMT</pubDate>
    </item>
    <item>
      <title>自监督聚类和基于能量的模型的贝叶斯统一。 （arXiv：2401.00873v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.00873</link>
      <description><![CDATA[自监督学习是一种流行且强大的利用大量数据的方法。
大量未标记的数据，针对这些数据有各种各样的培训目标
已在文献中提出。在本研究中，我们进行贝叶斯分析
最先进的自我监督学习目标，阐明
每个类别中的潜在概率图形模型并呈现
从第一原理推导出来的标准化方法。这
分析还表明整合自我监督学习的自然方法
使用基于可能性的生成模型。我们在内部实例化这个概念
基于集群的自监督学习和能量模型领域，引入
一个新颖的下界，被证明可以可靠地惩罚最重要的
失效模式。此外，这个新提出的下限使得
标准骨干架构的培训，无需
不对称元素，例如停止梯度、动量编码器或专门的
聚类层 - 通常是为了避免学习琐碎的解决方案而引入的。
我们的理论发现通过合成和实验得到证实
真实世界的数据，包括 SVHN、CIFAR10 和 CIFAR100，从而表明我们的
目标函数可以超越现有的自我监督学习
聚类、生成和分布外检测方面的策略
性能大幅提升。我们还证明 GEDI 可以集成
进入神经符号框架以减轻推理捷径问题
通过增强学习更高质量的符号表示
分类性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.00873</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:51 GMT</pubDate>
    </item>
    <item>
      <title>Metaverse 的联合多视图合成。 (arXiv:2401.00859v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2401.00859</link>
      <description><![CDATA[虚拟宇宙预计将提供沉浸式娱乐、教育和
商业应用程序。然而，虚拟现实（VR）通过无线传输
网络是数据和计算密集型的，因此引入
满足严格的服务质量要求的新颖解决方案。和
边缘智能和深度学习的最新进展，我们开发了
新颖的多视图合成框架，可以有效地提供
用于无线内容交付的计算、存储和通信资源
在虚拟宇宙中。我们提出了一种三维（3D）感知生成模型
使用单视图图像的集合。这些单视图图像是
传输给一组具有重叠视野的用户，从而避免
与传输图块或整个 3D 模型相比，可以传输大量内容。
然后，我们提出了一种联邦学习方法来保证有效的
学习过程。可以通过表征来提高训练性能
具有较大潜在特征空间的垂直和水平数据样本，
同时可以通过减少数量来实现低延迟通信
联邦学习期间传输的参数。我们还建议建立一个联邦
迁移学习框架能够快速适应不同的领域
目标域。仿真结果证明了我们的有效性
提出了用于 VR 内容交付的联合多视图合成框架。
]]></description>
      <guid>http://arxiv.org/abs/2401.00859</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:50 GMT</pubDate>
    </item>
    <item>
      <title>FlashVideo：文本到视频生成中的快速推理框架。 （arXiv：2401.00869v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00869</link>
      <description><![CDATA[在不断发展的机器学习领域，视频生成见证了
基于自回归的变压器模型的重大进步和
扩散模型，以合成动态和逼真的场景而闻名。然而，
这些模型经常面临推理时间延长的挑战，即使对于
生成 GIF 等短视频剪辑。本文介绍了FlashVideo，一个
专为快速生成文本到视频而定制的新颖框架。 Flash视频
代表了 RetNet 架构首次成功适应视频
一代，为该领域带来了独特的方法。利用
基于RetNet架构，FlashVideo降低推理时间复杂度
对于长度为 $L$ 的序列，从 $\mathcal{O}(L^2)$ 到 $\mathcal{O}(L)$，
显着加快推理速度。此外，我们采用
无冗余插帧方法，提高插帧效率
插值。我们的综合实验表明 FlashVideo
与传统方法相比，效率提高了 $\times9.17$
基于自回归的Transformer模型，其推理速度相同
与基于 BERT 的变压器模型的数量级。
]]></description>
      <guid>http://arxiv.org/abs/2401.00869</guid>
      <pubDate>Wed, 03 Jan 2024 03:14:50 GMT</pubDate>
    </item>
    </channel>
</rss>